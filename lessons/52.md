# Lesson 52: EIP-4844 and Blob Transactions

*"Study hard what interests you the most in the most undisciplined, irreverent and original manner possible." - Richard Feynman*

## Files with Inline Comments for This Lesson
- `crates/primitives/src/transaction/eip4844.rs` - Blob transaction structure
- `crates/consensus/common/src/validation.rs` - EIP-4844 validation
- `crates/rpc/rpc/src/eth/api/call.rs` - Blob gas estimation
- `crates/ethereum/evm/src/execute.rs` - Blob fee calculation
- `crates/net/network/src/transactions/blob.rs` - Blob transaction propagation

## What is EIP-4844?

EIP-4844 introduces "blob transactions" that carry large amounts of data (blobs) at a much lower cost than calldata. This enables layer 2 rollups to post their data to Ethereum cheaply, implementing "proto-danksharding" as a stepping stone to full data availability sharding.

```
EIP-4844 Blob Transaction Structure:
┌─────────────────────────────────────────────────┐
│             Blob Transaction                    │
│  ┌─────────────────┐  ┌─────────────────────┐  │
│  │   Standard TX   │  │    Blob Data        │  │
│  │  - to/from      │  │  - blob[0]: 128KB   │  │
│  │  - value        │  │  - blob[1]: 128KB   │  │
│  │  - gas_limit    │  │  - ...              │  │
│  │  - priority_fee │  │  - blob[5]: 128KB   │  │
│  └─────────────────┘  └─────────────────────┘  │
│                                                 │
│  KZG Commitments: [C0, C1, ..., C5]           │
│  KZG Proofs: [P0, P1, ..., P5]                │
│  Blob Gas: Used for pricing blob data          │
└─────────────────────────────────────────────────┘
```

## Blob Transaction Structure

EIP-4844 transactions extend EIP-1559 with blob-specific fields:

```rust
/// EIP-4844 blob transaction
/// Located in: crates/primitives/src/transaction/eip4844.rs

use alloy_eips::eip4844::{Blob, BlobTransactionSidecar};
use alloy_primitives::{keccak256, Address, Bytes, ChainId, TxKind, U256};

/// LESSON 52: Blob Transaction
/// Combines standard transaction with blob data
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct TxEip4844 {
    /// Chain ID for transaction
    pub chain_id: ChainId,
    /// Nonce of the transaction
    pub nonce: u64,
    /// Gas limit for execution
    pub gas_limit: u64,
    /// Maximum priority fee per gas (tip)
    pub max_priority_fee_per_gas: u128,
    /// Maximum fee per gas willing to pay
    pub max_fee_per_gas: u128,
    /// LESSON 52: Blob Gas Pricing
    /// Maximum fee per blob gas unit
    pub max_fee_per_blob_gas: u128,
    /// Recipient address
    pub to: Address,
    /// Transaction value
    pub value: U256,
    /// Input data
    pub input: Bytes,
    /// Access list (EIP-2930)
    pub access_list: AccessList,
    /// LESSON 52: Blob Commitments
    /// KZG commitments to blob data
    pub blob_versioned_hashes: Vec<B256>,
}

/// LESSON 52: Blob Sidecar
/// Contains the actual blob data and proofs
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct BlobTransactionSidecar {
    /// The blob data (up to 6 blobs)
    pub blobs: Vec<Blob>,
    /// KZG commitments for each blob
    pub commitments: Vec<Bytes48>,
    /// KZG proofs for each blob
    pub proofs: Vec<Bytes48>,
}

impl TxEip4844 {
    /// LESSON 52: Blob Count
    /// Number of blobs in this transaction
    pub fn blob_count(&self) -> usize {
        self.blob_versioned_hashes.len()
    }
    
    /// Calculate blob gas used
    pub fn blob_gas(&self) -> u64 {
        // Each blob uses exactly 131072 (2^17) blob gas
        self.blob_count() as u64 * BlobGasPerBlob::get()
    }
    
    /// LESSON 52: Effective Blob Fee
    /// Calculate the effective fee per blob gas
    pub fn effective_blob_gas_price(&self, blob_base_fee: u128) -> u128 {
        self.max_fee_per_blob_gas.min(blob_base_fee)
    }
    
    /// Total blob fee for this transaction
    pub fn blob_fee(&self, blob_base_fee: u128) -> U256 {
        let effective_price = self.effective_blob_gas_price(blob_base_fee);
        U256::from(self.blob_gas()) * U256::from(effective_price)
    }
    
    /// LESSON 52: Versioned Hash
    /// Calculate versioned hash from KZG commitment
    pub fn kzg_to_versioned_hash(commitment: &[u8; 48]) -> B256 {
        let mut hash = keccak256(commitment);
        // Set first byte to version (0x01 for EIP-4844)
        hash[0] = VERSIONED_HASH_VERSION_KZG;
        hash
    }
}
```

## Blob Gas Pricing

Blob gas uses a separate pricing mechanism from regular gas:

```rust
/// LESSON 52: Blob Gas Pricing
/// Located in: crates/primitives/src/basefee.rs

/// Target blob gas per block (3 blobs)
pub const TARGET_BLOB_GAS_PER_BLOCK: u64 = 393_216; // 3 * 131072
/// Maximum blob gas per block (6 blobs)
pub const MAX_BLOB_GAS_PER_BLOCK: u64 = 786_432; // 6 * 131072
/// Minimum blob gas price
pub const MIN_BLOB_GAS_PRICE: u128 = 1;
/// Blob gas price update fraction
pub const BLOB_GASPRICE_UPDATE_FRACTION: u128 = 3338477;

/// Calculate next blob base fee
pub fn calculate_excess_blob_gas(
    parent_excess_blob_gas: u64,
    parent_blob_gas_used: u64,
) -> u64 {
    // LESSON 52: Excess Calculation
    // Excess accumulates when we use more than target
    parent_excess_blob_gas
        .saturating_add(parent_blob_gas_used)
        .saturating_sub(TARGET_BLOB_GAS_PER_BLOCK)
}

/// Calculate blob base fee from excess blob gas
pub fn calc_blob_gasprice(excess_blob_gas: u64) -> u128 {
    if excess_blob_gas == 0 {
        return MIN_BLOB_GAS_PRICE;
    }
    
    // LESSON 52: Exponential Pricing
    // Price increases exponentially with excess
    let excess = U256::from(excess_blob_gas);
    let update_fraction = U256::from(BLOB_GASPRICE_UPDATE_FRACTION);
    
    // e^(excess_blob_gas / BLOB_GASPRICE_UPDATE_FRACTION)
    // Approximated using integer math
    let mut result = U256::from(MIN_BLOB_GAS_PRICE);
    let mut factor = excess;
    let mut denominator = update_fraction;
    
    // Taylor series approximation of e^x
    for i in 1..=10 {
        let term = factor / denominator;
        result += U256::from(MIN_BLOB_GAS_PRICE) * term;
        
        factor *= excess;
        denominator *= update_fraction * U256::from(i + 1);
        
        if term == U256::ZERO {
            break;
        }
    }
    
    result.min(U256::from(u128::MAX)).to()
}
```

## Blob Transaction Validation

Blob transactions require additional validation:

```rust
/// LESSON 52: Blob Validation
/// Located in: crates/consensus/common/src/validation.rs

pub fn validate_4844_header_standalone<H: BlockHeader>(
    header: &H,
    blob_params: BlobParams,
) -> Result<(), ConsensusError> {
    // LESSON 52: Required Fields
    // Blob gas used must be present
    let blob_gas_used = header.blob_gas_used()
        .ok_or(ConsensusError::BlobGasUsedMissing)?;
    
    // Excess blob gas must be present
    let excess_blob_gas = header.excess_blob_gas()
        .ok_or(ConsensusError::ExcessBlobGasMissing)?;
    
    // LESSON 52: Gas Limits
    // Blob gas used cannot exceed maximum
    if blob_gas_used > blob_params.max_blob_gas_per_block {
        return Err(ConsensusError::BlobGasUsedExceedsMaxBlobGasPerBlock {
            blob_gas_used,
            max_blob_gas_per_block: blob_params.max_blob_gas_per_block,
        });
    }
    
    // LESSON 52: Gas Granularity
    // Blob gas used must be multiple of gas per blob
    if blob_gas_used % blob_params.blob_gas_per_blob != 0 {
        return Err(ConsensusError::BlobGasUsedNotMultipleOfBlobGasPerBlob {
            blob_gas_used,
            blob_gas_per_blob: blob_params.blob_gas_per_blob,
        });
    }
    
    Ok(())
}

pub fn validate_against_parent_4844<H: BlockHeader>(
    header: &H,
    parent: &H,
    blob_params: BlobParams,
) -> Result<(), ConsensusError> {
    let blob_gas_used = header.blob_gas_used().unwrap_or(0);
    let parent_blob_gas_used = parent.blob_gas_used().unwrap_or(0);
    let parent_excess_blob_gas = parent.excess_blob_gas().unwrap_or(0);
    
    // LESSON 52: Excess Propagation
    // Calculate expected excess blob gas
    let expected_excess = calculate_excess_blob_gas(
        parent_excess_blob_gas,
        parent_blob_gas_used,
    );
    
    if header.excess_blob_gas() != Some(expected_excess) {
        return Err(ConsensusError::ExcessBlobGasDiff {
            diff: GotExpected {
                got: header.excess_blob_gas().unwrap_or(0),
                expected: expected_excess,
            },
            parent_excess_blob_gas,
            parent_blob_gas_used,
        });
    }
    
    Ok(())
}
```

## Blob Transaction Execution

During execution, blob fees are calculated and burned:

```rust
/// LESSON 52: Blob Execution
/// Located in: crates/ethereum/evm/src/execute.rs

impl<EvmConfig> BlockExecutor for EthBlockExecutor<EvmConfig> {
    fn execute_block(&mut self, block: &Block) -> Result<BlockExecutionResult, BlockExecutionError> {
        let mut cumulative_gas_used = 0;
        let mut receipts = Vec::new();
        let mut blob_gas_used = 0;
        
        // LESSON 52: Blob Base Fee
        // Calculate blob base fee for this block
        let blob_base_fee = calc_blob_gasprice(
            block.header.excess_blob_gas.unwrap_or(0)
        );
        
        for transaction in &block.body.transactions {
            // Execute regular transaction
            let result = self.execute_transaction(transaction)?;
            cumulative_gas_used += result.gas_used;
            
            // LESSON 52: Blob Fee Processing
            // Handle blob-specific fees
            if let Transaction::Eip4844(blob_tx) = transaction {
                // Calculate blob gas for this transaction
                let tx_blob_gas = blob_tx.blob_gas();
                blob_gas_used += tx_blob_gas;
                
                // Calculate blob fee
                let blob_fee = blob_tx.blob_fee(blob_base_fee);
                
                // LESSON 52: Blob Fee Burn
                // Blob fees are burned (removed from circulation)
                // Regular gas fees still go to validator
                self.burn_blob_fee(transaction.signer(), blob_fee)?;
                
                // Validate blob count doesn't exceed limit
                if blob_gas_used > MAX_BLOB_GAS_PER_BLOCK {
                    return Err(BlockExecutionError::BlobGasLimitExceeded);
                }
            }
            
            receipts.push(result.receipt);
        }
        
        // LESSON 52: Block Validation
        // Validate blob gas used matches header
        if block.header.blob_gas_used != Some(blob_gas_used) {
            return Err(BlockExecutionError::BlobGasUsedMismatch {
                got: blob_gas_used,
                expected: block.header.blob_gas_used.unwrap_or(0),
            });
        }
        
        Ok(BlockExecutionResult {
            receipts,
            gas_used: cumulative_gas_used,
            blob_gas_used,
        })
    }
    
    /// LESSON 52: Burn Blob Fee
    /// Remove blob fee from sender balance
    fn burn_blob_fee(&mut self, sender: Address, amount: U256) -> Result<(), Error> {
        let account = self.state.account_mut(sender)?;
        
        if account.balance < amount {
            return Err(Error::InsufficientFunds {
                available: account.balance,
                required: amount,
            });
        }
        
        // Subtract from balance (effectively burning)
        account.balance -= amount;
        
        Ok(())
    }
}
```

## Blob Transaction Propagation

The network handles blob transactions differently due to their size:

```rust
/// LESSON 52: Blob Propagation
/// Located in: crates/net/network/src/transactions/blob.rs

use std::collections::HashMap;

/// Manages blob transaction sidecars separately from transactions
pub struct BlobStore {
    /// Maps transaction hash to blob sidecar
    sidecars: HashMap<TxHash, BlobTransactionSidecar>,
    /// Maximum number of blob transactions to store
    max_blobs: usize,
    /// Storage size limit
    max_size: usize,
}

impl BlobStore {
    /// LESSON 52: Blob Storage
    /// Store blob sidecar for a transaction
    pub fn insert_blob(
        &mut self,
        tx_hash: TxHash,
        sidecar: BlobTransactionSidecar,
    ) -> Result<(), BlobStoreError> {
        // LESSON 52: Size Limits
        // Check if we have space
        if self.sidecars.len() >= self.max_blobs {
            return Err(BlobStoreError::StorageFull);
        }
        
        // Estimate size
        let sidecar_size = sidecar.blobs.len() * 131072; // 128KB per blob
        if self.current_size() + sidecar_size > self.max_size {
            return Err(BlobStoreError::SizeLimitExceeded);
        }
        
        // LESSON 52: Validation
        // Verify KZG commitments match versioned hashes
        self.validate_sidecar(&sidecar)?;
        
        self.sidecars.insert(tx_hash, sidecar);
        Ok(())
    }
    
    /// Retrieve blob sidecar
    pub fn get_blob(&self, tx_hash: &TxHash) -> Option<&BlobTransactionSidecar> {
        self.sidecars.get(tx_hash)
    }
    
    /// LESSON 52: Blob Validation
    /// Validate KZG commitments and proofs
    fn validate_sidecar(&self, sidecar: &BlobTransactionSidecar) -> Result<(), BlobStoreError> {
        if sidecar.blobs.len() != sidecar.commitments.len() ||
           sidecar.blobs.len() != sidecar.proofs.len() {
            return Err(BlobStoreError::InvalidSidecar);
        }
        
        // LESSON 52: KZG Verification
        // In practice, this would use a KZG library
        for (i, blob) in sidecar.blobs.iter().enumerate() {
            let commitment = &sidecar.commitments[i];
            let proof = &sidecar.proofs[i];
            
            // Verify: KZG.verify_blob_proof(blob, commitment, proof)
            if !self.verify_kzg_proof(blob, commitment, proof) {
                return Err(BlobStoreError::InvalidProof(i));
            }
        }
        
        Ok(())
    }
    
    /// LESSON 52: KZG Proof Verification
    /// Verify that commitment correctly commits to blob data
    fn verify_kzg_proof(
        &self,
        blob: &Blob,
        commitment: &Bytes48,
        proof: &Bytes48,
    ) -> bool {
        // This would use actual KZG verification
        // For now, return true (validation happens elsewhere)
        true
    }
    
    fn current_size(&self) -> usize {
        self.sidecars.values()
            .map(|s| s.blobs.len() * 131072)
            .sum()
    }
}

/// LESSON 52: Gossip Protocol
/// Handle blob transaction gossip
impl TransactionPool {
    pub fn handle_blob_transaction(
        &mut self,
        tx: Transaction,
        sidecar: BlobTransactionSidecar,
    ) -> Result<(), PoolError> {
        // LESSON 52: Separate Storage
        // Store transaction and sidecar separately
        let tx_hash = tx.hash();
        
        // Validate the transaction normally
        self.validate_transaction(&tx)?;
        
        // Store blob sidecar
        self.blob_store.insert_blob(tx_hash, sidecar)?;
        
        // Add transaction to pool
        self.insert_transaction(tx)?;
        
        Ok(())
    }
    
    /// LESSON 52: Blob Announcement
    /// Announce blob transaction to peers
    pub fn announce_blob_transaction(&self, tx_hash: TxHash) {
        // Only announce hash, not full blob data
        for peer in &self.peers {
            peer.send_announcement(AnnouncementMessage::BlobTx(tx_hash));
        }
    }
    
    /// LESSON 52: Blob Request
    /// Request blob sidecar from peer
    pub fn request_blob_sidecar(&self, peer: PeerId, tx_hash: TxHash) {
        let request = BlobSidecarRequest { tx_hash };
        self.send_to_peer(peer, request);
    }
}
```

## Gas Estimation for Blob Transactions

RPC methods need to estimate blob gas:

```rust
/// LESSON 52: Blob Gas Estimation
/// Located in: crates/rpc/rpc/src/eth/api/call.rs

impl<Provider> EthApi<Provider> {
    /// Estimate gas for a blob transaction
    pub async fn estimate_blob_gas(
        &self,
        request: TransactionRequest,
    ) -> RpcResult<BlobGasEstimate> {
        // LESSON 52: Blob Count
        // Determine number of blobs needed
        let blob_count = request.blob_versioned_hashes
            .as_ref()
            .map(|hashes| hashes.len())
            .unwrap_or(0);
        
        if blob_count == 0 {
            return Err(RpcError::InvalidParams("No blobs in request".to_string()));
        }
        
        // LESSON 52: Blob Gas Calculation
        // Each blob uses fixed amount of blob gas
        let blob_gas_used = blob_count as u64 * BlobGasPerBlob::get();
        
        // Get current blob base fee
        let current_block = self.provider.latest_header()?;
        let blob_base_fee = calc_blob_gasprice(
            current_block.excess_blob_gas.unwrap_or(0)
        );
        
        // LESSON 52: Fee Estimation
        // Estimate required max_fee_per_blob_gas
        let recommended_blob_fee = blob_base_fee * 120 / 100; // 20% buffer
        
        // Estimate regular gas
        let gas_estimate = self.estimate_gas(request.clone()).await?;
        
        Ok(BlobGasEstimate {
            blob_gas_used,
            blob_base_fee,
            recommended_max_fee_per_blob_gas: recommended_blob_fee,
            regular_gas_estimate: gas_estimate,
        })
    }
    
    /// Get blob gas price
    pub fn blob_gas_price(&self) -> RpcResult<U256> {
        let current_block = self.provider.latest_header()?;
        let blob_base_fee = calc_blob_gasprice(
            current_block.excess_blob_gas.unwrap_or(0)
        );
        
        Ok(U256::from(blob_base_fee))
    }
}

#[derive(Debug)]
pub struct BlobGasEstimate {
    pub blob_gas_used: u64,
    pub blob_base_fee: u128,
    pub recommended_max_fee_per_blob_gas: u128,
    pub regular_gas_estimate: U256,
}
```

## Summary - The Scaling Bridge

EIP-4844 is Ethereum's clever solution to the L2 data availability crisis. By creating a separate, temporary data storage system with its own economics, it enables:

1. **90%+ Cost Reduction** for L2 data posting (from $50k to $5k per batch)
2. **Scalability Path** towards full danksharding 
3. **Cryptographic Guarantees** via KZG commitments without permanent storage costs

**The Bigger Picture**: This isn't just about cheaper fees - it's about making Ethereum viable as the settlement layer for a multi-chain ecosystem. Think of it as building the interstate highway system that enables efficient commerce between cities (L2s).

**Common Pitfall**: Don't think of blobs as just "cheap calldata." They're a fundamentally different primitive with temporary availability, separate fee markets, and different security assumptions.

## Assignments

### Assignment 1: Blob Pool Manager
Create a blob transaction pool with efficient storage:

```rust
struct BlobPool {
    pending: HashMap<TxHash, BlobTransaction>,
    storage_limit: usize,
}

impl BlobPool {
    fn add_blob_transaction(&mut self, tx: BlobTransaction) -> Result<(), PoolError> {
        // TODO: Implement blob transaction management
    }
}
```

### Assignment 2: KZG Commitment Validator
Implement KZG commitment validation:

```rust
struct KzgValidator {
    trusted_setup: TrustedSetup,
}

impl KzgValidator {
    fn verify_blob_commitment(&self, blob: &Blob, commitment: &Commitment) -> bool {
        // TODO: Verify KZG commitment matches blob data
    }
}
```

### Assignment 3: Blob Fee Oracle
Create an oracle for predicting blob fees:

```rust
struct BlobFeeOracle {
    history: VecDeque<BlobUsage>,
}

impl BlobFeeOracle {
    fn predict_blob_fee(&self, blocks_ahead: u64) -> u128 {
        // TODO: Predict future blob fees based on usage patterns
    }
}
```

## Questions to Ponder

1. Why use exponential pricing for blob gas instead of linear?
2. How do KZG commitments enable efficient data availability proofs?
3. Why separate blob data from transaction execution?
4. What prevents blob spam attacks?
5. How does EIP-4844 enable layer 2 scaling?

## Assignment Answers

### Answer 1: Blob Pool Manager

```rust
use std::collections::{HashMap, VecDeque};
use std::time::{Duration, Instant};

#[derive(Debug)]
struct BlobTransaction {
    tx: Transaction,
    sidecar: BlobTransactionSidecar,
    timestamp: Instant,
    blob_count: usize,
    priority_score: u64,
}

impl BlobTransaction {
    fn size(&self) -> usize {
        // Each blob is ~128KB
        self.blob_count * 131072
    }
    
    fn age(&self) -> Duration {
        self.timestamp.elapsed()
    }
}

struct BlobPool {
    /// Pending blob transactions by hash
    pending: HashMap<TxHash, BlobTransaction>,
    /// Transactions ordered by priority
    priority_queue: VecDeque<TxHash>,
    /// Maximum storage size in bytes
    storage_limit: usize,
    /// Maximum number of transactions
    tx_limit: usize,
    /// Maximum age before eviction
    max_age: Duration,
}

impl BlobPool {
    fn new(storage_limit: usize, tx_limit: usize) -> Self {
        Self {
            pending: HashMap::new(),
            priority_queue: VecDeque::new(),
            storage_limit,
            tx_limit,
            max_age: Duration::from_secs(300), // 5 minutes
        }
    }
    
    fn add_blob_transaction(&mut self, tx: BlobTransaction) -> Result<(), PoolError> {
        let tx_hash = tx.tx.hash();
        
        // Check if already exists
        if self.pending.contains_key(&tx_hash) {
            return Err(PoolError::AlreadyExists);
        }
        
        // Validate blob transaction
        self.validate_blob_transaction(&tx)?;
        
        // Check capacity constraints
        if self.pending.len() >= self.tx_limit {
            self.evict_least_priority()?;
        }
        
        let tx_size = tx.size();
        if self.current_size() + tx_size > self.storage_limit {
            self.evict_by_size(tx_size)?;
        }
        
        // Calculate priority score
        let priority = self.calculate_priority(&tx);
        let mut blob_tx = tx;
        blob_tx.priority_score = priority;
        
        // Insert into pool
        self.pending.insert(tx_hash, blob_tx);
        
        // Update priority queue
        self.insert_by_priority(tx_hash);
        
        Ok(())
    }
    
    fn validate_blob_transaction(&self, tx: &BlobTransaction) -> Result<(), PoolError> {
        // Basic validation
        if tx.blob_count == 0 || tx.blob_count > 6 {
            return Err(PoolError::InvalidBlobCount);
        }
        
        // Verify sidecar matches transaction
        if tx.sidecar.blobs.len() != tx.blob_count {
            return Err(PoolError::SidecarMismatch);
        }
        
        // Verify versioned hashes
        if let Transaction::Eip4844(eip4844_tx) = &tx.tx {
            if eip4844_tx.blob_versioned_hashes.len() != tx.blob_count {
                return Err(PoolError::VersionedHashMismatch);
            }
            
            // Verify each commitment produces correct versioned hash
            for (i, commitment) in tx.sidecar.commitments.iter().enumerate() {
                let expected_hash = TxEip4844::kzg_to_versioned_hash(commitment);
                if eip4844_tx.blob_versioned_hashes[i] != expected_hash {
                    return Err(PoolError::CommitmentMismatch(i));
                }
            }
        }
        
        Ok(())
    }
    
    fn calculate_priority(&self, tx: &BlobTransaction) -> u64 {
        if let Transaction::Eip4844(eip4844_tx) = &tx.tx {
            // Priority = tip_per_gas * gas_limit + blob_fee_per_gas * blob_gas
            let gas_priority = eip4844_tx.max_priority_fee_per_gas * eip4844_tx.gas_limit as u128;
            let blob_priority = eip4844_tx.max_fee_per_blob_gas * tx.blob_count as u128 * 131072;
            
            (gas_priority + blob_priority).min(u64::MAX as u128) as u64
        } else {
            0
        }
    }
    
    fn insert_by_priority(&mut self, tx_hash: TxHash) {
        let tx_priority = self.pending[&tx_hash].priority_score;
        
        // Find insertion point (maintain descending order)
        let mut insert_pos = 0;
        for &existing_hash in &self.priority_queue {
            if self.pending[&existing_hash].priority_score >= tx_priority {
                insert_pos += 1;
            } else {
                break;
            }
        }
        
        self.priority_queue.insert(insert_pos, tx_hash);
    }
    
    fn evict_least_priority(&mut self) -> Result<(), PoolError> {
        if let Some(tx_hash) = self.priority_queue.pop_back() {
            self.pending.remove(&tx_hash);
            Ok(())
        } else {
            Err(PoolError::EmptyPool)
        }
    }
    
    fn evict_by_size(&mut self, needed_size: usize) -> Result<(), PoolError> {
        let mut freed_size = 0;
        let mut to_remove = Vec::new();
        
        // Remove oldest/lowest priority first
        while freed_size < needed_size && !self.priority_queue.is_empty() {
            if let Some(tx_hash) = self.priority_queue.pop_back() {
                if let Some(tx) = self.pending.get(&tx_hash) {
                    freed_size += tx.size();
                    to_remove.push(tx_hash);
                }
            }
        }
        
        for tx_hash in to_remove {
            self.pending.remove(&tx_hash);
        }
        
        if freed_size < needed_size {
            Err(PoolError::InsufficientSpace)
        } else {
            Ok(())
        }
    }
    
    fn cleanup_expired(&mut self) {
        let now = Instant::now();
        let mut to_remove = Vec::new();
        
        for (&tx_hash, tx) in &self.pending {
            if tx.age() > self.max_age {
                to_remove.push(tx_hash);
            }
        }
        
        for tx_hash in to_remove {
            self.pending.remove(&tx_hash);
            // Remove from priority queue
            if let Some(pos) = self.priority_queue.iter().position(|&h| h == tx_hash) {
                self.priority_queue.remove(pos);
            }
        }
    }
    
    fn get_best_blobs(&self, gas_limit: u64, blob_gas_limit: u64) -> Vec<&BlobTransaction> {
        let mut selected = Vec::new();
        let mut used_gas = 0;
        let mut used_blob_gas = 0;
        
        for &tx_hash in &self.priority_queue {
            if let Some(tx) = self.pending.get(&tx_hash) {
                if let Transaction::Eip4844(eip4844_tx) = &tx.tx {
                    let tx_gas = eip4844_tx.gas_limit;
                    let tx_blob_gas = tx.blob_count as u64 * 131072;
                    
                    if used_gas + tx_gas <= gas_limit && 
                       used_blob_gas + tx_blob_gas <= blob_gas_limit {
                        selected.push(tx);
                        used_gas += tx_gas;
                        used_blob_gas += tx_blob_gas;
                    }
                }
            }
        }
        
        selected
    }
    
    fn current_size(&self) -> usize {
        self.pending.values().map(|tx| tx.size()).sum()
    }
    
    fn stats(&self) -> BlobPoolStats {
        BlobPoolStats {
            pending_count: self.pending.len(),
            total_size: self.current_size(),
            avg_blob_count: self.pending.values()
                .map(|tx| tx.blob_count)
                .sum::<usize>() as f64 / self.pending.len().max(1) as f64,
        }
    }
}

#[derive(Debug)]
struct BlobPoolStats {
    pending_count: usize,
    total_size: usize,
    avg_blob_count: f64,
}

#[derive(Debug, thiserror::Error)]
enum PoolError {
    #[error("Transaction already exists")]
    AlreadyExists,
    #[error("Invalid blob count")]
    InvalidBlobCount,
    #[error("Sidecar mismatch")]
    SidecarMismatch,
    #[error("Versioned hash mismatch")]
    VersionedHashMismatch,
    #[error("Commitment mismatch at index {0}")]
    CommitmentMismatch(usize),
    #[error("Empty pool")]
    EmptyPool,
    #[error("Insufficient space")]
    InsufficientSpace,
}
```

### Answer 2: KZG Commitment Validator

```rust
// Note: This is a simplified implementation. Real KZG verification
// requires complex elliptic curve operations and a trusted setup.

#[derive(Debug)]
struct TrustedSetup {
    /// G1 powers of tau
    g1_powers: Vec<G1Point>,
    /// G2 powers of tau
    g2_powers: Vec<G2Point>,
}

#[derive(Debug, Clone)]
struct G1Point([u8; 48]); // Compressed BLS12-381 G1 point

#[derive(Debug, Clone)]
struct G2Point([u8; 96]); // Compressed BLS12-381 G2 point

struct KzgValidator {
    trusted_setup: TrustedSetup,
}

impl KzgValidator {
    fn new(trusted_setup: TrustedSetup) -> Self {
        Self { trusted_setup }
    }
    
    /// Verify that a KZG commitment correctly commits to blob data
    fn verify_blob_commitment(&self, blob: &Blob, commitment: &Commitment) -> bool {
        // Convert blob to polynomial evaluations
        let evaluations = self.blob_to_evaluations(blob);
        
        // Compute commitment from evaluations
        let computed_commitment = self.compute_commitment(&evaluations);
        
        // Check if commitments match
        computed_commitment.0 == commitment.0
    }
    
    /// Verify a KZG proof for polynomial evaluation
    fn verify_kzg_proof(
        &self,
        commitment: &Commitment,
        z: &Fr, // Evaluation point
        y: &Fr, // Claimed evaluation
        proof: &Proof,
    ) -> bool {
        // This implements the KZG verification equation:
        // e(proof, [τ - z]₂) = e(commitment - [y]₁, generator₂)
        
        // In a real implementation, this would use pairing operations
        // For now, we'll simulate the verification
        true // Simplified
    }
    
    /// Verify blob proof (specific to EIP-4844)
    fn verify_blob_proof(
        &self,
        blob: &Blob,
        commitment: &Commitment,
        proof: &Proof,
    ) -> bool {
        // EIP-4844 uses a specific evaluation point
        let z = self.get_blob_evaluation_point();
        
        // Compute evaluation of the blob polynomial at z
        let y = self.evaluate_blob_at_point(blob, &z);
        
        // Verify the KZG proof
        self.verify_kzg_proof(commitment, &z, &y, proof)
    }
    
    /// Convert blob data to polynomial evaluations
    fn blob_to_evaluations(&self, blob: &Blob) -> Vec<Fr> {
        // Blob contains 4096 field elements
        let mut evaluations = Vec::with_capacity(4096);
        
        for chunk in blob.0.chunks(32) {
            // Convert each 32-byte chunk to a field element
            let mut bytes = [0u8; 32];
            bytes.copy_from_slice(chunk);
            evaluations.push(Fr::from_bytes(bytes));
        }
        
        evaluations
    }
    
    /// Compute KZG commitment from polynomial evaluations
    fn compute_commitment(&self, evaluations: &[Fr]) -> Commitment {
        // Commitment = Σ(evaluation[i] * g1_powers[i])
        let mut result = G1Point::identity();
        
        for (i, &eval) in evaluations.iter().enumerate() {
            if i < self.trusted_setup.g1_powers.len() {
                let term = self.trusted_setup.g1_powers[i].multiply(eval);
                result = result.add(&term);
            }
        }
        
        Commitment(result.to_bytes())
    }
    
    /// Get the standard evaluation point for blob verification
    fn get_blob_evaluation_point(&self) -> Fr {
        // In EIP-4844, this is a specific point used for all blob proofs
        Fr::from_bytes([0u8; 32]) // Simplified
    }
    
    /// Evaluate blob polynomial at a specific point
    fn evaluate_blob_at_point(&self, blob: &Blob, z: &Fr) -> Fr {
        let evaluations = self.blob_to_evaluations(blob);
        
        // Use Horner's method for polynomial evaluation
        let mut result = Fr::zero();
        let mut power = Fr::one();
        
        for eval in evaluations {
            result = result.add(&eval.multiply(&power));
            power = power.multiply(z);
        }
        
        result
    }
    
    /// Batch verify multiple blob proofs
    fn batch_verify_blobs(
        &self,
        blobs: &[Blob],
        commitments: &[Commitment],
        proofs: &[Proof],
    ) -> bool {
        if blobs.len() != commitments.len() || blobs.len() != proofs.len() {
            return false;
        }
        
        // Verify each blob individually
        for ((blob, commitment), proof) in blobs.iter().zip(commitments).zip(proofs) {
            if !self.verify_blob_proof(blob, commitment, proof) {
                return false;
            }
        }
        
        true
    }
}

// Simplified field element implementation
#[derive(Debug, Clone, Copy)]
struct Fr([u8; 32]);

impl Fr {
    fn from_bytes(bytes: [u8; 32]) -> Self {
        Self(bytes)
    }
    
    fn zero() -> Self {
        Self([0u8; 32])
    }
    
    fn one() -> Self {
        let mut bytes = [0u8; 32];
        bytes[31] = 1;
        Self(bytes)
    }
    
    fn add(&self, other: &Self) -> Self {
        // Simplified addition mod BLS12-381 scalar field
        Self(self.0) // Just return self for simplicity
    }
    
    fn multiply(&self, other: &Self) -> Self {
        // Simplified multiplication mod BLS12-381 scalar field
        Self(self.0) // Just return self for simplicity
    }
}

impl G1Point {
    fn identity() -> Self {
        Self([0u8; 48])
    }
    
    fn multiply(&self, scalar: Fr) -> Self {
        // Scalar multiplication on G1
        Self(self.0) // Simplified
    }
    
    fn add(&self, other: &Self) -> Self {
        // Point addition on G1
        Self(self.0) // Simplified
    }
    
    fn to_bytes(&self) -> [u8; 48] {
        self.0
    }
}

#[derive(Debug)]
struct Commitment([u8; 48]);

#[derive(Debug)]
struct Proof([u8; 48]);

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_blob_commitment_verification() {
        let trusted_setup = TrustedSetup {
            g1_powers: vec![G1Point([0u8; 48]); 4096],
            g2_powers: vec![G2Point([0u8; 96]); 65],
        };
        
        let validator = KzgValidator::new(trusted_setup);
        let blob = Blob([0u8; 131072]); // Empty blob
        let commitment = Commitment([0u8; 48]);
        
        // This would be a real test in production
        assert!(validator.verify_blob_commitment(&blob, &commitment));
    }
}
```

### Answer 3: Blob Fee Oracle

```rust
use std::collections::VecDeque;

#[derive(Debug, Clone)]
struct BlobUsage {
    block_number: u64,
    blob_gas_used: u64,
    blob_gas_target: u64,
    blob_base_fee: u128,
    timestamp: u64,
}

impl BlobUsage {
    fn utilization_ratio(&self) -> f64 {
        self.blob_gas_used as f64 / self.blob_gas_target as f64
    }
}

struct BlobFeeOracle {
    /// Historical blob usage data
    history: VecDeque<BlobUsage>,
    /// Maximum history to keep
    max_history: usize,
    /// Machine learning model for predictions
    predictor: BlobFeePredictor,
}

impl BlobFeeOracle {
    fn new(max_history: usize) -> Self {
        Self {
            history: VecDeque::new(),
            max_history,
            predictor: BlobFeePredictor::new(),
        }
    }
    
    fn add_block_data(&mut self, usage: BlobUsage) {
        self.history.push_back(usage);
        
        if self.history.len() > self.max_history {
            self.history.pop_front();
        }
        
        // Update predictor with new data
        self.predictor.update(&self.history);
    }
    
    fn predict_blob_fee(&self, blocks_ahead: u64) -> u128 {
        if self.history.is_empty() {
            return MIN_BLOB_GAS_PRICE;
        }
        
        // Use different prediction strategies based on time horizon
        match blocks_ahead {
            1..=5 => self.predict_short_term(blocks_ahead),
            6..=50 => self.predict_medium_term(blocks_ahead),
            _ => self.predict_long_term(blocks_ahead),
        }
    }
    
    fn predict_short_term(&self, blocks_ahead: u64) -> u128 {
        // For immediate future, use current trend
        let recent_usage = self.get_recent_trend(10);
        let current_fee = self.history.back().unwrap().blob_base_fee;
        
        // Simple linear extrapolation
        let trend = self.calculate_usage_trend(&recent_usage);
        let fee_multiplier = 1.0 + (trend * blocks_ahead as f64 * 0.1);
        
        (current_fee as f64 * fee_multiplier) as u128
    }
    
    fn predict_medium_term(&self, blocks_ahead: u64) -> u128 {
        // Use pattern recognition for medium term
        let patterns = self.identify_usage_patterns();
        let seasonal_factor = self.calculate_seasonal_factor(blocks_ahead);
        
        let base_prediction = self.predict_short_term(5);
        (base_prediction as f64 * seasonal_factor) as u128
    }
    
    fn predict_long_term(&self, blocks_ahead: u64) -> u128 {
        // Long term: revert to mean with growth factor
        let historical_mean = self.calculate_historical_mean();
        let growth_rate = self.estimate_growth_rate();
        
        let time_factor = (blocks_ahead as f64 / 7200.0); // ~1 day
        let growth_factor = 1.0 + (growth_rate * time_factor);
        
        (historical_mean as f64 * growth_factor) as u128
    }
    
    fn get_recent_trend(&self, window: usize) -> Vec<&BlobUsage> {
        let start = self.history.len().saturating_sub(window);
        self.history.range(start..).collect()
    }
    
    fn calculate_usage_trend(&self, usage_data: &[&BlobUsage]) -> f64 {
        if usage_data.len() < 2 {
            return 0.0;
        }
        
        // Calculate linear regression slope for utilization ratio
        let n = usage_data.len() as f64;
        let sum_x: f64 = (0..usage_data.len()).map(|i| i as f64).sum();
        let sum_y: f64 = usage_data.iter().map(|u| u.utilization_ratio()).sum();
        let sum_xy: f64 = usage_data.iter().enumerate()
            .map(|(i, u)| i as f64 * u.utilization_ratio())
            .sum();
        let sum_x2: f64 = (0..usage_data.len()).map(|i| (i as f64).powi(2)).sum();
        
        // Slope = (n*sum_xy - sum_x*sum_y) / (n*sum_x2 - sum_x^2)
        let numerator = n * sum_xy - sum_x * sum_y;
        let denominator = n * sum_x2 - sum_x.powi(2);
        
        if denominator.abs() < f64::EPSILON {
            0.0
        } else {
            numerator / denominator
        }
    }
    
    fn identify_usage_patterns(&self) -> Vec<UsagePattern> {
        let mut patterns = Vec::new();
        
        // Identify daily patterns (assuming 12s block time, 7200 blocks/day)
        if self.history.len() >= 7200 {
            patterns.push(self.analyze_daily_pattern());
        }
        
        // Identify weekly patterns
        if self.history.len() >= 50400 { // 7 days
            patterns.push(self.analyze_weekly_pattern());
        }
        
        patterns
    }
    
    fn analyze_daily_pattern(&self) -> UsagePattern {
        // Group by hour of day and calculate average utilization
        let mut hourly_usage = vec![0.0f64; 24];
        let mut hourly_counts = vec![0usize; 24];
        
        for usage in &self.history {
            // Estimate hour from block timestamp (simplified)
            let hour = ((usage.timestamp / 3600) % 24) as usize;
            hourly_usage[hour] += usage.utilization_ratio();
            hourly_counts[hour] += 1;
        }
        
        // Calculate averages
        for (usage, count) in hourly_usage.iter_mut().zip(&hourly_counts) {
            if *count > 0 {
                *usage /= *count as f64;
            }
        }
        
        UsagePattern::Daily { hourly_utilization: hourly_usage }
    }
    
    fn analyze_weekly_pattern(&self) -> UsagePattern {
        // Similar to daily but for days of week
        let mut daily_usage = vec![0.0f64; 7];
        let mut daily_counts = vec![0usize; 7];
        
        for usage in &self.history {
            let day = ((usage.timestamp / 86400) % 7) as usize;
            daily_usage[day] += usage.utilization_ratio();
            daily_counts[day] += 1;
        }
        
        for (usage, count) in daily_usage.iter_mut().zip(&daily_counts) {
            if *count > 0 {
                *usage /= *count as f64;
            }
        }
        
        UsagePattern::Weekly { daily_utilization: daily_usage }
    }
    
    fn calculate_seasonal_factor(&self, blocks_ahead: u64) -> f64 {
        // Estimate time of day/week for prediction
        let current_time = self.history.back().unwrap().timestamp;
        let future_time = current_time + (blocks_ahead * 12); // 12s per block
        
        let hour = ((future_time / 3600) % 24) as usize;
        let day = ((future_time / 86400) % 7) as usize;
        
        // Apply seasonal adjustments based on patterns
        let hourly_factor = self.get_hourly_factor(hour);
        let daily_factor = self.get_daily_factor(day);
        
        (hourly_factor + daily_factor) / 2.0
    }
    
    fn get_hourly_factor(&self, hour: usize) -> f64 {
        // Peak hours: 14:00-18:00 UTC (1.2x), Off hours: 02:00-06:00 UTC (0.8x)
        match hour {
            2..=5 => 0.8,   // Low activity
            14..=17 => 1.2, // High activity
            _ => 1.0,       // Normal
        }
    }
    
    fn get_daily_factor(&self, day: usize) -> f64 {
        // Weekends typically lower activity
        match day {
            0 | 6 => 0.9, // Sunday, Saturday
            _ => 1.0,     // Weekdays
        }
    }
    
    fn calculate_historical_mean(&self) -> u128 {
        if self.history.is_empty() {
            return MIN_BLOB_GAS_PRICE;
        }
        
        let sum: u128 = self.history.iter().map(|u| u.blob_base_fee).sum();
        sum / self.history.len() as u128
    }
    
    fn estimate_growth_rate(&self) -> f64 {
        if self.history.len() < 2 {
            return 0.0;
        }
        
        // Calculate growth rate over the full history
        let first = self.history.front().unwrap();
        let last = self.history.back().unwrap();
        let blocks_elapsed = last.block_number - first.block_number;
        
        if blocks_elapsed == 0 {
            return 0.0;
        }
        
        let fee_ratio = last.blob_base_fee as f64 / first.blob_base_fee as f64;
        let time_ratio = blocks_elapsed as f64;
        
        (fee_ratio.ln() / time_ratio).max(-0.1).min(0.1) // Cap growth rate
    }
    
    fn confidence_interval(&self, prediction: u128, blocks_ahead: u64) -> (u128, u128) {
        // Calculate prediction confidence based on historical volatility
        let volatility = self.calculate_volatility();
        let time_factor = (blocks_ahead as f64).sqrt();
        let error_margin = volatility * time_factor * 0.1;
        
        let lower = (prediction as f64 * (1.0 - error_margin)) as u128;
        let upper = (prediction as f64 * (1.0 + error_margin)) as u128;
        
        (lower.max(MIN_BLOB_GAS_PRICE), upper)
    }
    
    fn calculate_volatility(&self) -> f64 {
        if self.history.len() < 2 {
            return 0.0;
        }
        
        // Calculate standard deviation of price changes
        let price_changes: Vec<f64> = self.history.windows(2)
            .map(|w| {
                let change = w[1].blob_base_fee as f64 / w[0].blob_base_fee as f64;
                (change - 1.0).abs()
            })
            .collect();
        
        let mean = price_changes.iter().sum::<f64>() / price_changes.len() as f64;
        let variance = price_changes.iter()
            .map(|x| (x - mean).powi(2))
            .sum::<f64>() / price_changes.len() as f64;
        
        variance.sqrt()
    }
}

#[derive(Debug)]
enum UsagePattern {
    Daily { hourly_utilization: Vec<f64> },
    Weekly { daily_utilization: Vec<f64> },
}

struct BlobFeePredictor {
    // Simplified ML predictor
    weights: Vec<f64>,
}

impl BlobFeePredictor {
    fn new() -> Self {
        Self {
            weights: vec![1.0, 0.5, 0.3, 0.1], // Simple weighted average
        }
    }
    
    fn update(&mut self, history: &VecDeque<BlobUsage>) {
        // In a real implementation, this would update ML model parameters
        // For now, just adjust weights based on recent accuracy
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_blob_fee_prediction() {
        let mut oracle = BlobFeeOracle::new(1000);
        
        // Add some sample data
        for i in 0..100 {
            oracle.add_block_data(BlobUsage {
                block_number: i,
                blob_gas_used: 393_216 / 2, // Half target
                blob_gas_target: 393_216,
                blob_base_fee: MIN_BLOB_GAS_PRICE,
                timestamp: i * 12,
            });
        }
        
        let prediction = oracle.predict_blob_fee(10);
        assert!(prediction >= MIN_BLOB_GAS_PRICE);
        
        let (lower, upper) = oracle.confidence_interval(prediction, 10);
        assert!(lower <= prediction && prediction <= upper);
    }
}
```