# Lesson 93: Resource Management

*"The best way to manage resources is to not waste them." - Unknown*

## Overview
Resource management involves the efficient allocation, monitoring, and cleanup of system resources. This lesson covers memory management, file descriptors, connection pools, and resource lifecycle management.

## Key Concepts
- **Resource Lifecycle**: Creation, usage, and cleanup patterns
- **Memory Management**: Heap allocation and garbage collection optimization
- **Connection Pooling**: Reusing expensive connections
- **Resource Limits**: Preventing resource exhaustion

## Why Resource Management Matters (Like Managing a Restaurant)

Think of resource management like running a restaurant. You have limited tables (connections), kitchen space (memory), staff (threads), and ingredients (file descriptors). Good resource management ensures:

- **No Waste**: Like not letting food spoil, we prevent memory leaks
- **Efficient Reuse**: Like washing dishes for the next customer, we pool connections
- **Capacity Planning**: Like managing reservations, we set resource limits
- **Quick Cleanup**: Like clearing tables fast, we clean up unused resources

From Reth's actual resource management, we see these principles in action:

```rust
// From crates/storage/provider/src/providers/database/provider.rs - Real Reth resource patterns
impl<TX: DbTx, N: NodeTypes> DatabaseProvider<TX, N> {
    /// Resource-aware transaction management
    pub fn new(tx: TX, static_file_provider: StaticFileProvider<N>) -> Self {
        // WHY: Database transactions are expensive resources
        // Like reserving a table in a restaurant - you need to manage it carefully
        Self {
            tx,
            static_file_provider,
            // WHY: Cache expensive computations to avoid re-doing work
            // Like preparing ingredients in advance during slow hours
            cache: HashMap::new(),
        }
    }
    
    /// Efficient resource cleanup on drop
    fn cleanup_resources(&mut self) {
        // WHY: Automatic cleanup prevents resource leaks
        // Like restaurant staff automatically clearing tables when customers leave
        
        // Close open cursors
        self.close_cursors();
        
        // Clear caches to free memory
        self.cache.clear();
        
        // Commit or rollback transaction
        if self.tx.is_dirty() {
            // WHY: Dirty transactions need explicit handling
            // Like making sure the kitchen knows if an order was completed
            if let Err(e) = self.tx.commit() {
                warn!("Failed to commit transaction: {}", e);
                // Rollback on failure
                let _ = self.tx.rollback();
            }
        }
    }
}
```

## Real-World Resource Management Patterns

### 1. Memory Management (Like Restaurant Kitchen Space)

Kitchen space is precious - you can't waste it, and you need to clean as you go. Memory management works similarly:

```rust
// Memory management like managing kitchen space
pub struct KitchenMemoryManager {
    // Like prep station counters - small, frequently used spaces
    small_pools: Vec<MemoryPool>,
    // Like main cooking areas - medium-sized workspaces
    medium_pools: Vec<MemoryPool>,
    // Like banquet prep areas - large, occasionally used spaces
    large_pools: Vec<MemoryPool>,
    // Like dishwashing station - cleanup tracking
    cleanup_tracker: CleanupTracker,
}

impl KitchenMemoryManager {
    pub fn allocate_workspace(&mut self, size: WorkspaceSize) -> Result<Workspace, ResourceError> {
        // WHY: Use the right size space for the task
        // Like choosing prep station vs full kitchen based on what you're cooking
        
        let workspace = match size {
            WorkspaceSize::Small => {
                // WHY: Small frequent allocations use pools
                // Like having dedicated prep stations for common tasks
                self.small_pools.iter_mut()
                    .find(|pool| pool.has_available_space())
                    .and_then(|pool| pool.allocate())
                    .ok_or(ResourceError::SmallPoolExhausted)?
            }
            WorkspaceSize::Medium => {
                // WHY: Medium allocations need balanced approach
                // Like main cooking stations - need more space but still reusable
                self.medium_pools.iter_mut()
                    .find(|pool| pool.has_available_space())
                    .and_then(|pool| pool.allocate())
                    .unwrap_or_else(|| self.allocate_from_heap(size))
            }
            WorkspaceSize::Large => {
                // WHY: Large allocations go straight to heap
                // Like setting up special banquet prep - too big for regular pools
                self.allocate_from_heap(size)
            }
        };
        
        // WHY: Track all allocations for cleanup
        // Like keeping a log of which stations are in use
        self.cleanup_tracker.track_allocation(&workspace);
        
        Ok(workspace)
    }
    
    pub fn cleanup_unused_workspaces(&mut self) -> CleanupResult {
        // WHY: Regular cleanup prevents resource exhaustion
        // Like clearing and sanitizing stations between dinner rushes
        
        let mut freed_space = 0;
        let stale_threshold = Duration::from_secs(300); // 5 minutes
        
        // Clean up stale allocations
        let stale_workspaces = self.cleanup_tracker.find_stale_workspaces(stale_threshold);
        
        for workspace in stale_workspaces {
            match workspace.size {
                WorkspaceSize::Small => {
                    // WHY: Return to pool for reuse
                    // Like cleaning prep station for next use
                    if let Some(pool) = self.find_pool_for_workspace(&workspace) {
                        pool.return_workspace(workspace);
                    }
                }
                WorkspaceSize::Medium => {
                    // WHY: Clean and potentially return to pool
                    if let Some(pool) = self.find_pool_for_workspace(&workspace) {
                        pool.return_workspace(workspace);
                    } else {
                        // Return to heap if pool is full
                        self.deallocate_to_heap(workspace);
                    }
                }
                WorkspaceSize::Large => {
                    // WHY: Large allocations go back to heap
                    // Like dismantling special banquet setup
                    self.deallocate_to_heap(workspace);
                }
            }
            freed_space += workspace.size;
        }
        
        CleanupResult { freed_space, workspaces_cleaned: stale_workspaces.len() }
    }
}
```

### 2. Connection Pooling (Like Restaurant Table Management)

Tables are expensive to set up and tear down. Connection pooling works like table management:

```rust
// Connection pooling like restaurant table management
pub struct RestaurantConnectionPool {
    // Like having different sections for different party sizes
    small_table_pool: TablePool,    // 2-4 people
    medium_table_pool: TablePool,   // 4-8 people
    large_table_pool: TablePool,    // 8+ people
    // Like tracking table status
    table_status: HashMap<TableId, TableStatus>,
    // Like having a host managing reservations
    reservation_manager: ReservationManager,
}

impl RestaurantConnectionPool {
    pub async fn get_table(&self, party_size: usize) -> Result<Table, ResourceError> {
        // WHY: Match party size to appropriate table
        // Like not seating a couple at a banquet table
        
        let table = match party_size {
            1..=4 => {
                // WHY: Try small tables first
                // Like prioritizing 4-tops for small parties
                self.small_table_pool.acquire().await
                    .or_else(|_| self.medium_table_pool.acquire())
                    .or_else(|_| self.large_table_pool.acquire())
            }
            5..=8 => {
                // WHY: Medium tables are best fit
                self.medium_table_pool.acquire().await
                    .or_else(|_| self.large_table_pool.acquire())
                    .or_else(|_| self.small_table_pool.acquire())
            }
            _ => {
                // WHY: Large parties need large tables
                self.large_table_pool.acquire().await
            }
        }?;
        
        // WHY: Track table assignment
        // Like host marking table as occupied
        self.table_status.insert(table.id, TableStatus::Occupied {
            party_size,
            seated_at: Instant::now(),
        });
        
        Ok(table)
    }
    
    pub async fn return_table(&self, table: Table) -> Result<(), ResourceError> {
        // WHY: Clean and prepare table for next party
        // Like bussing and resetting table
        
        // Mark table as cleaning
        self.table_status.insert(table.id, TableStatus::Cleaning);
        
        // Clean the table (reset connection state)
        let cleaned_table = self.clean_table(table).await?;
        
        // Return to appropriate pool
        match cleaned_table.size {
            TableSize::Small => self.small_table_pool.return_table(cleaned_table).await?,
            TableSize::Medium => self.medium_table_pool.return_table(cleaned_table).await?,
            TableSize::Large => self.large_table_pool.return_table(cleaned_table).await?,
        }
        
        // Update status
        self.table_status.insert(cleaned_table.id, TableStatus::Available);
        
        Ok(())
    }
    
    pub async fn health_check(&self) -> PoolHealthReport {
        // WHY: Regular health checks prevent service degradation
        // Like manager checking table cleanliness and staff performance
        
        let mut report = PoolHealthReport::new();
        
        // Check each pool
        for (pool_name, pool) in [
            ("small", &self.small_table_pool),
            ("medium", &self.medium_table_pool),
            ("large", &self.large_table_pool),
        ] {
            let pool_health = pool.check_health().await;
            
            // WHY: Identify problems early
            if pool_health.unhealthy_connections > 0 {
                warn!("Pool {} has {} unhealthy connections", pool_name, pool_health.unhealthy_connections);
                // Remove unhealthy connections
                pool.remove_unhealthy_connections().await;
            }
            
            report.add_pool_health(pool_name, pool_health);
        }
        
        report
    }
}
```

### 3. Common Resource Management Pitfalls

#### The "Hoarding" Problem (Not Releasing Resources)

**Wrong:**
```rust
// Dangerous - holding onto resources too long
pub struct ResourceHogger {
    database_connections: Vec<DbConnection>,
    file_handles: Vec<FileHandle>,
    memory_buffers: Vec<Buffer>,
}

impl ResourceHogger {
    pub fn process_requests(&mut self) {
        // Acquiring resources but never releasing them
        for _ in 0..100 {
            let conn = self.acquire_database_connection();
            let file = self.open_file("data.txt");
            let buffer = self.allocate_buffer(1024);
            
            // Use resources but never clean up
            self.database_connections.push(conn);
            self.file_handles.push(file);
            self.memory_buffers.push(buffer);
        }
        // All resources are held until struct is dropped!
    }
}
```

**Right:**
```rust
// Good - using RAII and proper resource lifecycle
pub struct ResourceManager {
    connection_pool: ConnectionPool,
    file_cache: FileCache,
    buffer_pool: BufferPool,
}

impl ResourceManager {
    pub async fn process_requests(&self) -> Result<(), ResourceError> {
        for _ in 0..100 {
            // WHY: Use scoped resource acquisition
            // Like getting a table, using it, and returning it
            {
                let conn = self.connection_pool.acquire().await?;
                let file = self.file_cache.get_file("data.txt").await?;
                let buffer = self.buffer_pool.get_buffer(1024).await?;
                
                // Use resources
                self.process_with_resources(&conn, &file, &buffer).await?;
                
                // WHY: Resources are automatically returned when scope ends
                // Like automatic table cleanup when customers leave
            } // conn, file, buffer are automatically returned here
        }
        Ok(())
    }
}
```

#### The "Infinite Growth" Problem (No Limits)

**Wrong:**
```rust
// Dangerous - no limits on resource usage
pub struct UnlimitedCache {
    cache: HashMap<String, CachedData>,
}

impl UnlimitedCache {
    pub fn cache_data(&mut self, key: String, data: CachedData) {
        // No limit checking - cache grows forever
        self.cache.insert(key, data);
    }
}
```

**Right:**
```rust
// Safe - bounded cache with eviction policy
pub struct BoundedCache {
    cache: HashMap<String, CachedData>,
    access_order: VecDeque<String>,
    max_size: usize,
    current_size: usize,
}

impl BoundedCache {
    pub fn cache_data(&mut self, key: String, data: CachedData) -> Result<(), ResourceError> {
        // WHY: Check limits before adding
        // Like checking restaurant capacity before seating guests
        
        if self.current_size >= self.max_size {
            // WHY: Evict least recently used items
            // Like asking early diners to leave when you need tables
            self.evict_lru_items(data.size)?;
        }
        
        // Add new data
        self.cache.insert(key.clone(), data);
        self.access_order.push_back(key);
        self.current_size += data.size;
        
        Ok(())
    }
    
    fn evict_lru_items(&mut self, space_needed: usize) -> Result<(), ResourceError> {
        let mut freed_space = 0;
        
        while freed_space < space_needed && !self.access_order.is_empty() {
            // WHY: Remove least recently used items first
            if let Some(oldest_key) = self.access_order.pop_front() {
                if let Some(data) = self.cache.remove(&oldest_key) {
                    freed_space += data.size;
                    self.current_size -= data.size;
                }
            }
        }
        
        if freed_space >= space_needed {
            Ok(())
        } else {
            Err(ResourceError::EvictionFailed)
        }
    }
}
```

## Integration with Reth's Resource Management

Understanding how Reth manages resources in practice:

```rust
// How Reth's DatabaseProvider manages resources efficiently
impl<TX: DbTx, N: NodeTypes> DatabaseProvider<TX, N> {
    /// Resource-efficient cursor management
    pub fn get_cursor<T: Table>(&self) -> Result<impl DbCursorRO<T>, DatabaseError> {
        // WHY: Cursors are expensive database resources
        // Like reserving a specific workstation in the kitchen
        
        let cursor = self.tx.cursor_read::<T>()?;
        
        // WHY: Track cursor for cleanup
        // Like keeping track of which workstations are in use
        self.active_cursors.insert(cursor.id(), cursor.clone());
        
        Ok(cursor)
    }
    
    /// Efficient batch processing with resource limits
    pub fn process_batch<T, F>(&self, items: Vec<T>, batch_size: usize, processor: F) -> Result<(), DatabaseError>
    where
        F: Fn(&[T]) -> Result<(), DatabaseError>,
    {
        // WHY: Process in batches to control memory usage
        // Like cooking orders in batches rather than all at once
        
        for batch in items.chunks(batch_size) {
            // WHY: Check resource pressure before each batch
            // Like checking if kitchen is getting overwhelmed
            if self.is_resource_pressure_high() {
                // Give system time to recover
                std::thread::sleep(Duration::from_millis(100));
                
                // Run cleanup if needed
                self.cleanup_if_needed()?;
            }
            
            // Process batch
            processor(batch)?;
            
            // WHY: Periodic cleanup prevents resource accumulation
            // Like cleaning as you go in the kitchen
            if self.should_cleanup() {
                self.cleanup_temporary_resources()?;
            }
        }
        
        Ok(())
    }
    
    fn is_resource_pressure_high(&self) -> bool {
        // WHY: Monitor multiple resource indicators
        // Like checking if kitchen is running out of space, clean dishes, etc.
        
        let memory_usage = self.get_memory_usage();
        let cursor_count = self.active_cursors.len();
        let cache_size = self.cache.len();
        
        memory_usage > MEMORY_PRESSURE_THRESHOLD ||
        cursor_count > MAX_ACTIVE_CURSORS ||
        cache_size > MAX_CACHE_SIZE
    }
    
    fn cleanup_if_needed(&self) -> Result<(), DatabaseError> {
        // WHY: Progressive cleanup based on resource pressure
        // Like different levels of kitchen cleaning based on how busy it is
        
        if self.is_resource_pressure_high() {
            // Immediate cleanup
            self.cleanup_temporary_resources()?;
            self.close_unused_cursors()?;
            self.evict_cache_entries(CACHE_EVICTION_RATIO)?;
        } else if self.should_cleanup() {
            // Routine cleanup
            self.cleanup_temporary_resources()?;
        }
        
        Ok(())
    }
}

## Resource Management Framework

```rust
use std::collections::HashMap;
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, Instant};
use std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};
use tokio::sync::Semaphore;

#[derive(Debug, Clone)]
pub struct ResourceManager {
    memory_manager: Arc<MemoryManager>,
    connection_pool: Arc<ConnectionPoolManager>,
    file_descriptor_manager: Arc<FileDescriptorManager>,
    thread_pool_manager: Arc<ThreadPoolManager>,
    resource_limits: ResourceLimits,
    metrics_collector: Arc<ResourceMetricsCollector>,
}

impl ResourceManager {
    pub fn new(config: ResourceConfig) -> Self {
        Self {
            memory_manager: Arc::new(MemoryManager::new(config.memory_config)),
            connection_pool: Arc::new(ConnectionPoolManager::new(config.connection_config)),
            file_descriptor_manager: Arc::new(FileDescriptorManager::new(config.fd_config)),
            thread_pool_manager: Arc::new(ThreadPoolManager::new(config.thread_config)),
            resource_limits: config.resource_limits,
            metrics_collector: Arc::new(ResourceMetricsCollector::new()),
        }
    }
    
    pub async fn allocate_resource(&self, resource_type: ResourceType, size: usize) -> Result<ResourceHandle, ResourceError> {
        // Check resource limits
        self.check_resource_limits(&resource_type, size).await?;
        
        // Allocate based on type
        let handle = match resource_type {
            ResourceType::Memory => {
                self.memory_manager.allocate(size).await?
            }
            ResourceType::Connection => {
                self.connection_pool.acquire().await?
            }
            ResourceType::FileDescriptor => {
                self.file_descriptor_manager.allocate().await?
            }
            ResourceType::Thread => {
                self.thread_pool_manager.allocate().await?
            }
        };
        
        // Record allocation metrics
        self.metrics_collector.record_allocation(&resource_type, size).await;
        
        Ok(handle)
    }
    
    pub async fn deallocate_resource(&self, handle: ResourceHandle) -> Result<(), ResourceError> {
        // Record deallocation metrics
        self.metrics_collector.record_deallocation(&handle.resource_type, handle.size).await;
        
        // Deallocate based on type
        match handle.resource_type {
            ResourceType::Memory => {
                self.memory_manager.deallocate(handle).await?;
            }
            ResourceType::Connection => {
                self.connection_pool.release(handle).await?;
            }
            ResourceType::FileDescriptor => {
                self.file_descriptor_manager.deallocate(handle).await?;
            }
            ResourceType::Thread => {
                self.thread_pool_manager.deallocate(handle).await?;
            }
        }
        
        Ok(())
    }
    
    pub async fn monitor_resource_usage(&self) -> Result<ResourceUsageReport, ResourceError> {
        let memory_usage = self.memory_manager.get_usage_stats().await?;
        let connection_usage = self.connection_pool.get_usage_stats().await?;
        let fd_usage = self.file_descriptor_manager.get_usage_stats().await?;
        let thread_usage = self.thread_pool_manager.get_usage_stats().await?;
        
        Ok(ResourceUsageReport {
            memory_usage,
            connection_usage,
            fd_usage,
            thread_usage,
            timestamp: Instant::now(),
        })
    }
    
    pub async fn cleanup_resources(&self) -> Result<CleanupResult, ResourceError> {
        let mut cleanup_result = CleanupResult::new();
        
        // Cleanup memory
        let memory_cleanup = self.memory_manager.cleanup().await?;
        cleanup_result.add_memory_cleanup(memory_cleanup);
        
        // Cleanup connections
        let connection_cleanup = self.connection_pool.cleanup().await?;
        cleanup_result.add_connection_cleanup(connection_cleanup);
        
        // Cleanup file descriptors
        let fd_cleanup = self.file_descriptor_manager.cleanup().await?;
        cleanup_result.add_fd_cleanup(fd_cleanup);
        
        // Cleanup threads
        let thread_cleanup = self.thread_pool_manager.cleanup().await?;
        cleanup_result.add_thread_cleanup(thread_cleanup);
        
        Ok(cleanup_result)
    }
    
    async fn check_resource_limits(&self, resource_type: &ResourceType, size: usize) -> Result<(), ResourceError> {
        match resource_type {
            ResourceType::Memory => {
                if self.memory_manager.get_used_memory().await + size > self.resource_limits.max_memory {
                    return Err(ResourceError::MemoryLimitExceeded);
                }
            }
            ResourceType::Connection => {
                if self.connection_pool.get_active_connections().await >= self.resource_limits.max_connections {
                    return Err(ResourceError::ConnectionLimitExceeded);
                }
            }
            ResourceType::FileDescriptor => {
                if self.file_descriptor_manager.get_open_fds().await >= self.resource_limits.max_file_descriptors {
                    return Err(ResourceError::FileDescriptorLimitExceeded);
                }
            }
            ResourceType::Thread => {
                if self.thread_pool_manager.get_active_threads().await >= self.resource_limits.max_threads {
                    return Err(ResourceError::ThreadLimitExceeded);
                }
            }
        }
        
        Ok(())
    }
}

pub struct MemoryManager {
    allocations: Arc<RwLock<HashMap<String, MemoryAllocation>>>,
    total_allocated: AtomicU64,
    allocation_counter: AtomicU64,
    memory_pools: Vec<MemoryPool>,
    gc_threshold: usize,
    fragmentation_threshold: f64,
}

impl MemoryManager {
    pub fn new(config: MemoryConfig) -> Self {
        Self {
            allocations: Arc::new(RwLock::new(HashMap::new())),
            total_allocated: AtomicU64::new(0),
            allocation_counter: AtomicU64::new(0),
            memory_pools: Self::create_memory_pools(config.pool_sizes),
            gc_threshold: config.gc_threshold,
            fragmentation_threshold: config.fragmentation_threshold,
        }
    }
    
    pub async fn allocate(&self, size: usize) -> Result<ResourceHandle, ResourceError> {
        // Try to allocate from appropriate pool
        if let Some(pool) = self.find_suitable_pool(size) {
            if let Some(allocation) = pool.allocate().await? {
                return Ok(self.create_handle(allocation));
            }
        }
        
        // Fall back to heap allocation
        let allocation_id = self.allocation_counter.fetch_add(1, Ordering::SeqCst).to_string();
        let allocation = MemoryAllocation {
            id: allocation_id.clone(),
            size,
            allocated_at: Instant::now(),
            last_accessed: Instant::now(),
            access_count: 1,
            allocation_type: AllocationType::Heap,
        };
        
        // Record allocation
        {
            let mut allocations = self.allocations.write().unwrap();
            allocations.insert(allocation_id.clone(), allocation);
        }
        
        self.total_allocated.fetch_add(size as u64, Ordering::SeqCst);
        
        // Check if GC is needed
        if self.should_trigger_gc().await {
            self.trigger_gc().await?;
        }
        
        Ok(ResourceHandle {
            id: allocation_id,
            resource_type: ResourceType::Memory,
            size,
            allocated_at: Instant::now(),
        })
    }
    
    pub async fn deallocate(&self, handle: ResourceHandle) -> Result<(), ResourceError> {
        let allocation = {
            let mut allocations = self.allocations.write().unwrap();
            allocations.remove(&handle.id)
        };
        
        if let Some(allocation) = allocation {
            self.total_allocated.fetch_sub(allocation.size as u64, Ordering::SeqCst);
            
            // Return to pool if applicable
            if allocation.allocation_type == AllocationType::Pool {
                if let Some(pool) = self.find_pool_for_size(allocation.size) {
                    pool.deallocate(allocation).await?;
                }
            }
        }
        
        Ok(())
    }
    
    pub async fn get_usage_stats(&self) -> Result<MemoryUsageStats, ResourceError> {
        let allocations = self.allocations.read().unwrap();
        let total_allocated = self.total_allocated.load(Ordering::SeqCst);
        let allocation_count = allocations.len();
        
        // Calculate fragmentation
        let fragmentation = self.calculate_fragmentation(&allocations);
        
        // Get pool statistics
        let pool_stats = self.get_pool_stats().await?;
        
        Ok(MemoryUsageStats {
            total_allocated,
            allocation_count,
            fragmentation,
            pool_stats,
            gc_runs: 0, // Would track GC runs
        })
    }
    
    pub async fn cleanup(&self) -> Result<MemoryCleanupResult, ResourceError> {
        let mut cleanup_result = MemoryCleanupResult::new();
        
        // Clean up unused allocations
        let stale_allocations = self.find_stale_allocations().await?;
        for allocation_id in stale_allocations {
            if let Some(allocation) = self.allocations.write().unwrap().remove(&allocation_id) {
                self.total_allocated.fetch_sub(allocation.size as u64, Ordering::SeqCst);
                cleanup_result.freed_memory += allocation.size;
                cleanup_result.freed_allocations += 1;
            }
        }
        
        // Defragment memory pools
        let defrag_result = self.defragment_pools().await?;
        cleanup_result.add_defragmentation_result(defrag_result);
        
        // Run garbage collection
        let gc_result = self.run_gc().await?;
        cleanup_result.add_gc_result(gc_result);
        
        Ok(cleanup_result)
    }
    
    async fn should_trigger_gc(&self) -> bool {
        let total_allocated = self.total_allocated.load(Ordering::SeqCst);
        total_allocated as usize > self.gc_threshold
    }
    
    async fn trigger_gc(&self) -> Result<(), ResourceError> {
        // Implement garbage collection logic
        // - Find unreferenced allocations
        // - Compact memory pools
        // - Update fragmentation metrics
        Ok(())
    }
    
    fn create_memory_pools(pool_sizes: Vec<usize>) -> Vec<MemoryPool> {
        pool_sizes.into_iter().map(|size| MemoryPool::new(size, 100)).collect()
    }
    
    fn find_suitable_pool(&self, size: usize) -> Option<&MemoryPool> {
        self.memory_pools.iter().find(|pool| pool.can_allocate(size))
    }
    
    fn find_pool_for_size(&self, size: usize) -> Option<&MemoryPool> {
        self.memory_pools.iter().find(|pool| pool.block_size() == size)
    }
    
    fn calculate_fragmentation(&self, allocations: &HashMap<String, MemoryAllocation>) -> f64 {
        // Calculate memory fragmentation percentage
        if allocations.is_empty() {
            return 0.0;
        }
        
        // Simplified fragmentation calculation
        let total_size: usize = allocations.values().map(|a| a.size).sum();
        let avg_size = total_size / allocations.len();
        let variance: f64 = allocations.values()
            .map(|a| (a.size as f64 - avg_size as f64).powi(2))
            .sum::<f64>() / allocations.len() as f64;
        
        (variance.sqrt() / avg_size as f64) * 100.0
    }
    
    async fn find_stale_allocations(&self) -> Result<Vec<String>, ResourceError> {
        let stale_threshold = Duration::from_secs(3600); // 1 hour
        let now = Instant::now();
        
        let allocations = self.allocations.read().unwrap();
        let stale_allocations: Vec<String> = allocations.iter()
            .filter(|(_, allocation)| {
                now.duration_since(allocation.last_accessed) > stale_threshold
            })
            .map(|(id, _)| id.clone())
            .collect();
        
        Ok(stale_allocations)
    }
    
    async fn defragment_pools(&self) -> Result<DefragmentationResult, ResourceError> {
        let mut result = DefragmentationResult::new();
        
        for pool in &self.memory_pools {
            let pool_result = pool.defragment().await?;
            result.add_pool_result(pool_result);
        }
        
        Ok(result)
    }
    
    async fn run_gc(&self) -> Result<GcResult, ResourceError> {
        // Implement garbage collection
        Ok(GcResult::new())
    }
    
    async fn get_pool_stats(&self) -> Result<Vec<PoolStats>, ResourceError> {
        let mut stats = Vec::new();
        
        for pool in &self.memory_pools {
            let pool_stats = pool.get_stats().await?;
            stats.push(pool_stats);
        }
        
        Ok(stats)
    }
    
    fn create_handle(&self, allocation: MemoryAllocation) -> ResourceHandle {
        ResourceHandle {
            id: allocation.id.clone(),
            resource_type: ResourceType::Memory,
            size: allocation.size,
            allocated_at: allocation.allocated_at,
        }
    }
    
    pub async fn get_used_memory(&self) -> usize {
        self.total_allocated.load(Ordering::SeqCst) as usize
    }
}

pub struct ConnectionPoolManager {
    pools: HashMap<String, ConnectionPool>,
    connection_factory: Arc<dyn ConnectionFactory>,
    pool_config: PoolConfig,
    metrics: Arc<ConnectionMetrics>,
}

impl ConnectionPoolManager {
    pub fn new(config: ConnectionConfig) -> Self {
        Self {
            pools: HashMap::new(),
            connection_factory: Arc::new(DefaultConnectionFactory::new()),
            pool_config: config.pool_config,
            metrics: Arc::new(ConnectionMetrics::new()),
        }
    }
    
    pub async fn acquire(&self) -> Result<ResourceHandle, ResourceError> {
        let pool_name = "default"; // Would determine pool based on connection type
        
        let pool = self.pools.get(pool_name)
            .ok_or(ResourceError::PoolNotFound(pool_name.to_string()))?;
        
        let connection = pool.acquire().await?;
        
        // Record metrics
        self.metrics.record_acquisition().await;
        
        Ok(ResourceHandle {
            id: connection.id.clone(),
            resource_type: ResourceType::Connection,
            size: 1, // Connection count
            allocated_at: Instant::now(),
        })
    }
    
    pub async fn release(&self, handle: ResourceHandle) -> Result<(), ResourceError> {
        let pool_name = "default"; // Would determine pool from handle
        
        let pool = self.pools.get(pool_name)
            .ok_or(ResourceError::PoolNotFound(pool_name.to_string()))?;
        
        pool.release(handle).await?;
        
        // Record metrics
        self.metrics.record_release().await;
        
        Ok(())
    }
    
    pub async fn get_usage_stats(&self) -> Result<ConnectionUsageStats, ResourceError> {
        let mut total_active = 0;
        let mut total_idle = 0;
        let mut total_max = 0;
        
        for pool in self.pools.values() {
            let stats = pool.get_stats().await?;
            total_active += stats.active_connections;
            total_idle += stats.idle_connections;
            total_max += stats.max_connections;
        }
        
        Ok(ConnectionUsageStats {
            active_connections: total_active,
            idle_connections: total_idle,
            max_connections: total_max,
            pool_count: self.pools.len(),
        })
    }
    
    pub async fn cleanup(&self) -> Result<ConnectionCleanupResult, ResourceError> {
        let mut cleanup_result = ConnectionCleanupResult::new();
        
        for pool in self.pools.values() {
            let pool_cleanup = pool.cleanup().await?;
            cleanup_result.add_pool_cleanup(pool_cleanup);
        }
        
        Ok(cleanup_result)
    }
    
    pub async fn get_active_connections(&self) -> usize {
        self.pools.values()
            .map(|pool| pool.active_count())
            .sum()
    }
}

pub struct FileDescriptorManager {
    open_fds: Arc<RwLock<HashMap<String, FileDescriptor>>>,
    fd_counter: AtomicU64,
    max_fds: usize,
    cleanup_threshold: usize,
}

impl FileDescriptorManager {
    pub fn new(config: FileDescriptorConfig) -> Self {
        Self {
            open_fds: Arc::new(RwLock::new(HashMap::new())),
            fd_counter: AtomicU64::new(0),
            max_fds: config.max_fds,
            cleanup_threshold: config.cleanup_threshold,
        }
    }
    
    pub async fn allocate(&self) -> Result<ResourceHandle, ResourceError> {
        // Check if we're at the limit
        if self.get_open_fds().await >= self.max_fds {
            // Try cleanup first
            self.cleanup_stale_fds().await?;
            
            // Check again
            if self.get_open_fds().await >= self.max_fds {
                return Err(ResourceError::FileDescriptorLimitExceeded);
            }
        }
        
        let fd_id = self.fd_counter.fetch_add(1, Ordering::SeqCst).to_string();
        let fd = FileDescriptor {
            id: fd_id.clone(),
            opened_at: Instant::now(),
            last_accessed: Instant::now(),
            access_count: 1,
            file_path: String::new(), // Would be set based on actual file
        };
        
        {
            let mut open_fds = self.open_fds.write().unwrap();
            open_fds.insert(fd_id.clone(), fd);
        }
        
        Ok(ResourceHandle {
            id: fd_id,
            resource_type: ResourceType::FileDescriptor,
            size: 1,
            allocated_at: Instant::now(),
        })
    }
    
    pub async fn deallocate(&self, handle: ResourceHandle) -> Result<(), ResourceError> {
        let mut open_fds = self.open_fds.write().unwrap();
        open_fds.remove(&handle.id);
        
        Ok(())
    }
    
    pub async fn get_usage_stats(&self) -> Result<FileDescriptorUsageStats, ResourceError> {
        let open_fds = self.open_fds.read().unwrap();
        
        Ok(FileDescriptorUsageStats {
            open_fds: open_fds.len(),
            max_fds: self.max_fds,
            utilization: (open_fds.len() as f64 / self.max_fds as f64) * 100.0,
        })
    }
    
    pub async fn cleanup(&self) -> Result<FileDescriptorCleanupResult, ResourceError> {
        let cleanup_result = self.cleanup_stale_fds().await?;
        Ok(cleanup_result)
    }
    
    async fn cleanup_stale_fds(&self) -> Result<FileDescriptorCleanupResult, ResourceError> {
        let stale_threshold = Duration::from_secs(300); // 5 minutes
        let now = Instant::now();
        
        let mut cleanup_result = FileDescriptorCleanupResult::new();
        
        {
            let mut open_fds = self.open_fds.write().unwrap();
            let stale_fds: Vec<String> = open_fds.iter()
                .filter(|(_, fd)| now.duration_since(fd.last_accessed) > stale_threshold)
                .map(|(id, _)| id.clone())
                .collect();
            
            for fd_id in stale_fds {
                open_fds.remove(&fd_id);
                cleanup_result.closed_fds += 1;
            }
        }
        
        Ok(cleanup_result)
    }
    
    pub async fn get_open_fds(&self) -> usize {
        self.open_fds.read().unwrap().len()
    }
}

#[derive(Debug, Clone, PartialEq)]
pub enum ResourceType {
    Memory,
    Connection,
    FileDescriptor,
    Thread,
}

#[derive(Debug, Clone)]
pub struct ResourceHandle {
    pub id: String,
    pub resource_type: ResourceType,
    pub size: usize,
    pub allocated_at: Instant,
}

#[derive(Debug, Clone)]
pub struct ResourceLimits {
    pub max_memory: usize,
    pub max_connections: usize,
    pub max_file_descriptors: usize,
    pub max_threads: usize,
}

#[derive(Debug, Clone)]
pub struct MemoryAllocation {
    pub id: String,
    pub size: usize,
    pub allocated_at: Instant,
    pub last_accessed: Instant,
    pub access_count: u64,
    pub allocation_type: AllocationType,
}

#[derive(Debug, Clone, PartialEq)]
pub enum AllocationType {
    Heap,
    Pool,
    Stack,
}

#[derive(Debug, Clone)]
pub struct FileDescriptor {
    pub id: String,
    pub opened_at: Instant,
    pub last_accessed: Instant,
    pub access_count: u64,
    pub file_path: String,
}

#[derive(Debug)]
pub enum ResourceError {
    MemoryLimitExceeded,
    ConnectionLimitExceeded,
    FileDescriptorLimitExceeded,
    ThreadLimitExceeded,
    PoolNotFound(String),
    AllocationFailed(String),
    DeallocationFailed(String),
    CleanupFailed(String),
}

impl std::fmt::Display for ResourceError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            ResourceError::MemoryLimitExceeded => write!(f, "Memory limit exceeded"),
            ResourceError::ConnectionLimitExceeded => write!(f, "Connection limit exceeded"),
            ResourceError::FileDescriptorLimitExceeded => write!(f, "File descriptor limit exceeded"),
            ResourceError::ThreadLimitExceeded => write!(f, "Thread limit exceeded"),
            ResourceError::PoolNotFound(pool) => write!(f, "Pool not found: {}", pool),
            ResourceError::AllocationFailed(msg) => write!(f, "Allocation failed: {}", msg),
            ResourceError::DeallocationFailed(msg) => write!(f, "Deallocation failed: {}", msg),
            ResourceError::CleanupFailed(msg) => write!(f, "Cleanup failed: {}", msg),
        }
    }
}

impl std::error::Error for ResourceError {}

// Supporting types and result structures
pub struct ResourceConfig;
pub struct MemoryConfig;
pub struct ConnectionConfig;
pub struct FileDescriptorConfig;
pub struct ThreadConfig;
pub struct ResourceUsageReport;
pub struct CleanupResult;
pub struct MemoryUsageStats;
pub struct ConnectionUsageStats;
pub struct FileDescriptorUsageStats;
pub struct ThreadUsageStats;
pub struct MemoryCleanupResult;
pub struct ConnectionCleanupResult;
pub struct FileDescriptorCleanupResult;
pub struct ThreadCleanupResult;
pub struct ResourceMetricsCollector;
pub struct MemoryPool;
pub struct ConnectionPool;
pub struct ThreadPoolManager;
pub struct ConnectionFactory;
pub struct DefaultConnectionFactory;
pub struct PoolConfig;
pub struct ConnectionMetrics;
pub struct PoolStats;
pub struct DefragmentationResult;
pub struct GcResult;

// Stub implementations for supporting types
impl ResourceConfig {
    pub fn new() -> Self { Self }
    pub memory_config: MemoryConfig,
    pub connection_config: ConnectionConfig,
    pub fd_config: FileDescriptorConfig,
    pub thread_config: ThreadConfig,
    pub resource_limits: ResourceLimits,
}

impl MemoryConfig {
    pub fn new() -> Self { Self }
    pub pool_sizes: Vec<usize>,
    pub gc_threshold: usize,
    pub fragmentation_threshold: f64,
}

impl ResourceUsageReport {
    pub memory_usage: MemoryUsageStats,
    pub connection_usage: ConnectionUsageStats,
    pub fd_usage: FileDescriptorUsageStats,
    pub thread_usage: ThreadUsageStats,
    pub timestamp: Instant,
}

impl CleanupResult {
    pub fn new() -> Self { Self }
    pub fn add_memory_cleanup(&mut self, _cleanup: MemoryCleanupResult) {}
    pub fn add_connection_cleanup(&mut self, _cleanup: ConnectionCleanupResult) {}
    pub fn add_fd_cleanup(&mut self, _cleanup: FileDescriptorCleanupResult) {}
    pub fn add_thread_cleanup(&mut self, _cleanup: ThreadCleanupResult) {}
}

impl MemoryCleanupResult {
    pub fn new() -> Self { Self { freed_memory: 0, freed_allocations: 0 } }
    pub fn add_defragmentation_result(&mut self, _result: DefragmentationResult) {}
    pub fn add_gc_result(&mut self, _result: GcResult) {}
    pub freed_memory: usize,
    pub freed_allocations: usize,
}

impl FileDescriptorCleanupResult {
    pub fn new() -> Self { Self { closed_fds: 0 } }
    pub closed_fds: usize,
}

impl ResourceMetricsCollector {
    pub fn new() -> Self { Self }
    pub async fn record_allocation(&self, _resource_type: &ResourceType, _size: usize) {}
    pub async fn record_deallocation(&self, _resource_type: &ResourceType, _size: usize) {}
}
```

## Summary
Resource management ensures efficient use of system resources through proper allocation, monitoring, and cleanup. Effective resource management prevents leaks, reduces fragmentation, and maintains system stability.

## Assignments
1. **Resource Manager**: Build comprehensive resource management system
2. **Memory Pool**: Implement efficient memory pooling with cleanup
3. **Connection Pool**: Create connection pooling with health monitoring

## Questions to Ponder
1. How do you prevent resource leaks in async systems?
2. What metrics indicate resource management issues?
3. How do you balance resource utilization with performance?
4. What cleanup strategies work best for different resource types?
5. How do you handle resource contention in multi-threaded applications?