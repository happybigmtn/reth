# Lesson 49: Account History Indexing

*"There is no harm in doubt and skepticism, for it is through these that new discoveries are made." - Richard Feynman*

## Files with Inline Comments for This Lesson
- `crates/stages/stages/src/stages/index_account_history.rs` - Account history stage
- `crates/stages/stages/src/stages/index_storage_history.rs` - Storage history stage
- `crates/stages/stages/src/stages/utils.rs` - History indexing utilities
- `crates/db-api/src/models/sharded_key.rs` - Sharded key structure
- `crates/db-api/src/tables/mod.rs` - AccountsHistory table

## What is Account History Indexing?

The account history stage creates an index that tracks when each account was modified. This enables historical queries like "What was this account's balance at block 1,000,000?" Without this index, answering such queries would require replaying all transactions from genesis.

```
Account History Index:
┌─────────────────────────────────────────────────┐
│          Account Changes Over Time              │
│  Block 100: Account A created (balance: 10)    │
│  Block 200: Account A updated (balance: 50)    │
│  Block 300: Account B created (balance: 20)    │
│  Block 400: Account A updated (balance: 30)    │
│  Block 500: Account B updated (balance: 0)     │
└─────────────────────┬───────────────────────────┘
                      │ Index by account
┌─────────────────────▼───────────────────────────┐
│            Account History Index                │
│  Account A → [100, 200, 400]                   │
│  Account B → [300, 500]                        │
│                                                 │
│  Query: "Account A at block 350?"              │
│  Answer: Look at block 200 (last change)       │
└─────────────────────────────────────────────────┘
```

## The Account History Stage

The stage processes account changesets and builds the index:

```rust
/// Account history indexing stage
/// Located in: crates/stages/stages/src/stages/index_account_history.rs

use reth_db_api::{models::ShardedKey, tables};
use reth_stages_api::{Stage, ExecInput, ExecOutput, StageError};

/// LESSON 49: Account History Stage
/// Indexes when accounts were modified for historical queries
#[derive(Debug)]
pub struct IndexAccountHistoryStage {
    /// Blocks to process before committing
    pub commit_threshold: u64,
    /// Optional pruning configuration
    pub prune_mode: Option<PruneMode>,
    /// ETL configuration for batch processing
    pub etl_config: EtlConfig,
}

impl<Provider> Stage<Provider> for IndexAccountHistoryStage {
    fn execute(&mut self, provider: &Provider, mut input: ExecInput) -> Result<ExecOutput, StageError> {
        // LESSON 49: Pruning Setup
        // Skip already pruned history if configured
        if let Some((prune_target, prune_mode)) = self.calculate_prune_target(&input)? {
            if prune_target > input.checkpoint().block_number {
                input.checkpoint = Some(StageCheckpoint::new(prune_target));
                self.save_prune_checkpoint(provider, prune_target, prune_mode)?;
            }
        }
        
        if input.target_reached() {
            return Ok(ExecOutput::done(input.checkpoint()));
        }
        
        let mut range = input.next_block_range();
        let first_sync = input.checkpoint().block_number == 0;
        
        // LESSON 49: First Sync Optimization
        // Clear table on first sync - faster to rebuild
        if first_sync {
            provider.tx_ref().clear::<tables::AccountsHistory>()?;
            range = 0..=*input.next_block_range().end();
        }
        
        info!(target: "sync::stages::index_account_history", ?first_sync, "Collecting indices");
        
        // LESSON 49: ETL Collection
        // Collect all account changes in the range
        let collector = collect_history_indices::<
            _,
            tables::AccountChangeSets,
            tables::AccountsHistory,
            _
        >(
            provider,
            range.clone(),
            ShardedKey::new,  // Shard key constructor
            |(block_number, changeset)| (block_number, changeset.address),
            &self.etl_config,
        )?;
        
        info!(target: "sync::stages::index_account_history", "Loading indices into database");
        
        // LESSON 49: Load Into Database
        // Write collected indices using sharding
        load_history_indices::<_, tables::AccountsHistory, _>(
            provider,
            collector,
            first_sync,
            ShardedKey::new,
            ShardedKey::<Address>::decode,
            |key| key.key,  // Extract address from sharded key
        )?;
        
        Ok(ExecOutput {
            checkpoint: StageCheckpoint::new(*range.end()),
            done: true,
        })
    }
}
```

## Sharded Keys for Efficient Storage

History is stored in shards to avoid huge lists:

```rust
/// LESSON 49: Sharded Key Structure
/// Located in: crates/db-api/src/models/sharded_key.rs

/// Key split into shards to avoid unbounded growth
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct ShardedKey<T> {
    /// The actual key (address or storage key)
    pub key: T,
    /// Highest block number in this shard
    pub highest_block_number: BlockNumber,
}

/// LESSON 49: Shard Size
/// Each shard holds this many block numbers
pub const NUM_OF_INDICES_IN_SHARD: usize = 2_000;

impl<T> ShardedKey<T> {
    /// Create a new sharded key
    pub fn new(key: T, block_number: BlockNumber) -> Self {
        Self {
            key,
            // Use max value for the last/active shard
            highest_block_number: if block_number == u64::MAX {
                u64::MAX
            } else {
                block_number
            },
        }
    }
    
    /// LESSON 49: Shard Selection
    /// Determine which shard a block belongs to
    pub fn shard_index_for_block(block_number: BlockNumber) -> u64 {
        if block_number == 0 {
            return u64::MAX; // Genesis goes to last shard
        }
        
        // Calculate shard based on block number
        let shard = block_number / NUM_OF_INDICES_IN_SHARD as u64;
        
        // Last block in shard
        shard * NUM_OF_INDICES_IN_SHARD as u64 + (NUM_OF_INDICES_IN_SHARD as u64 - 1)
    }
}
```

## History Collection Process

The ETL collector gathers changes efficiently:

```rust
/// LESSON 49: History Collection
/// Located in: crates/stages/stages/src/stages/utils.rs

pub fn collect_history_indices<Provider, CS, H, F>(
    provider: &Provider,
    range: RangeInclusive<BlockNumber>,
    sharded_key_factory: F,
    changeset_mapper: impl Fn((BlockNumber, CS::Value)) -> (BlockNumber, Address),
    etl_config: &EtlConfig,
) -> Result<Collector<ShardedKey<Address>, BlockNumberList>, StageError>
where
    Provider: DBProvider,
    CS: Table<Key = BlockNumber>,
    F: Fn(Address, BlockNumber) -> ShardedKey<Address>,
{
    let mut collector = Collector::new(etl_config.file_size, etl_config.dir.clone());
    
    // LESSON 49: Changeset Iteration
    // Walk through all changesets in the range
    let mut changeset_cursor = provider.tx_ref().cursor_read::<CS>()?;
    
    for entry in changeset_cursor.walk_range(range)? {
        let (block_number, changeset_value) = entry?;
        let (_, address) = changeset_mapper((block_number, changeset_value));
        
        // LESSON 49: ETL Collection
        // Collect address → block number mapping
        let sharded_key = sharded_key_factory(address, block_number);
        collector.insert(sharded_key, BlockNumberList::new([block_number])?)?;
    }
    
    Ok(collector)
}
```

## Loading History Indices

The collected data is loaded with sharding logic:

```rust
/// LESSON 49: Loading Indices
pub fn load_history_indices<Provider, H, F>(
    provider: &Provider,
    mut collector: Collector<ShardedKey<Address>, BlockNumberList>,
    first_sync: bool,
    sharded_key_factory: F,
    decode_key: impl Fn(&[u8]) -> Result<ShardedKey<Address>, DatabaseError>,
    get_address: impl Fn(&ShardedKey<Address>) -> Address,
) -> Result<(), StageError>
where
    Provider: DBProvider,
    H: Table<Key = ShardedKey<Address>, Value = BlockNumberList>,
    F: Fn(Address, BlockNumber) -> ShardedKey<Address>,
{
    let tx = provider.tx_ref();
    let mut history_cursor = tx.cursor_write::<H>()?;
    
    let total_entries = collector.len();
    let interval = (total_entries / 100).max(1);
    
    for (index, entry) in collector.iter()?.enumerate() {
        let (sharded_key_bytes, block_list_bytes) = entry?;
        
        // LESSON 49: Progress Reporting
        if index % interval == 0 {
            info!(
                target: "sync::stages::index_history",
                progress = %format!("{:.2}%", (index as f64 / total_entries as f64) * 100.0),
                "Loading history indices"
            );
        }
        
        let sharded_key = decode_key(&sharded_key_bytes)?;
        let new_blocks = BlockNumberList::decode(&block_list_bytes)?;
        
        // LESSON 49: Shard Management
        // Check if we need to handle shard overflow
        if let Some((_, existing_list)) = history_cursor.seek_exact(&sharded_key)? {
            let mut combined = existing_list.iter().collect::<Vec<_>>();
            combined.extend(new_blocks.iter());
            
            // Check if shard is full
            if combined.len() > NUM_OF_INDICES_IN_SHARD {
                // LESSON 49: Shard Splitting
                // Split into multiple shards
                let chunks = combined.chunks(NUM_OF_INDICES_IN_SHARD);
                
                for chunk in chunks {
                    let highest_block = *chunk.last().unwrap();
                    let shard_key = sharded_key_factory(
                        get_address(&sharded_key),
                        highest_block
                    );
                    
                    history_cursor.upsert(
                        shard_key,
                        BlockNumberList::new(chunk)?
                    )?;
                }
            } else {
                // Update existing shard
                history_cursor.upsert(
                    sharded_key,
                    BlockNumberList::new(&combined)?
                )?;
            }
        } else if first_sync {
            // Fast path for initial sync
            history_cursor.append(sharded_key, new_blocks)?;
        } else {
            history_cursor.upsert(sharded_key, new_blocks)?;
        }
    }
    
    Ok(())
}
```

## Unwinding History

During reorgs, remove history entries:

```rust
impl IndexAccountHistoryStage {
    fn unwind(&mut self, provider: &Provider, input: UnwindInput) -> Result<UnwindOutput, StageError> {
        let (range, unwind_progress, _) = input.unwind_block_range_with_threshold(self.commit_threshold);
        
        // LESSON 49: History Unwind
        // Remove account history entries in the range
        provider.unwind_account_history_indices_range(range)?;
        
        Ok(UnwindOutput {
            checkpoint: StageCheckpoint::new(unwind_progress),
        })
    }
}

impl HistoryWriter for DatabaseProvider {
    fn unwind_account_history_indices_range(
        &self,
        range: RangeInclusive<BlockNumber>,
    ) -> Result<(), ProviderError> {
        // LESSON 49: Unwind Implementation
        // Walk through history entries and remove block numbers
        let mut cursor = self.tx_ref().cursor_write::<tables::AccountsHistory>()?;
        
        for (sharded_key, mut list) in cursor.walk_range(..)? {
            // Remove block numbers in the unwind range
            let original_len = list.len();
            list.retain(|&block| !range.contains(&block));
            
            if list.len() < original_len {
                if list.is_empty() {
                    // Remove empty shard
                    cursor.delete_current()?;
                } else {
                    // Update shard with remaining blocks
                    cursor.upsert(sharded_key, list)?;
                }
            }
        }
        
        Ok(())
    }
}
```

## Summary

The account history indexing stage creates an efficient index for historical account queries. It uses sharding to prevent unbounded list growth and ETL for batch processing. This index is essential for RPC methods like `eth_getBalance` and `eth_getTransactionCount` when queried with historical block numbers.

## Assignments

### Assignment 1: Shard Calculator
Create a function that calculates shard statistics:

```rust
fn calculate_shard_stats(address: Address, block_numbers: &[BlockNumber]) -> ShardStats {
    // TODO: Calculate number of shards, average fill, etc.
}

struct ShardStats {
    shard_count: usize,
    average_fill: f64,
    full_shards: usize,
}
```

### Assignment 2: History Query
Implement a function to find account state at a block:

```rust
fn find_account_at_block(
    provider: &impl DatabaseProvider,
    address: Address,
    target_block: BlockNumber,
) -> Result<Option<BlockNumber>, ProviderError> {
    // TODO: Query history index to find last change before target
}
```

### Assignment 3: History Compactor
Create a tool to compact sparse history shards:

```rust
struct HistoryCompactor {
    min_shard_fill: f64,
}

impl HistoryCompactor {
    fn should_compact(&self, shard_entries: usize) -> bool {
        // TODO: Decide if shard should be compacted
    }
    
    fn compact_shards(&self, shards: Vec<BlockNumberList>) -> Vec<BlockNumberList> {
        // TODO: Merge sparse shards
    }
}
```

## Questions to Ponder

1. Why does the history index use sharding instead of simple lists?
2. How does pruning affect historical queries?
3. What's the trade-off between shard size and query performance?
4. Why clear the table on first sync instead of merging?
5. How does this index enable efficient archive node queries?

## Assignment Answers

### Answer 1: Shard Calculator

```rust
use std::collections::HashMap;

fn calculate_shard_stats(address: Address, block_numbers: &[BlockNumber]) -> ShardStats {
    if block_numbers.is_empty() {
        return ShardStats {
            shard_count: 0,
            average_fill: 0.0,
            full_shards: 0,
        };
    }
    
    // Group blocks by shard
    let mut shards: HashMap<u64, Vec<BlockNumber>> = HashMap::new();
    
    for &block in block_numbers {
        let shard_index = ShardedKey::<Address>::shard_index_for_block(block);
        shards.entry(shard_index).or_default().push(block);
    }
    
    let shard_count = shards.len();
    let full_shards = shards
        .values()
        .filter(|blocks| blocks.len() >= NUM_OF_INDICES_IN_SHARD)
        .count();
    
    let total_entries: usize = shards.values().map(|v| v.len()).sum();
    let average_fill = total_entries as f64 / shard_count as f64;
    
    ShardStats {
        shard_count,
        average_fill,
        full_shards,
    }
}

struct ShardStats {
    shard_count: usize,
    average_fill: f64,
    full_shards: usize,
}

impl ShardStats {
    fn fill_percentage(&self) -> f64 {
        (self.average_fill / NUM_OF_INDICES_IN_SHARD as f64) * 100.0
    }
    
    fn fragmentation_score(&self) -> f64 {
        // Higher score = more fragmentation
        if self.shard_count == 0 {
            return 0.0;
        }
        
        let ideal_shards = (self.total_entries() as f64 / NUM_OF_INDICES_IN_SHARD as f64).ceil();
        (self.shard_count as f64 / ideal_shards - 1.0).max(0.0)
    }
    
    fn total_entries(&self) -> usize {
        (self.average_fill * self.shard_count as f64) as usize
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_shard_stats() {
        let blocks = vec![1, 1000, 2000, 3000, 4000, 5000];
        let stats = calculate_shard_stats(Address::random(), &blocks);
        
        assert_eq!(stats.shard_count, 3); // Blocks spread across 3 shards
        assert_eq!(stats.full_shards, 0); // No shard has 2000 entries
        assert_eq!(stats.average_fill, 2.0); // 6 blocks / 3 shards
    }
}
```

### Answer 2: History Query

```rust
use reth_db_api::{cursor::DbCursorRO, tables};
use reth_provider::{DatabaseProvider, ProviderError};

fn find_account_at_block(
    provider: &impl DatabaseProvider,
    address: Address,
    target_block: BlockNumber,
) -> Result<Option<BlockNumber>, ProviderError> {
    let tx = provider.tx_ref();
    let mut cursor = tx.cursor_read::<tables::AccountsHistory>()?;
    
    // Start from the highest shard (most recent)
    let start_key = ShardedKey::new(address, u64::MAX);
    
    // Walk backwards through shards
    let mut last_change_before_target = None;
    
    // Seek to address shards
    let walker = cursor.walk_back(Some(start_key))?;
    
    for entry in walker {
        let (sharded_key, block_list) = entry?;
        
        // Stop if we've moved to a different address
        if sharded_key.key != address {
            break;
        }
        
        // Check blocks in this shard (they're sorted)
        for block in block_list.iter().rev() {
            if block <= target_block {
                // Found the last change before target
                return Ok(Some(block));
            }
        }
        
        // Check if this entire shard is after our target
        if let Some(first_block) = block_list.iter().next() {
            if first_block > target_block {
                // Keep searching older shards
                continue;
            }
        }
    }
    
    Ok(last_change_before_target)
}

// Extended version with changeset lookup
fn get_account_at_block(
    provider: &impl DatabaseProvider,
    address: Address,
    target_block: BlockNumber,
) -> Result<Option<Account>, ProviderError> {
    // Find the last change
    let change_block = find_account_at_block(provider, address, target_block)?;
    
    if let Some(block) = change_block {
        // Look up the account state after that change
        let mut changeset_cursor = provider
            .tx_ref()
            .cursor_read::<tables::AccountChangeSets>()?;
            
        // Find the changeset entry
        if let Some((_, changeset)) = changeset_cursor.seek_exact(block)? {
            if changeset.address == address {
                // This gives us the state BEFORE the change
                // We need to apply changes up to this block
                return provider.account_at_block(address, block);
            }
        }
    }
    
    // No history found - account doesn't exist at target block
    Ok(None)
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_history_query() {
        let provider = test_provider();
        let address = Address::random();
        
        // Insert some history
        provider.insert_account_history(address, vec![100, 200, 300, 400]).unwrap();
        
        // Query at different blocks
        assert_eq!(find_account_at_block(&provider, address, 50), Ok(None));
        assert_eq!(find_account_at_block(&provider, address, 150), Ok(Some(100)));
        assert_eq!(find_account_at_block(&provider, address, 250), Ok(Some(200)));
        assert_eq!(find_account_at_block(&provider, address, 500), Ok(Some(400)));
    }
}
```

### Answer 3: History Compactor

```rust
struct HistoryCompactor {
    min_shard_fill: f64,
    target_shard_fill: f64,
}

impl Default for HistoryCompactor {
    fn default() -> Self {
        Self {
            min_shard_fill: 0.25, // Compact shards less than 25% full
            target_shard_fill: 0.75, // Target 75% full shards
        }
    }
}

impl HistoryCompactor {
    fn should_compact(&self, shard_entries: usize) -> bool {
        let fill_ratio = shard_entries as f64 / NUM_OF_INDICES_IN_SHARD as f64;
        fill_ratio < self.min_shard_fill
    }
    
    fn compact_shards(&self, shards: Vec<BlockNumberList>) -> Vec<BlockNumberList> {
        if shards.is_empty() {
            return vec![];
        }
        
        // Collect all blocks
        let mut all_blocks: Vec<BlockNumber> = shards
            .iter()
            .flat_map(|shard| shard.iter())
            .collect();
        
        // Sort to maintain order
        all_blocks.sort_unstable();
        all_blocks.dedup(); // Remove any duplicates
        
        // Calculate target entries per shard
        let target_per_shard = (NUM_OF_INDICES_IN_SHARD as f64 * self.target_shard_fill) as usize;
        
        // Repack into new shards
        let mut new_shards = Vec::new();
        let mut current_shard = Vec::with_capacity(target_per_shard);
        
        for block in all_blocks {
            current_shard.push(block);
            
            if current_shard.len() >= target_per_shard {
                new_shards.push(
                    BlockNumberList::new(&current_shard)
                        .expect("valid block list")
                );
                current_shard = Vec::with_capacity(target_per_shard);
            }
        }
        
        // Don't forget the last partial shard
        if !current_shard.is_empty() {
            new_shards.push(
                BlockNumberList::new(&current_shard)
                    .expect("valid block list")
            );
        }
        
        new_shards
    }
    
    fn analyze_compaction(&self, shards: &[BlockNumberList]) -> CompactionAnalysis {
        let total_entries: usize = shards.iter().map(|s| s.len()).sum();
        let sparse_shards = shards
            .iter()
            .filter(|s| self.should_compact(s.len()))
            .count();
        
        let new_shards = self.compact_shards(shards.to_vec());
        let space_saved = shards.len().saturating_sub(new_shards.len());
        
        CompactionAnalysis {
            original_shards: shards.len(),
            sparse_shards,
            new_shards: new_shards.len(),
            space_saved,
            total_entries,
        }
    }
}

#[derive(Debug)]
struct CompactionAnalysis {
    original_shards: usize,
    sparse_shards: usize,
    new_shards: usize,
    space_saved: usize,
    total_entries: usize,
}

impl CompactionAnalysis {
    fn savings_percentage(&self) -> f64 {
        if self.original_shards == 0 {
            return 0.0;
        }
        
        (self.space_saved as f64 / self.original_shards as f64) * 100.0
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_compaction() {
        let compactor = HistoryCompactor::default();
        
        // Create some sparse shards
        let shards = vec![
            BlockNumberList::new(&vec![1, 2, 3]).unwrap(), // 3 entries (sparse)
            BlockNumberList::new(&vec![100, 101]).unwrap(), // 2 entries (sparse)
            BlockNumberList::new(&vec![200; 1500]).unwrap(), // 1500 entries (ok)
            BlockNumberList::new(&vec![300; 100]).unwrap(), // 100 entries (sparse)
        ];
        
        let analysis = compactor.analyze_compaction(&shards);
        
        assert_eq!(analysis.sparse_shards, 3);
        assert!(analysis.new_shards < analysis.original_shards);
        assert_eq!(analysis.total_entries, 1605);
    }
}
```