# Lesson 11: Static Files - Efficient Immutable Storage

*"The worthwhile problems are the ones you can really solve or help solve, the ones you can really contribute something to." - Richard Feynman*

## Files with Inline Comments for This Lesson
- `crates/static-file/static-file/src/lib.rs` - Main static file module
- `crates/static-file/static-file/src/segments/mod.rs` - Segment trait definition
- `crates/static-file/static-file/src/segments/headers.rs` - Headers segment implementation
- `crates/storage/provider/src/providers/static_file/writer.rs` - Static file writer

## Why Static Files? - The Deep WHY

### The Core Problem: Database Bloat

Imagine you're running a newspaper archive. Every day, you add new newspapers to your collection. After a while, you have 50 years of newspapers, but 99% of requests are for newspapers from the last month. However, you're storing everything in the same filing cabinet system, making it slow and expensive to access recent news.

This is exactly what happens with blockchain databases:

**The Database Bottleneck:**
- **Size**: Full Ethereum archive is several terabytes and growing
- **Performance**: Random access patterns hurt cache efficiency
- **Immutability**: Most data never changes after finalization (like old newspapers)
- **Cost**: Keeping everything in active database is expensive

### The Static Files Solution: Archival Storage

Static files are like moving old newspapers to a specialized archive:
- **Compressed**: Old newspapers are microfiched (compressed)
- **Efficient**: Specialized for read-only access
- **Separate**: Don't clutter the active filing system
- **Intact**: Still accessible when needed

**Why This Works:**
1. **Temporal Locality**: Most blockchain queries are for recent data
2. **Immutability**: Old finalized blocks never change
3. **Compression**: Similar data compresses well (headers are very similar)
4. **Sequential Access**: Historical queries often want ranges, not random access

### Real Performance Impact

```rust
// Without static files: Query must search entire database
let header = db.get_header(block_1000000)?; // Slow - searches 4TB database

// With static files: Query goes directly to the right file
let header = static_files.get_header(block_1000000)?; // Fast - memory-mapped access
```

Static files solve these problems by moving immutable data to optimized file formats.

## Static File Architecture

### File Organization

```
static_files/
├── headers/
│   ├── headers_0_499999.jar      # First 500k headers
│   ├── headers_500000_999999.jar # Next 500k headers
│   └── ...
├── transactions/
│   ├── transactions_0_499999.jar
│   └── ...
└── receipts/
    ├── receipts_0_999999.jar     # First 1M receipts
    └── ...
```

Each file:
- Contains a fixed range of data
- Is immutable once written
- Uses NippyJar format (custom compression)

### NippyJar Format: The "Pickle Jar" for Blockchain Data

**Why the name?** Think of NippyJar like a pickle jar - it takes raw data (cucumbers) and transforms it into a preserved, compressed format (pickles) that lasts longer and takes less space.

NippyJar is Reth's custom file format optimized for blockchain data:

```rust
pub struct NippyJar {
    /// Data file containing compressed entries
    data_file: File,
    /// Index mapping keys to offsets
    index: Vec<u64>,
    /// Compression algorithm used
    compression: Compression,
    /// File header with metadata
    header: NippyJarHeader,
}
```

**The Genius of NippyJar Design:**

1. **Dictionary Compression**: Like how ZIP files get better compression when files are similar, NippyJar builds a dictionary of common patterns in blockchain data. Headers are 60-70% similar, so they compress extremely well.

2. **Index-First Design**: 
   ```rust
   // Instead of scanning the entire file to find block 1000000
   // We use an index to jump directly to the right position
   let offset = jar.index[block_number as usize]; // O(1) lookup
   let data = jar.read_at(offset)?; // Direct access
   ```

3. **Memory Mapping Magic**:
   ```rust
   // The OS maps the file directly into memory
   // No need to load the entire file into RAM
   let mmap = unsafe { Mmap::map(&jar.data_file)? };
   let data = &mmap[offset..offset + length]; // Zero-copy access
   ```

**Benefits in Practice:**
- **Compression**: 50-70% space savings (like compressing 10GB to 3GB)
- **Random Access**: O(1) lookups via index (like a book's index)
- **Memory Mapping**: OS-level optimization (like having a book always open)
- **Checksums**: Data integrity verification (like receipt verification)

## The Segment System: Organizing the Archive

### Think of Segments as Specialized Filing Cabinets

Just like a library has different sections (fiction, non-fiction, reference), static files are organized into segments based on data type and access patterns:

```rust
pub enum StaticFileSegment {
    /// Block headers including canonical marker
    Headers,        // Like a card catalog - small, frequently accessed
    /// Transaction data (without receipts)
    Transactions,   // Like books - medium size, occasionally accessed
    /// Transaction receipts with logs
    Receipts,       // Like detailed records - large, rarely accessed
    /// Block metadata (future use)
    BlockMeta,      // Like archive metadata - small, utility data
}
```

**Why Separate Segments?**

1. **Different Access Patterns**:
   - Headers: Frequently accessed, small, highly similar
   - Transactions: Medium access, variable size
   - Receipts: Rarely accessed, large, different structure

2. **Optimization by Type**:
   ```rust
   // Headers compress extremely well (similar structure)
   let header_compression = Compression::Zstd { level: 12, dict: Some(header_dict) };
   
   // Transactions need different compression strategy
   let tx_compression = Compression::Zstd { level: 8, dict: None };
   ```

3. **Independent Evolution**: Each segment can evolve its format independently without affecting others.

### The Segment Trait

```rust
pub trait Segment<Provider: StaticFileProviderFactory> {
    /// Which segment type this handles
    fn segment(&self) -> StaticFileSegment;
    
    /// Copy data from database to static files
    fn copy_to_static_files(
        &self,
        provider: Provider,
        block_range: RangeInclusive<BlockNumber>,
    ) -> ProviderResult<()>;
}
```

## Headers Segment Implementation

Let's examine how headers are moved to static files:

```rust
impl Segment for Headers {
    fn copy_to_static_files(
        &self,
        provider: Provider,
        block_range: RangeInclusive<BlockNumber>,
    ) -> ProviderResult<()> {
        // 1. Get a writer for the correct file
        let mut writer = provider.static_file_provider()
            .get_writer(*block_range.start(), StaticFileSegment::Headers)?;
        
        // 2. Read data from multiple database tables
        let headers = provider.headers_range(block_range.clone())?;
        let tds = provider.headers_td_range(block_range.clone())?;
        let canonicals = provider.canonical_headers_range(block_range)?;
        
        // 3. Write each header with metadata
        for ((header, td), canonical) in headers.zip(tds).zip(canonicals) {
            writer.append_header(&header, td, &canonical)?;
        }
        
        // 4. Commit when done
        writer.commit()?;
        
        Ok(())
    }
}
```

## The Static File Writer

### Writer Lifecycle

```rust
pub struct StaticFileProviderRW<N> {
    /// Underlying NippyJar writer
    writer: NippyJarWriter,
    /// Current block range being written
    reader: StaticFileProvider<N>,
    /// Segment type
    segment: StaticFileSegment,
    /// Metrics tracking
    metrics: Option<Arc<StaticFileProviderMetrics>>,
}

impl<N> StaticFileProviderRW<N> {
    /// Create new static file if needed
    pub fn create(
        &mut self,
        segment: StaticFileSegment,
        block_start: BlockNumber,
    ) -> ProviderResult<()> {
        // Determine file path and range
        let path = self.data_path(segment, block_start);
        let file_range = self.file_range(block_start);
        
        // Create new NippyJar
        self.writer = NippyJarWriter::new(path)?
            .with_compression(Compression::Zstd);
        
        Ok(())
    }
}
```

### Appending Data

```rust
impl<N: NodePrimitives> StaticFileProviderRW<N> {
    /// Append a header to the static file
    pub fn append_header(
        &mut self,
        header: &N::BlockHeader,
        total_difficulty: U256,
        canonical_hash: &BlockHash,
    ) -> ProviderResult<()> {
        // Ensure we're writing in order
        let expected = self.writer.rows() as u64;
        if header.number() != expected {
            return Err(ProviderError::BlockNumberMismatch {
                expected,
                got: header.number(),
            });
        }
        
        // Encode and compress data
        let mut buf = Vec::new();
        header.to_compact(&mut buf);
        total_difficulty.to_compact(&mut buf);
        canonical_hash.to_compact(&mut buf);
        
        // Append to file
        self.writer.append(&buf)?;
        
        Ok(())
    }
    
    /// Append a transaction
    pub fn append_transaction(
        &mut self,
        tx_num: TxNumber,
        transaction: &N::SignedTransaction,
    ) -> ProviderResult<()> {
        // Transactions use their global number as key
        self.writer.append_with_key(tx_num, transaction)?;
        Ok(())
    }
}
```

### Committing Changes

```rust
impl<N> StaticFileProviderRW<N> {
    /// Finalize the current file
    pub fn commit(&mut self) -> ProviderResult<()> {
        let start = Instant::now();
        
        // Flush data to disk
        self.writer.commit()?;
        
        // Update segment header
        let header = SegmentHeader {
            block_range: self.block_range.clone(),
            tx_range: self.tx_range.clone(),
            segment: self.segment,
        };
        self.writer.set_header(header)?;
        
        // Record metrics
        if let Some(metrics) = &self.metrics {
            metrics.record_segment_operation(
                self.segment,
                StaticFileProviderOperation::Commit,
                start.elapsed(),
            );
        }
        
        Ok(())
    }
}
```

## Reading from Static Files

### The Provider Interface

```rust
impl<N: NodePrimitives> StaticFileProvider<N> {
    /// Get header by block number
    pub fn header(&self, block_number: BlockNumber) -> ProviderResult<Option<N::BlockHeader>> {
        // 1. Find the correct file
        let jar = self.find_jar(StaticFileSegment::Headers, block_number)?;
        
        // 2. Calculate offset within file
        let offset = block_number - jar.block_range().start();
        
        // 3. Read and decompress
        let data = jar.get(offset as usize)?;
        
        // 4. Decode
        let header = N::BlockHeader::from_compact(&data)?;
        
        Ok(Some(header))
    }
}
```

### Efficient Range Queries

```rust
impl<N> StaticFileProvider<N> {
    /// Get headers in range
    pub fn headers_range(
        &self,
        range: Range<BlockNumber>,
    ) -> ProviderResult<Vec<N::BlockHeader>> {
        let mut headers = Vec::with_capacity(range.len());
        
        // Group by file to minimize opens
        for (jar, file_range) in self.jars_for_range(StaticFileSegment::Headers, range)? {
            // Memory map the data region
            let mmap = jar.mmap_data()?;
            
            // Bulk decompress
            let entries = jar.decompress_range(&mmap, file_range)?;
            
            // Decode all at once
            for data in entries {
                headers.push(N::BlockHeader::from_compact(&data)?);
            }
        }
        
        Ok(headers)
    }
}
```

## Migration Process

### When to Migrate

Data is moved to static files when:
1. **Finalized**: Block is sufficiently deep (e.g., 10,000 blocks)
2. **Space Pressure**: Database is getting too large
3. **Scheduled**: During maintenance windows

### The Migration Algorithm

```rust
pub fn migrate_to_static_files(
    provider: &DatabaseProvider,
    up_to_block: BlockNumber,
) -> ProviderResult<()> {
    // 1. Determine what needs migration
    let current = provider.static_file_provider()
        .get_highest_static_file_block(StaticFileSegment::Headers)?
        .unwrap_or(0);
    
    if current >= up_to_block {
        return Ok(()); // Nothing to do
    }
    
    let range = (current + 1)..=up_to_block;
    
    // 2. Copy each segment
    for segment in [Headers, Transactions, Receipts] {
        segment.copy_to_static_files(provider, range.clone())?;
    }
    
    // 3. Verify integrity
    verify_static_files(provider, range.clone())?;
    
    // 4. Delete from database
    provider.prune_database_tables(range)?;
    
    Ok(())
}
```

## Benefits and Trade-offs

### Benefits

1. **Space Efficiency**
   - 50-70% compression ratio
   - No index overhead
   - Deduplicated data

2. **Performance**
   - Sequential reads are fast
   - Memory-mapped access
   - Parallel decompression

3. **Simplicity**
   - Immutable files are easy to backup
   - No write-ahead log needed
   - Clear separation of concerns

### Trade-offs

1. **Complexity**
   - Two storage systems to manage
   - Migration process needs care
   - More code paths

2. **Random Access**
   - Still need indices for lookups
   - Decompression overhead
   - Memory mapping limitations

## Static File Configuration

```rust
pub struct StaticFileConfig {
    /// Blocks per file
    pub blocks_per_file: u64,
    /// Compression level (1-22)
    pub compression_level: u8,
    /// Enable checksums
    pub enable_checksums: bool,
    /// Migration batch size
    pub migration_batch_size: u64,
}

impl Default for StaticFileConfig {
    fn default() -> Self {
        Self {
            blocks_per_file: 500_000,      // 500k blocks per file
            compression_level: 12,         // Good balance
            enable_checksums: true,        // Safety first
            migration_batch_size: 10_000,  // Migrate 10k at a time
        }
    }
}
```

## Common Patterns: How Components Work Together

### 1. Transparent Access: The Invisible Archive

**The User's Perspective**: "I want block 1000000's header"
**The System's Reality**: "Is it in the archive (static files) or active storage (database)?"

```rust
impl DatabaseProvider {
    pub fn header(&self, number: BlockNumber) -> ProviderResult<Option<Header>> {
        // Try static files first (like checking the archive)
        if let Some(header) = self.static_file_provider.header(number)? {
            return Ok(Some(header));
        }
        
        // Fall back to database (like checking active storage)
        self.database.get::<tables::Headers>(number)
    }
}
```

**This pattern is like a librarian who:**
1. First checks if the book is in the archive (static files)
2. If not found, checks the active collection (database)
3. Returns the book without you knowing where it came from

**Why This Order?**
- Static files are faster for old data (compressed, memory-mapped)
- Database is faster for recent data (in cache, indexed)
- Most queries are for recent data, but when they're not, static files shine

### Real-World Performance Impact

```rust
// Without static files: ALL data in database
let headers = db.get_headers(0..1_000_000)?; // Slow: 4TB database scan

// With static files: Hybrid approach
let old_headers = static_files.get_headers(0..990_000)?;    // Fast: sequential read
let new_headers = db.get_headers(990_000..1_000_000)?;      // Fast: cache hit
let combined = [old_headers, new_headers].concat();         // Combined result
```

### 2. Bulk Operations

```rust
fn export_headers_to_file(
    provider: &DatabaseProvider,
    range: Range<BlockNumber>,
    output: &Path,
) -> ProviderResult<()> {
    // Use static files for efficient export
    let headers = provider.static_file_provider
        .headers_range(range)?;
    
    // Write in streaming fashion
    let file = File::create(output)?;
    let mut writer = BufWriter::new(file);
    
    for header in headers {
        serde_json::to_writer(&mut writer, &header)?;
        writeln!(&mut writer)?;
    }
    
    Ok(())
}
```

### 3. Pruning Integration

```rust
impl StaticFileProvider {
    pub fn prune_headers(&mut self, keep_last: u64) -> ProviderResult<()> {
        let highest = self.get_highest_static_file_block(Headers)?
            .ok_or(ProviderError::NoStaticFiles)?;
        
        let prune_until = highest.saturating_sub(keep_last);
        
        // Delete old files
        for jar in self.find_jars_before(Headers, prune_until)? {
            jar.delete()?;
        }
        
        Ok(())
    }
}
```

## Assignments with Solutions

### 1. Calculate compression ratio for different data types

```rust
use reth_codecs::Compact;

fn measure_compression_ratio<T: Compact>(
    items: &[T],
    compression: Compression,
) -> f64 {
    // Encode items
    let mut uncompressed = Vec::new();
    for item in items {
        item.to_compact(&mut uncompressed);
    }
    
    // Compress
    let compressed = compression.compress(&uncompressed).unwrap();
    
    // Calculate ratio
    let ratio = compressed.len() as f64 / uncompressed.len() as f64;
    
    println!("Uncompressed: {} bytes", uncompressed.len());
    println!("Compressed: {} bytes", compressed.len());
    println!("Ratio: {:.2}%", ratio * 100.0);
    
    ratio
}

// Test with real data
fn test_compression() {
    // Headers are highly compressible (similar structure)
    let headers: Vec<Header> = fetch_headers(0..1000);
    let header_ratio = measure_compression_ratio(&headers, Compression::Zstd);
    assert!(header_ratio < 0.4); // Expect 60%+ compression
    
    // Transactions vary more
    let txs: Vec<TransactionSigned> = fetch_transactions(0..1000);
    let tx_ratio = measure_compression_ratio(&txs, Compression::Zstd);
    assert!(tx_ratio < 0.6); // Expect 40%+ compression
}
```

### 2. Implement a static file reader with caching

```rust
use lru::LruCache;
use std::sync::Mutex;

pub struct CachedStaticFileReader<N> {
    provider: StaticFileProvider<N>,
    header_cache: Mutex<LruCache<BlockNumber, N::BlockHeader>>,
    tx_cache: Mutex<LruCache<TxNumber, N::SignedTransaction>>,
}

impl<N: NodePrimitives> CachedStaticFileReader<N> {
    pub fn new(provider: StaticFileProvider<N>, cache_size: usize) -> Self {
        Self {
            provider,
            header_cache: Mutex::new(LruCache::new(cache_size)),
            tx_cache: Mutex::new(LruCache::new(cache_size)),
        }
    }
    
    pub fn header(&self, number: BlockNumber) -> ProviderResult<Option<N::BlockHeader>> {
        // Check cache first
        {
            let mut cache = self.header_cache.lock().unwrap();
            if let Some(header) = cache.get(&number) {
                return Ok(Some(header.clone()));
            }
        }
        
        // Read from static file
        if let Some(header) = self.provider.header(number)? {
            // Update cache
            let mut cache = self.header_cache.lock().unwrap();
            cache.put(number, header.clone());
            Ok(Some(header))
        } else {
            Ok(None)
        }
    }
    
    pub fn headers_range(
        &self,
        range: Range<BlockNumber>,
    ) -> ProviderResult<Vec<N::BlockHeader>> {
        let mut headers = Vec::with_capacity(range.len());
        let mut missing = Vec::new();
        
        // Check cache for each header
        {
            let mut cache = self.header_cache.lock().unwrap();
            for number in range.clone() {
                if let Some(header) = cache.get(&number) {
                    headers.push((number, header.clone()));
                } else {
                    missing.push(number);
                }
            }
        }
        
        // Batch read missing headers
        if !missing.is_empty() {
            let missing_headers = self.provider.headers_by_numbers(missing)?;
            
            // Update cache and results
            let mut cache = self.header_cache.lock().unwrap();
            for (number, header) in missing_headers {
                cache.put(number, header.clone());
                headers.push((number, header));
            }
        }
        
        // Sort by block number
        headers.sort_by_key(|(n, _)| *n);
        Ok(headers.into_iter().map(|(_, h)| h).collect())
    }
}
```

### 3. Design a parallel static file writer

```rust
use rayon::prelude::*;
use std::sync::mpsc;

pub struct ParallelStaticFileWriter<N> {
    writers: Vec<StaticFileProviderRW<N>>,
    segment: StaticFileSegment,
}

impl<N: NodePrimitives> ParallelStaticFileWriter<N> {
    pub fn write_headers_parallel(
        &mut self,
        headers: Vec<(N::BlockHeader, U256, BlockHash)>,
        chunks: usize,
    ) -> ProviderResult<()> {
        // Split headers into chunks
        let chunk_size = headers.len() / chunks;
        let header_chunks: Vec<_> = headers.chunks(chunk_size).collect();
        
        // Create channel for results
        let (tx, rx) = mpsc::channel();
        
        // Process chunks in parallel
        header_chunks.into_par_iter()
            .enumerate()
            .for_each(|(idx, chunk)| {
                let tx = tx.clone();
                
                // Each thread processes its chunk
                let result = self.process_chunk(idx, chunk);
                tx.send((idx, result)).unwrap();
            });
        
        // Collect results
        drop(tx);
        let mut results = Vec::new();
        for (idx, result) in rx {
            results.push((idx, result?));
        }
        
        // Merge files in order
        results.sort_by_key(|(idx, _)| *idx);
        self.merge_files(results)?;
        
        Ok(())
    }
    
    fn process_chunk(
        &self,
        chunk_idx: usize,
        headers: &[(N::BlockHeader, U256, BlockHash)],
    ) -> ProviderResult<PathBuf> {
        // Create temporary file for chunk
        let temp_path = self.temp_path(chunk_idx);
        let mut writer = NippyJarWriter::new(&temp_path)?;
        
        // Write headers
        for (header, td, canonical) in headers {
            let mut buf = Vec::new();
            header.to_compact(&mut buf);
            td.to_compact(&mut buf);
            canonical.to_compact(&mut buf);
            writer.append(&buf)?;
        }
        
        writer.commit()?;
        Ok(temp_path)
    }
    
    fn merge_files(&self, chunks: Vec<(usize, PathBuf)>) -> ProviderResult<()> {
        // Merge temporary files into final file
        let mut final_writer = self.writers[0].writer.clone();
        
        for (_, chunk_path) in chunks {
            let reader = NippyJar::open(&chunk_path)?;
            
            // Copy all entries
            for i in 0..reader.len() {
                let data = reader.get(i)?;
                final_writer.append(&data)?;
            }
            
            // Clean up temp file
            std::fs::remove_file(chunk_path)?;
        }
        
        final_writer.commit()?;
        Ok(())
    }
}
```

## Common Pitfalls and How to Avoid Them

### 1. Migration Timing Issues

**The Problem**: Moving data too early or too late

```rust
// ❌ DON'T: Move data before it's finalized
let latest_block = provider.latest_block()?;
migrate_to_static_files(provider, latest_block)?; // Dangerous!

// ✅ DO: Wait for finalization
const FINALIZATION_DEPTH: u64 = 10_000;
let safe_block = latest_block.saturating_sub(FINALIZATION_DEPTH);
migrate_to_static_files(provider, safe_block)?; // Safe!
```

**Why This Matters**: If you move data too early and a reorg happens, you'll have inconsistent state.

### 2. Index Corruption

**The Problem**: If the index and data get out of sync

```rust
// ❌ DON'T: Write data and index separately
jar.write_data(&data)?;
// ... crash here ...
jar.write_index(&index)?; // Index never gets written!

// ✅ DO: Use atomic operations
jar.write_atomic(&data, &index)?; // Either both succeed or both fail
```

### 3. Memory Mapping Limitations

**The Problem**: Memory mapping doesn't work well over networks

```rust
// ❌ DON'T: Try to memory map network files
let network_file = File::open("s3://bucket/file.jar")?;
let mmap = unsafe { Mmap::map(&network_file)? }; // Will be slow!

// ✅ DO: Use streaming for remote files
let mut stream = S3Stream::open("s3://bucket/file.jar")?;
let data = stream.read_range(offset, length)?; // Optimized for network
```

## Connections to Other Concepts

### How Static Files Connect to the Broader System

**1. With Staged Sync (Lesson 19)**:
```rust
// Staged sync fills the database with new data
let new_blocks = sync_stage.execute()?;

// Static file producer moves old data to archive
let migrated = static_file_producer.run(targets)?;

// Result: Database stays small, history preserved
```

**2. With State Root Calculation (Lesson 18)**:
```rust
// State root needs historical data
let historical_state = static_files.get_state_at(old_block)?;
let current_state = database.get_latest_state()?;

// Can calculate state root using both sources
let state_root = calculate_state_root(historical_state, current_state)?;
```

**3. With Chain Reorgs (Lesson 20)**:
```rust
// During reorg, static files provide stability
if reorg_detected {
    // Static files are immutable - safe to use
    let canonical_chain = static_files.get_canonical_chain()?;
    
    // Database might have inconsistent state
    database.rollback_to_static_file_boundary()?;
}
```

## Questions to Ponder - Detailed Answers

### 1. Why not use a standard format like Parquet?

**The Short Answer**: Blockchain data has unique requirements that generic formats don't optimize for.

**The Deep Answer**:

**Custom Requirements**:
- Need exact byte-level compatibility with consensus
- Blockchain-specific compression patterns
- Integration with existing codebase

**Performance Considerations**:
```rust
// Parquet: Optimized for analytics queries
let analytics_query = "SELECT AVG(gas_used) FROM transactions WHERE block_number > 1000000";

// NippyJar: Optimized for blockchain access patterns
let blockchain_query = "Get block 1000000 header"; // O(1) lookup
```

**Flexibility Needs**:
- Can evolve format with protocol changes
- Add custom indices as needed
- Optimize for specific hardware

**Real-World Analogy**: It's like asking "Why don't libraries use the same storage system as grocery stores?" Both store things, but libraries optimize for finding specific books, while grocery stores optimize for high-turnover inventory.

### 2. How do static files handle reorganizations?

**Finalization depth**:
```rust
const FINALIZATION_DEPTH: u64 = 10_000;

// Only move finalized blocks
let safe_block = latest_block - FINALIZATION_DEPTH;
migrate_to_static_files(provider, safe_block)?;
```

**Reorg handling**:
- Static files only contain finalized data
- During reorg, read from database
- After finalization, migrate to static

**Emergency recovery**:
- Can rebuild static files from database
- Database remains source of truth
- Static files are optimization only

### 3. What about distributed/cloud storage?

**Future possibilities**:
- Store static files in S3/GCS
- Use CDN for distribution
- Implement tiered storage

**Current limitations**:
- Memory mapping requires local files
- Latency impacts performance
- Complexity of distributed systems

**Hybrid approach**:
```rust
pub enum StorageBackend {
    Local(PathBuf),
    S3 { bucket: String, prefix: String },
    Hybrid { hot: PathBuf, cold: String },
}
```