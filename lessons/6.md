# Lesson 6: Introduction to Storage - MDBX Database

*"It doesn't matter how beautiful your theory is, it doesn't matter how smart you are. If it doesn't agree with experiment, it's wrong." - Richard Feynman*

## Files with Inline Comments for This Lesson
- `crates/storage/db/src/implementation/mdbx/mod.rs` - MDBX database implementation
- `crates/storage/db/src/abstraction/database.rs` - Database trait abstractions
- `crates/storage/libmdbx-rs/src/lib.rs` - Low-level MDBX bindings

## What is MDBX? - The Ferrari of Databases

MDBX (Memory-Mapped Database eXtended) is a key-value database that Reth uses as its primary storage engine. It's a fork of LMDB (Lightning Memory-Mapped Database) with additional features and optimizations.

**Think of MDBX as the Ferrari of databases:**
- Incredibly fast (zero-copy reads)
- Sophisticated engineering (memory-mapping)
- High performance requirements (blockchain storage)
- Requires expert handling (careful transaction management)

### Key Properties of MDBX

1. **Memory-Mapped I/O**: The entire database is mapped into virtual memory
   - Like having a magic book where any page instantly appears when you think about it
   - No waiting for disk reads - data is "already in memory"

2. **B+ Tree Structure**: Data is organized in sorted B+ trees for efficient range queries
   - Like a perfectly organized library where books are sorted and findable in log(n) time
   - Range queries are extremely fast (get all blocks 1000-2000)

3. **ACID Transactions**: Atomicity, Consistency, Isolation, Durability
   - **Atomicity**: All changes happen or none happen (no partial updates)
   - **Consistency**: Database never enters invalid state
   - **Isolation**: Concurrent operations don't interfere
   - **Durability**: Committed changes survive crashes

4. **Zero-Copy Reads**: Data is read directly from mapped memory
   - No malloc/free overhead
   - No copying bytes around
   - Data structure pointers point directly into file

5. **MVCC**: Multi-Version Concurrency Control allows concurrent readers
   - Multiple readers can access database simultaneously
   - Writers don't block readers (and vice versa)
   - Each transaction sees a consistent snapshot

## Why MDBX for Blockchain Storage? - Perfect Match

Blockchain storage has unique requirements that perfectly match MDBX's strengths:

### The Blockchain Storage Problem

**Traditional databases were designed for business applications:**
- Random insert/update/delete patterns
- OLTP workloads (many small transactions)
- Normalized data (many tables with relationships)
- Consistent small dataset size

**Blockchain storage is different:**
- **Append-mostly**: New blocks are constantly added, old blocks never change
- **Read-heavy**: Many more reads than writes (100:1 ratio or higher)
- **Range queries**: "Get all blocks from 1000 to 2000" is common
- **Large dataset**: Can grow to terabytes (Ethereum mainnet ~1TB+)
- **Crash safety**: Must not corrupt on power loss (financial data!)
- **High performance**: Need to validate blocks in real-time

### Why MDBX is Perfect

**1. Append-mostly → B+ Tree structure**
```rust
// Inserting sequential block numbers is optimal for B+ trees
for block_num in 1_000_000..2_000_000 {
    db.put(block_num, block_data)?; // Always inserting at "end" of tree
}
// This creates a perfectly balanced tree with minimal rebalancing
```

**2. Read-heavy → Zero-copy + MVCC**
```rust
// Multiple threads can read simultaneously without blocking
let handles: Vec<_> = (0..16).map(|_| {
    thread::spawn(|| {
        let tx = db.tx()?; // Each gets its own read transaction
        tx.get(random_block_number())? // Zero-copy access
    })
}).collect();
```

**3. Range queries → B+ Tree + Cursors**
```rust
// Extremely efficient range queries
let mut cursor = tx.cursor::<BlocksTable>()?;
cursor.seek(BlockNumber(1000))?; // Jump directly to start
while let Some((num, block)) = cursor.next()? {
    if num >= BlockNumber(2000) { break; }
    process_block(block);
} // Sequential reads are cache-friendly
```

**4. Large dataset → Memory mapping**
- OS handles caching automatically
- Can efficiently access TB-sized databases
- Virtual memory allows "infinite" address space

**5. Crash safety → ACID properties**
```rust
// Either all block data is written or none
let tx = db.tx_mut()?;
tx.put::<Headers>(block.number, block.header)?;
tx.put::<Bodies>(block.number, block.body)?;
tx.put::<Receipts>(block.number, block.receipts)?;
tx.commit()?; // Atomic commit - all or nothing
```

**Real-world performance comparison:**
- SQLite: ~1,000 blocks/sec insertion
- PostgreSQL: ~500 blocks/sec insertion
- MDBX: ~10,000+ blocks/sec insertion (with batching)

MDBX isn't just faster - it's designed for exactly the workload that blockchains create.

## Memory Mapping Explained

Traditional databases:
```rust
// Traditional approach - multiple copies and syscalls
let data = read_from_disk(key)?;        // Syscall, copy to kernel buffer
let parsed = deserialize(&data)?;        // Copy to user space, deserialize
return parsed;                           // Return copy
```

MDBX approach:
```rust
// MDBX - zero copies
let ptr = mmap_pointer(key)?;            // Get pointer to mapped memory
let data = unsafe { &*ptr };             // Cast to type - no copies!
return data;                             // Return reference
```

### Virtual Memory Magic

When you memory-map a file:
1. OS creates virtual memory pages pointing to the file
2. Pages are loaded on-demand when accessed (page fault)
3. OS manages caching automatically via page cache
4. Multiple processes can share the same mapped pages

## MDBX Architecture in Reth

### Database Layout

```
reth_db/
├── mdbx.dat          # Main database file (up to 4TB)
└── mdbx.lck          # Lock file for coordination
```

### Internal Structure

MDBX uses two B+ trees:
1. **Main B+ Tree**: Maps database ID → root page of sub-database
2. **Sub-database B+ Trees**: One for each table (accounts, blocks, etc.)

```
Main B+ Tree
├── "accounts" → Page 1234
├── "blocks" → Page 5678
├── "transactions" → Page 9012
└── ...

Accounts B+ Tree (at Page 1234)
├── Address1 → Account1
├── Address2 → Account2
└── ...
```

## Transaction Model

MDBX supports two transaction types:

### Read Transactions (RO)
```rust
// Multiple concurrent readers allowed
let tx = db.begin_ro_txn()?;
let value = tx.get(table, key)?;
// No commit needed - automatically closed
```

### Write Transactions (RW)
```rust
// Only ONE writer at a time
let tx = db.begin_rw_txn()?;
tx.put(table, key, value)?;
tx.commit()?;  // Makes changes durable
```

## Reth's Database Abstraction

Reth wraps MDBX with a clean abstraction:

```rust
pub trait Database: Send + Sync {
    type TX: DbTx;
    type TXMut: DbTxMut;
    
    fn tx(&self) -> Result<Self::TX>;
    fn tx_mut(&self) -> Result<Self::TXMut>;
}

pub trait DbTx: Send + Sync {
    type Cursor<T: Table>: DbCursor<T>;
    
    fn get<T: Table>(&self, key: T::Key) -> Result<Option<T::Value>>;
    fn cursor<T: Table>(&self) -> Result<Self::Cursor<T>>;
}
```

This abstraction allows:
- Swapping database backends
- Testing with in-memory databases
- Adding metrics/logging

## Tables in Reth

Reth defines strongly-typed tables:

```rust
pub trait Table: Send + Sync + Debug + 'static {
    const NAME: &'static str;
    type Key: Key;
    type Value: Value;
}

// Example table definition
pub struct AccountsTable;
impl Table for AccountsTable {
    const NAME: &'static str = "accounts";
    type Key = Address;
    type Value = Account;
}
```

Benefits:
- Type safety: Can't put wrong data in tables
- Auto serialization/deserialization
- Clear schema documentation

## Performance Optimizations

### 1. Write Batching
```rust
// Bad - one transaction per write
for (k, v) in items {
    let tx = db.tx_mut()?;
    tx.put(table, k, v)?;
    tx.commit()?;  // Slow! Disk sync each time
}

// Good - batch writes
let tx = db.tx_mut()?;
for (k, v) in items {
    tx.put(table, k, v)?;
}
tx.commit()?;  // One disk sync for all
```

### 2. Cursor Usage for Range Queries
```rust
// Inefficient - many individual lookups
let mut accounts = vec![];
for addr in addresses {
    if let Some(acc) = tx.get::<AccountsTable>(addr)? {
        accounts.push(acc);
    }
}

// Efficient - cursor for sequential access
let mut cursor = tx.cursor::<AccountsTable>()?;
cursor.seek(start_addr)?;
while let Some((addr, acc)) = cursor.next()? {
    if addr > end_addr { break; }
    accounts.push(acc);
}
```

### 3. Read Transaction Reuse
```rust
// Bad - new transaction each time
fn get_balance(&self, addr: Address) -> Result<U256> {
    let tx = self.db.tx()?;  // Creates new read transaction
    let account = tx.get::<AccountsTable>(addr)?;
    Ok(account.map(|a| a.balance).unwrap_or_default())
}

// Good - reuse transaction
fn get_balances(&self, addrs: &[Address]) -> Result<Vec<U256>> {
    let tx = self.db.tx()?;  // One transaction for all reads
    addrs.iter()
        .map(|addr| {
            let account = tx.get::<AccountsTable>(addr)?;
            Ok(account.map(|a| a.balance).unwrap_or_default())
        })
        .collect()
}
```

## MDBX Configuration in Reth - Tuning the Ferrari

Configuring MDBX is like tuning a race car - each setting affects performance and safety:

```rust
pub struct MdbxEnvConfig {
    pub path: PathBuf,
    pub max_size: usize,        // Default: 4TB (virtual limit)
    pub growth_step: isize,     // Default: 4GB (growth increment) 
    pub max_readers: u32,       // Default: 32,000 (concurrent readers)
    pub log_level: LogLevel,    // For debugging
}
```

### Memory Settings - Virtual vs Physical

**max_size (Virtual Memory Limit):**
```rust
// This is NOT how much disk space you use!
// It's the maximum virtual address space MDBX can use
max_size: 4 * 1024 * 1024 * 1024 * 1024,  // 4TB virtual

// Actual disk usage grows as needed:
// Day 1: 1GB actual, 4TB virtual reserved
// Day 30: 100GB actual, 4TB virtual reserved  
// Day 365: 1TB actual, 4TB virtual reserved
```

**Why virtual memory is brilliant:**
- On 64-bit systems, virtual address space is "free" (massive address space)
- Physical memory/disk is allocated on-demand
- Allows database to grow without reconfiguration
- OS handles the mapping automatically

**growth_step (Auto-resize Increment):**
```rust
// When database file needs to grow, grow by this much
growth_step: 4 * 1024 * 1024 * 1024,  // 4GB chunks

// Why not grow byte-by-byte?
// - File system operations are expensive
// - Growing in large chunks reduces fragmentation
// - Amortizes the cost of resize operations
```

### Safety vs Performance Trade-offs

**sync_mode - The Crucial Choice:**

```rust
// Safe mode (default) - Maximum durability
sync_mode: SyncMode::Safe,
// - Every commit calls fsync() 
// - Guarantees data on disk before returning
// - Survives power failures
// - ~100 commits/second max (due to disk latency)

// Fast mode - Better performance, slight risk
sync_mode: SyncMode::Fast,
// - Relies on OS write caching
// - ~1000+ commits/second
// - Small window where committed data might be lost on power failure
// - Good for non-critical data or UPS-protected systems

// Utterly mode - Maximum speed, no durability guarantees
sync_mode: SyncMode::Utterly,
// - No fsync() calls at all
// - Memory-speed performance
// - Can lose several seconds of commits on crash
// - Only for testing or disposable data
```

**Real-world configuration for Reth:**

```rust
// Archive node (stores all history)
MdbxEnvConfig {
    max_size: 8 * TB,          // Ethereum will grow beyond 4TB
    sync_mode: SyncMode::Safe, // Financial data needs durability
    max_readers: 64,           // Many RPC queries
}

// Full node (recent data only)
MdbxEnvConfig {
    max_size: 2 * TB,          // Pruned data is smaller
    sync_mode: SyncMode::Fast, // Can accept slight risk for speed
    max_readers: 32,           // Fewer concurrent queries
}
```

**Common mistake:** Setting max_size too small. Unlike other databases, MDBX can't grow beyond this limit. Always set it larger than you think you'll need!

## Common Pitfalls and Solutions

### 1. Long-Running Read Transactions
```rust
// Problem: Blocks writers and prevents cleanup
let tx = db.tx()?;
let data = tx.get(key)?;
expensive_computation(&data);  // tx still open!
// tx dropped here - too late

// Solution: Close transaction ASAP
let data = {
    let tx = db.tx()?;
    tx.get(key)?
};
expensive_computation(&data);  // tx already closed
```

### 2. Database Growth
MDBX files never shrink automatically:
```rust
// Monitor size
let stat = env.stat()?;
let size_gb = stat.psize * stat.leaf_pages / 1_073_741_824;
println!("Database size: {} GB", size_gb);

// Compact if needed (offline operation)
if size_gb > threshold {
    compact_database(&old_path, &new_path)?;
}
```

### 3. Reader Slot Exhaustion
```rust
// Problem: Too many concurrent readers
for i in 0..100_000 {
    thread::spawn(move || {
        let tx = db.tx()?;  // Eventually fails: no reader slots
    });
}

// Solution: Use connection pool or limit concurrency
let semaphore = Arc::new(Semaphore::new(1000));
for i in 0..100_000 {
    let sem = semaphore.clone();
    thread::spawn(move || {
        let _permit = sem.acquire();
        let tx = db.tx()?;
    });
}
```

## Assignments with Solutions

### 1. Calculate database size for 20M blocks

Each block needs approximately:
- Header: ~500 bytes
- Transactions: ~100 KB average
- Receipts: ~50 KB average
- State changes: ~20 KB average

Total per block: ~170 KB

For 20M blocks:
```
20,000,000 × 170 KB = 3,400,000,000 KB = 3.4 TB
```

Plus indices and state: ~5-6 TB total

### 2. Write a function to iterate all accounts
```rust
use reth_db::{Database, AccountsTable};

fn iterate_all_accounts<DB: Database>(db: &DB) -> Result<u64> {
    let tx = db.tx()?;
    let mut cursor = tx.cursor::<AccountsTable>()?;
    let mut count = 0;
    
    // Start from beginning
    cursor.first()?;
    
    // Iterate all entries
    while let Some((address, account)) = cursor.next()? {
        count += 1;
        // Process account if needed
        if count % 100_000 == 0 {
            println!("Processed {} accounts", count);
        }
    }
    
    Ok(count)
}
```

### 3. Implement a simple cache on top of MDBX
```rust
use std::sync::Arc;
use parking_lot::RwLock;
use lru::LruCache;

struct CachedDatabase<DB> {
    db: DB,
    cache: Arc<RwLock<LruCache<Address, Account>>>,
}

impl<DB: Database> CachedDatabase<DB> {
    fn new(db: DB, cache_size: usize) -> Self {
        Self {
            db,
            cache: Arc::new(RwLock::new(LruCache::new(cache_size))),
        }
    }
    
    fn get_account(&self, addr: Address) -> Result<Option<Account>> {
        // Check cache first
        {
            let cache = self.cache.read();
            if let Some(account) = cache.peek(&addr) {
                return Ok(Some(account.clone()));
            }
        }
        
        // Not in cache, fetch from DB
        let tx = self.db.tx()?;
        let account = tx.get::<AccountsTable>(addr)?;
        
        // Update cache
        if let Some(ref acc) = account {
            let mut cache = self.cache.write();
            cache.put(addr, acc.clone());
        }
        
        Ok(account)
    }
}
```

## Questions to Ponder - Detailed Answers

### 1. Why use B+ trees instead of hash tables?

**Hash tables are fast for point lookups:**
```rust
// Hash table: O(1) average case
let block = hash_table.get(block_number)?; // Super fast single lookup
```

**But B+ trees excel at blockchain workloads:**
```rust
// B+ tree: O(log n) but with incredible range query performance
let blocks = btree.range(1000..2000)?; // Get 1000 blocks in one operation!
```

**Why B+ trees win for blockchain:**

1. **Sorted iteration**: "Give me all blocks in order"
   ```rust
   // Hash table: Impossible to iterate in order
   // B+ tree: Natural iteration follows sort order
   for (number, block) in blocks_btree.iter() {
       // Blocks come out in order: 1, 2, 3, ...
   }
   ```

2. **Range queries**: "Get blocks 1,000,000 to 1,001,000"
   ```rust
   // Hash table: Need 1000 individual lookups
   for i in 1_000_000..1_001_000 {
       hash_table.get(i)?; // 1000 separate operations
   }
   
   // B+ tree: One efficient range scan
   btree.range(1_000_000..1_001_000)?; // Single operation
   ```

3. **Prefix iteration**: "Find all storage for account 0x123..."
   ```rust
   // Storage key = (address, slot)
   // B+ tree can efficiently find all keys starting with address
   btree.scan_prefix((address, _))?;
   ```

4. **Cache locality**: Sequential data stored together on disk
   - Hash tables scatter related data across memory
   - B+ trees keep related data close together
   - Results in fewer cache misses and disk seeks

### 2. What happens during a power failure?

**The Crash Safety Challenge:**

Imagine you're writing a check, and the power goes out halfway through. When power returns, what state is your checkbook in?

**MDBX's Solution - Copy-on-Write + Atomic Commits:**

```rust
// Step 1: Transaction begins
let tx = db.tx_mut()?;

// Step 2: Make changes (but only in memory)
tx.put(key1, value1)?; // Not yet on disk
tx.put(key2, value2)?; // Not yet on disk

// Step 3: Commit (atomic operation)
tx.commit()?; // Either ALL changes are written or NONE
```

**How MDBX ensures crash safety:**

1. **Copy-on-Write**: Never overwrite existing data
   - Old data stays intact until new data is fully written
   - If crash occurs during write, old data is still valid

2. **Atomic commits**: All-or-nothing guarantee
   - Transaction metadata updated last
   - If metadata update fails, entire transaction is ignored
   - If metadata update succeeds, entire transaction is committed

3. **Write ordering**: Careful ordering of disk writes
   - Data pages written first
   - Metadata updated last (commits the transaction)
   - Uses disk flush/sync to ensure ordering

**Recovery process:**
```rust
// On restart after crash
fn recover_database() {
    // 1. Check transaction log
    // 2. If last transaction was fully committed, database is consistent
    // 3. If last transaction was incomplete, ignore it (rollback)
    // 4. Database is now in consistent state
}
```

**Real-world result**: Database will be in the state of the last successful commit. No corruption, no partial updates, no data loss of committed transactions.

### 3. How does MVCC enable concurrent reads?

**The Traditional Database Problem:**
```rust
// Without MVCC: Readers and writers block each other
let write_lock = db.write_lock()?; // Blocks ALL readers
write_data()?;
write_lock.release(); // Now readers can proceed
```

**MDBX's MVCC Solution:**
```rust
// With MVCC: Readers and writers work simultaneously
let reader1 = db.tx()?; // Sees version N
let reader2 = db.tx()?; // Sees version N  
let writer = db.tx_mut()?; // Creates version N+1

// All three can work simultaneously!
```

**How MVCC works - Version Management:**

1. **Each transaction sees a consistent snapshot**
   ```rust
   // Time T1: Database has accounts {Alice: 100, Bob: 50}
   let reader = db.tx()?; // Sees snapshot at T1
   
   // Time T2: Writer changes Alice: 100 → 90, Bob: 50 → 60
   let writer = db.tx_mut()?;
   writer.put("Alice", 90)?;
   writer.put("Bob", 60)?;
   writer.commit()?;
   
   // Reader still sees {Alice: 100, Bob: 50} - consistent snapshot!
   assert_eq!(reader.get("Alice")?, 100);
   ```

2. **Writers create new versions without affecting readers**
   - Version N: Alice=100, Bob=50 (readers see this)
   - Version N+1: Alice=90, Bob=60 (new readers will see this)
   - Both versions exist simultaneously

3. **Garbage collection happens when safe**
   ```rust
   // Old versions cleaned up when no readers need them
   // This is why long-running read transactions are problematic!
   ```

**Why long-running reads are problematic:**
```rust
// BAD: Long-running read transaction
let tx = db.tx()?; // Starts at version N
let data = tx.get(key)?;
expensive_computation_for_hours(data); // tx still open!
// Prevents cleanup of versions N, N+1, N+2, ... N+1000
// Database file grows unnecessarily
```

MVCC is like having a time machine - each transaction travels back to when it started and sees that version of the database, while new transactions see the present.