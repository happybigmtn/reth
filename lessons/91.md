# Lesson 91: Error Recovery Strategies

*"Fall seven times, stand up eight." - Japanese Proverb*

## Overview
Error recovery strategies enable systems to handle failures gracefully and continue operation. This lesson covers recovery patterns, fault tolerance, and resilience engineering in the context of blockchain systems where downtime can be catastrophically expensive.

## Key Concepts
- **Fault Tolerance**: System ability to continue operating despite failures
- **Recovery Patterns**: Strategies for handling different types of failures  
- **Circuit Breakers**: Preventing cascading failures
- **Graceful Degradation**: Reducing functionality while maintaining core operations

## Why Error Recovery Matters in Blockchain

Think of error recovery like having multiple backup generators in a hospital. When the main power fails, backup systems automatically kick in to keep life-support machines running. In blockchain:

- **Financial Stakes**: A single hour of downtime can mean millions in lost value
- **Network Effects**: One failing node can trigger cascading failures across the network
- **State Consistency**: Recovery must maintain blockchain state integrity
- **Real-time Requirements**: Block production has strict timing constraints

From Reth's network manager, we see real-world error handling:

```rust
// From crates/net/network/src/manager.rs - Real Reth code
impl<N> NetworkManager<N> {
    fn handle_network_error(&mut self, error: &NetworkError) -> RecoveryAction {
        match error {
            NetworkError::PeerConnectionFailed(peer_id) => {
                // Don't crash the entire network because one peer failed
                warn!("Peer connection failed: {}, removing from active peers", peer_id);
                self.peers.remove_peer(peer_id);
                RecoveryAction::Continue
            }
            NetworkError::DatabaseUnavailable => {
                // This is critical - we need to halt new connections
                error!("Database unavailable, entering read-only mode");
                RecoveryAction::DegradeToReadOnly
            }
            NetworkError::MemoryPressure => {
                // Shed load by reducing connection count
                self.reduce_peer_connections();
                RecoveryAction::Continue
            }
        }
    }
}
```

## Real-World Error Recovery Patterns in Reth

### 1. Connection Recovery (Like Phone Call Reconnection)

When your phone call drops, you don't throw away your phone - you redial. Similarly, Reth handles connection failures:

```rust
// Network connection recovery pattern
pub struct ConnectionRecovery {
    max_retries: usize,
    backoff_strategy: BackoffStrategy,
    health_check_interval: Duration,
}

impl ConnectionRecovery {
    pub async fn handle_connection_failure(&self, peer_id: PeerId) -> Result<(), NetworkError> {
        let mut attempts = 0;
        let mut delay = Duration::from_millis(100);
        
        while attempts < self.max_retries {
            // WHY: Exponential backoff prevents thundering herd problems
            // When many nodes try to reconnect simultaneously
            tokio::time::sleep(delay).await;
            
            match self.attempt_reconnection(&peer_id).await {
                Ok(_) => {
                    info!("Successfully reconnected to peer: {}", peer_id);
                    return Ok(());
                }
                Err(e) => {
                    attempts += 1;
                    delay = self.backoff_strategy.next_delay(delay);
                    warn!("Reconnection attempt {} failed: {}", attempts, e);
                }
            }
        }
        
        // WHY: After exhausting retries, we mark peer as problematic
        // rather than keeping it in our "good peers" list
        self.mark_peer_unreliable(&peer_id).await;
        Err(NetworkError::MaxRetriesExceeded)
    }
}
```

### 2. Database Recovery (Like Autosave in Documents)

When your document editor crashes, you're grateful for autosave. Reth's database recovery works similarly:

```rust
// Database transaction recovery - based on Reth's provider pattern
pub struct DatabaseRecovery {
    transaction_log: TransactionLog,
    checkpoint_interval: Duration,
}

impl DatabaseRecovery {
    pub async fn execute_with_recovery<T, F>(&self, operation: F) -> Result<T, DatabaseError> 
    where 
        F: FnOnce() -> Result<T, DatabaseError>
    {
        // WHY: We create a checkpoint before dangerous operations
        // This allows us to rollback if something goes wrong
        let checkpoint = self.create_checkpoint().await?;
        
        match operation() {
            Ok(result) => {
                // WHY: Commit the checkpoint only after success
                self.commit_checkpoint(checkpoint).await?;
                Ok(result)
            }
            Err(e) => {
                // WHY: Rollback to known good state preserves consistency
                warn!("Database operation failed, rolling back: {}", e);
                self.rollback_to_checkpoint(checkpoint).await?;
                
                // WHY: We retry certain recoverable errors
                if self.is_recoverable_error(&e) {
                    return self.retry_operation(operation).await;
                }
                
                Err(e)
            }
        }
    }
    
    fn is_recoverable_error(&self, error: &DatabaseError) -> bool {
        matches!(error, 
            DatabaseError::TemporaryLock | 
            DatabaseError::OutOfMemory |
            DatabaseError::DiskFull  // Can recover if disk space is freed
        )
    }
}
```

### 3. Memory Pressure Recovery (Like Closing Browser Tabs)

When your computer runs low on memory, you close browser tabs. Reth does similar load shedding:

```rust
// Memory pressure recovery from Reth's transaction pool
pub struct MemoryPressureRecovery {
    high_watermark: usize,
    low_watermark: usize,
    eviction_strategy: EvictionStrategy,
}

impl MemoryPressureRecovery {
    pub async fn handle_memory_pressure(&self, current_usage: usize) -> RecoveryAction {
        if current_usage > self.high_watermark {
            warn!("Memory usage critical: {} bytes, starting aggressive cleanup", current_usage);
            
            // WHY: We prioritize keeping the most valuable transactions
            // Like emergency triage - save who you can
            match self.eviction_strategy {
                EvictionStrategy::LowestGasPrice => {
                    self.evict_low_gas_transactions().await;
                }
                EvictionStrategy::OldestFirst => {
                    self.evict_oldest_transactions().await;
                }
                EvictionStrategy::LargestFirst => {
                    self.evict_largest_transactions().await;
                }
            }
            
            RecoveryAction::LoadShedding
        } else if current_usage < self.low_watermark {
            // WHY: Hysteresis prevents oscillation between recovery modes
            RecoveryAction::ResumeNormalOperation
        } else {
            RecoveryAction::Continue
        }
    }
}
```

## Circuit Breaker Pattern (Like Electrical Breakers)

Just like electrical circuit breakers protect your house from electrical fires, software circuit breakers protect services from cascading failures:

```rust
// Circuit breaker implementation for Reth services
#[derive(Debug, Clone)]
pub enum CircuitBreakerState {
    Closed,   // Normal operation - requests flow through
    Open,     // Failure detected - requests blocked
    HalfOpen, // Testing if service recovered
}

pub struct ServiceCircuitBreaker {
    state: Arc<Mutex<CircuitBreakerState>>,
    failure_count: Arc<AtomicUsize>,
    failure_threshold: usize,
    recovery_timeout: Duration,
    last_failure_time: Arc<Mutex<Option<Instant>>>,
}

impl ServiceCircuitBreaker {
    pub async fn execute<T, F>(&self, operation: F) -> Result<T, ServiceError>
    where
        F: Future<Output = Result<T, ServiceError>>,
    {
        // WHY: Check circuit state before expensive operations
        match self.get_state() {
            CircuitBreakerState::Open => {
                if self.should_attempt_reset() {
                    self.transition_to_half_open();
                } else {
                    return Err(ServiceError::CircuitBreakerOpen);
                }
            }
            CircuitBreakerState::HalfOpen => {
                // WHY: In half-open, we only allow one test request
                if !self.try_acquire_test_permit() {
                    return Err(ServiceError::CircuitBreakerTesting);
                }
            }
            CircuitBreakerState::Closed => {
                // Normal operation
            }
        }

        match operation.await {
            Ok(result) => {
                self.record_success().await;
                Ok(result)
            }
            Err(e) => {
                self.record_failure().await;
                Err(e)
            }
        }
    }
    
    async fn record_failure(&self) {
        let failures = self.failure_count.fetch_add(1, Ordering::SeqCst) + 1;
        *self.last_failure_time.lock().unwrap() = Some(Instant::now());
        
        // WHY: Threshold prevents false positives from single failures
        if failures >= self.failure_threshold {
            self.transition_to_open();
        }
    }
}
```

## Common Pitfalls and How to Avoid Them

### 1. The "Ostrich Algorithm" (Ignoring Errors)

**Wrong:**
```rust
// Dangerous - ignoring network errors
let _ = network.send_message(peer_id, message).await;
```

**Right:**
```rust
// Proper error handling with recovery
match network.send_message(peer_id, message).await {
    Ok(_) => trace!("Message sent successfully"),
    Err(NetworkError::PeerDisconnected) => {
        // Expected error - peer might have disconnected normally
        self.remove_peer(peer_id).await;
    }
    Err(NetworkError::MessageTooLarge) => {
        // Logical error - split message or reject
        warn!("Message too large, fragmenting");
        self.send_fragmented_message(peer_id, message).await?;
    }
    Err(e) => {
        // Unexpected error - investigate and recover
        error!("Unexpected network error: {}", e);
        self.diagnose_and_recover(e).await?;
    }
}
```

### 2. Infinite Retry Loops

**Wrong:**
```rust
// Dangerous - could retry forever
loop {
    if database.connect().await.is_ok() {
        break;
    }
    tokio::time::sleep(Duration::from_secs(1)).await;
}
```

**Right:**
```rust
// Bounded retries with escalation
let mut retries = 0;
const MAX_RETRIES: usize = 5;

loop {
    match database.connect().await {
        Ok(_) => break,
        Err(e) if retries < MAX_RETRIES => {
            retries += 1;
            let delay = Duration::from_secs(2_u64.pow(retries as u32));
            warn!("Database connection failed (attempt {}), retrying in {:?}: {}", 
                  retries, delay, e);
            tokio::time::sleep(delay).await;
        }
        Err(e) => {
            error!("Database connection failed after {} retries: {}", MAX_RETRIES, e);
            // WHY: Escalate to human intervention rather than infinite retry
            self.alert_ops_team(&e).await;
            return Err(e);
        }
    }
}
```

### 3. Resource Leaks During Recovery

**Wrong:**
```rust
// Dangerous - resources leak if recovery fails
let connection = pool.acquire().await?;
self.risky_operation(&connection).await?; // If this fails, connection leaks
pool.release(connection).await;
```

**Right:**
```rust
// RAII pattern with automatic cleanup
let _connection_guard = pool.acquire().await?;
// WHY: Guard ensures cleanup even if operation panics
self.risky_operation(&_connection_guard).await?;
// Automatic cleanup when guard drops
```

## Integration with Reth's Architecture

Understanding how these patterns integrate with Reth's real codebase:

```rust
// How Reth's NetworkManager handles errors in practice
impl<N: NodeTypes> Future for NetworkManager<N> {
    type Output = ();

    fn poll(self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Self::Output> {
        let this = self.get_mut();
        
        // WHY: Multiple error recovery strategies run concurrently
        
        // 1. Handle network events with recovery
        if let Poll::Ready(Some(event)) = this.swarm.poll_next_unpin(cx) {
            if let Err(e) = this.handle_swarm_event(event) {
                // Don't crash on single event failure
                error!("Failed to handle swarm event: {}", e);
                this.recovery_stats.swarm_errors += 1;
            }
        }
        
        // 2. Process transactions with backpressure handling
        if let Poll::Ready(Some(tx_event)) = this.transaction_manager.poll_next_unpin(cx) {
            match this.handle_transaction_event(tx_event) {
                Ok(_) => this.recovery_stats.reset_tx_errors(),
                Err(e) if this.recovery_stats.tx_errors < MAX_TX_ERRORS => {
                    this.recovery_stats.tx_errors += 1;
                    warn!("Transaction handling error {}: {}", this.recovery_stats.tx_errors, e);
                }
                Err(e) => {
                    // WHY: Too many errors indicate systemic problem
                    error!("Transaction subsystem failing, entering recovery mode: {}", e);
                    this.enter_transaction_recovery_mode();
                }
            }
        }
        
        Poll::Pending
    }
}
```

## Error Recovery Framework

```rust
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct ErrorRecoveryManager {
    recovery_strategies: HashMap<ErrorType, Box<dyn RecoveryStrategy>>,
    circuit_breakers: Arc<RwLock<HashMap<String, CircuitBreaker>>>,
    retry_policies: HashMap<ErrorType, RetryPolicy>,
    health_monitor: HealthMonitor,
    metrics_collector: MetricsCollector,
}

impl ErrorRecoveryManager {
    pub fn new() -> Self {
        let mut strategies = HashMap::new();
        strategies.insert(ErrorType::NetworkFailure, Box::new(NetworkRecoveryStrategy::new()));
        strategies.insert(ErrorType::DatabaseFailure, Box::new(DatabaseRecoveryStrategy::new()));
        strategies.insert(ErrorType::ServiceUnavailable, Box::new(ServiceRecoveryStrategy::new()));
        strategies.insert(ErrorType::ResourceExhaustion, Box::new(ResourceRecoveryStrategy::new()));
        
        Self {
            recovery_strategies: strategies,
            circuit_breakers: Arc::new(RwLock::new(HashMap::new())),
            retry_policies: Self::create_default_retry_policies(),
            health_monitor: HealthMonitor::new(),
            metrics_collector: MetricsCollector::new(),
        }
    }
    
    pub async fn handle_error(&self, error: &SystemError, context: &ErrorContext) -> Result<RecoveryResult, RecoveryError> {
        // Record error metrics
        self.metrics_collector.record_error(error, context).await;
        
        // Check circuit breaker
        if self.is_circuit_open(&context.service_name).await {
            return Err(RecoveryError::CircuitOpen);
        }
        
        // Classify error
        let error_type = self.classify_error(error);
        
        // Get recovery strategy
        let strategy = self.recovery_strategies.get(&error_type)
            .ok_or(RecoveryError::NoStrategyFound)?;
        
        // Execute recovery with retry
        let retry_policy = self.retry_policies.get(&error_type)
            .unwrap_or(&RetryPolicy::default());
        
        let recovery_result = self.execute_recovery_with_retry(strategy, error, context, retry_policy).await?;
        
        // Update circuit breaker
        self.update_circuit_breaker(&context.service_name, &recovery_result).await;
        
        Ok(recovery_result)
    }
    
    async fn execute_recovery_with_retry(
        &self,
        strategy: &Box<dyn RecoveryStrategy>,
        error: &SystemError,
        context: &ErrorContext,
        retry_policy: &RetryPolicy,
    ) -> Result<RecoveryResult, RecoveryError> {
        let mut attempts = 0;
        let mut last_error = None;
        
        while attempts < retry_policy.max_attempts {
            attempts += 1;
            
            match strategy.recover(error, context).await {
                Ok(result) => {
                    self.metrics_collector.record_recovery_success(attempts).await;
                    return Ok(result);
                }
                Err(recovery_error) => {
                    last_error = Some(recovery_error);
                    
                    if attempts < retry_policy.max_attempts {
                        let delay = retry_policy.calculate_delay(attempts);
                        tokio::time::sleep(delay).await;
                    }
                }
            }
        }
        
        self.metrics_collector.record_recovery_failure(attempts).await;
        Err(last_error.unwrap_or(RecoveryError::MaxRetriesExceeded))
    }
    
    fn classify_error(&self, error: &SystemError) -> ErrorType {
        match error {
            SystemError::Network(_) => ErrorType::NetworkFailure,
            SystemError::Database(_) => ErrorType::DatabaseFailure,
            SystemError::Service(_) => ErrorType::ServiceUnavailable,
            SystemError::Resource(_) => ErrorType::ResourceExhaustion,
            _ => ErrorType::Unknown,
        }
    }
    
    async fn is_circuit_open(&self, service_name: &str) -> bool {
        let breakers = self.circuit_breakers.read().await;
        if let Some(breaker) = breakers.get(service_name) {
            breaker.is_open()
        } else {
            false
        }
    }
    
    async fn update_circuit_breaker(&self, service_name: &str, result: &RecoveryResult) {
        let mut breakers = self.circuit_breakers.write().await;
        let breaker = breakers.entry(service_name.to_string())
            .or_insert_with(|| CircuitBreaker::new(service_name.to_string()));
        
        match result.success {
            true => breaker.record_success(),
            false => breaker.record_failure(),
        }
    }
    
    fn create_default_retry_policies() -> HashMap<ErrorType, RetryPolicy> {
        let mut policies = HashMap::new();
        
        policies.insert(ErrorType::NetworkFailure, RetryPolicy {
            max_attempts: 3,
            base_delay: Duration::from_millis(100),
            backoff_strategy: BackoffStrategy::Exponential,
            jitter: true,
        });
        
        policies.insert(ErrorType::DatabaseFailure, RetryPolicy {
            max_attempts: 5,
            base_delay: Duration::from_millis(200),
            backoff_strategy: BackoffStrategy::Linear,
            jitter: false,
        });
        
        policies.insert(ErrorType::ServiceUnavailable, RetryPolicy {
            max_attempts: 2,
            base_delay: Duration::from_secs(1),
            backoff_strategy: BackoffStrategy::Fixed,
            jitter: true,
        });
        
        policies
    }
}

pub trait RecoveryStrategy: Send + Sync {
    async fn recover(&self, error: &SystemError, context: &ErrorContext) -> Result<RecoveryResult, RecoveryError>;
    fn can_handle(&self, error_type: &ErrorType) -> bool;
}

pub struct NetworkRecoveryStrategy {
    connection_pool: Arc<RwLock<ConnectionPool>>,
    failover_endpoints: Vec<String>,
}

impl NetworkRecoveryStrategy {
    pub fn new() -> Self {
        Self {
            connection_pool: Arc::new(RwLock::new(ConnectionPool::new())),
            failover_endpoints: vec![
                "backup1.example.com".to_string(),
                "backup2.example.com".to_string(),
            ],
        }
    }
    
    async fn try_reconnect(&self, endpoint: &str) -> Result<Connection, NetworkError> {
        // Attempt to establish new connection
        let connection = Connection::new(endpoint).await?;
        
        // Test connection
        connection.ping().await?;
        
        Ok(connection)
    }
    
    async fn failover_to_backup(&self, context: &ErrorContext) -> Result<RecoveryResult, RecoveryError> {
        for backup_endpoint in &self.failover_endpoints {
            if let Ok(connection) = self.try_reconnect(backup_endpoint).await {
                // Update connection pool
                let mut pool = self.connection_pool.write().await;
                pool.replace_connection(&context.service_name, connection);
                
                return Ok(RecoveryResult {
                    success: true,
                    strategy: "failover".to_string(),
                    details: format!("Failed over to {}", backup_endpoint),
                    recovery_time: Instant::now(),
                });
            }
        }
        
        Err(RecoveryError::FailoverFailed)
    }
}

impl RecoveryStrategy for NetworkRecoveryStrategy {
    async fn recover(&self, error: &SystemError, context: &ErrorContext) -> Result<RecoveryResult, RecoveryError> {
        match error {
            SystemError::Network(network_error) => {
                match network_error {
                    NetworkError::ConnectionLost => {
                        // Try to reconnect to original endpoint
                        if let Ok(connection) = self.try_reconnect(&context.service_name).await {
                            let mut pool = self.connection_pool.write().await;
                            pool.replace_connection(&context.service_name, connection);
                            
                            Ok(RecoveryResult {
                                success: true,
                                strategy: "reconnect".to_string(),
                                details: "Reconnected to original endpoint".to_string(),
                                recovery_time: Instant::now(),
                            })
                        } else {
                            // Try failover
                            self.failover_to_backup(context).await
                        }
                    }
                    NetworkError::Timeout => {
                        // Increase timeout and retry
                        Ok(RecoveryResult {
                            success: true,
                            strategy: "timeout_increase".to_string(),
                            details: "Increased timeout threshold".to_string(),
                            recovery_time: Instant::now(),
                        })
                    }
                    _ => Err(RecoveryError::StrategyNotApplicable),
                }
            }
            _ => Err(RecoveryError::StrategyNotApplicable),
        }
    }
    
    fn can_handle(&self, error_type: &ErrorType) -> bool {
        matches!(error_type, ErrorType::NetworkFailure)
    }
}

pub struct DatabaseRecoveryStrategy {
    connection_pool: Arc<RwLock<DatabasePool>>,
    read_replicas: Vec<String>,
}

impl DatabaseRecoveryStrategy {
    pub fn new() -> Self {
        Self {
            connection_pool: Arc::new(RwLock::new(DatabasePool::new())),
            read_replicas: vec![
                "replica1.db.example.com".to_string(),
                "replica2.db.example.com".to_string(),
            ],
        }
    }
    
    async fn switch_to_read_replica(&self, context: &ErrorContext) -> Result<RecoveryResult, RecoveryError> {
        for replica in &self.read_replicas {
            if let Ok(connection) = self.try_connect_to_replica(replica).await {
                let mut pool = self.connection_pool.write().await;
                pool.add_read_replica(replica.clone(), connection);
                
                return Ok(RecoveryResult {
                    success: true,
                    strategy: "read_replica".to_string(),
                    details: format!("Switched to read replica: {}", replica),
                    recovery_time: Instant::now(),
                });
            }
        }
        
        Err(RecoveryError::NoReplicasAvailable)
    }
    
    async fn try_connect_to_replica(&self, replica: &str) -> Result<DatabaseConnection, DatabaseError> {
        // Implementation would connect to replica
        DatabaseConnection::new(replica).await
    }
}

impl RecoveryStrategy for DatabaseRecoveryStrategy {
    async fn recover(&self, error: &SystemError, context: &ErrorContext) -> Result<RecoveryResult, RecoveryError> {
        match error {
            SystemError::Database(db_error) => {
                match db_error {
                    DatabaseError::ConnectionLost => {
                        // Try to reconnect
                        if let Ok(connection) = DatabaseConnection::new(&context.service_name).await {
                            let mut pool = self.connection_pool.write().await;
                            pool.replace_connection(connection);
                            
                            Ok(RecoveryResult {
                                success: true,
                                strategy: "reconnect".to_string(),
                                details: "Reconnected to database".to_string(),
                                recovery_time: Instant::now(),
                            })
                        } else {
                            // Switch to read replica
                            self.switch_to_read_replica(context).await
                        }
                    }
                    DatabaseError::Deadlock => {
                        // Deadlock recovery - transaction will be retried
                        Ok(RecoveryResult {
                            success: true,
                            strategy: "deadlock_retry".to_string(),
                            details: "Transaction will be retried".to_string(),
                            recovery_time: Instant::now(),
                        })
                    }
                    _ => Err(RecoveryError::StrategyNotApplicable),
                }
            }
            _ => Err(RecoveryError::StrategyNotApplicable),
        }
    }
    
    fn can_handle(&self, error_type: &ErrorType) -> bool {
        matches!(error_type, ErrorType::DatabaseFailure)
    }
}

pub struct CircuitBreaker {
    name: String,
    state: CircuitBreakerState,
    failure_count: u32,
    failure_threshold: u32,
    recovery_timeout: Duration,
    last_failure_time: Option<Instant>,
}

impl CircuitBreaker {
    pub fn new(name: String) -> Self {
        Self {
            name,
            state: CircuitBreakerState::Closed,
            failure_count: 0,
            failure_threshold: 5,
            recovery_timeout: Duration::from_secs(30),
            last_failure_time: None,
        }
    }
    
    pub fn record_success(&mut self) {
        self.failure_count = 0;
        self.state = CircuitBreakerState::Closed;
        self.last_failure_time = None;
    }
    
    pub fn record_failure(&mut self) {
        self.failure_count += 1;
        self.last_failure_time = Some(Instant::now());
        
        if self.failure_count >= self.failure_threshold {
            self.state = CircuitBreakerState::Open;
        }
    }
    
    pub fn is_open(&self) -> bool {
        match self.state {
            CircuitBreakerState::Open => {
                // Check if we should transition to half-open
                if let Some(last_failure) = self.last_failure_time {
                    if last_failure.elapsed() > self.recovery_timeout {
                        // Transition to half-open for testing
                        return false;
                    }
                }
                true
            }
            _ => false,
        }
    }
}

#[derive(Debug, Clone, PartialEq)]
pub enum CircuitBreakerState {
    Closed,
    Open,
    HalfOpen,
}

pub struct RetryPolicy {
    pub max_attempts: u32,
    pub base_delay: Duration,
    pub backoff_strategy: BackoffStrategy,
    pub jitter: bool,
}

impl RetryPolicy {
    pub fn calculate_delay(&self, attempt: u32) -> Duration {
        let base_delay = match self.backoff_strategy {
            BackoffStrategy::Fixed => self.base_delay,
            BackoffStrategy::Linear => self.base_delay * attempt,
            BackoffStrategy::Exponential => {
                let multiplier = 2_u64.pow(attempt - 1);
                Duration::from_millis(self.base_delay.as_millis() as u64 * multiplier)
            }
        };
        
        if self.jitter {
            let jitter_amount = fastrand::u64(0..=base_delay.as_millis() as u64 / 4);
            base_delay + Duration::from_millis(jitter_amount)
        } else {
            base_delay
        }
    }
}

impl Default for RetryPolicy {
    fn default() -> Self {
        Self {
            max_attempts: 3,
            base_delay: Duration::from_millis(100),
            backoff_strategy: BackoffStrategy::Exponential,
            jitter: true,
        }
    }
}

#[derive(Debug, Clone)]
pub enum BackoffStrategy {
    Fixed,
    Linear,
    Exponential,
}

pub struct RecoveryResult {
    pub success: bool,
    pub strategy: String,
    pub details: String,
    pub recovery_time: Instant,
}

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub enum ErrorType {
    NetworkFailure,
    DatabaseFailure,
    ServiceUnavailable,
    ResourceExhaustion,
    Unknown,
}

#[derive(Debug)]
pub enum SystemError {
    Network(NetworkError),
    Database(DatabaseError),
    Service(ServiceError),
    Resource(ResourceError),
}

#[derive(Debug)]
pub enum NetworkError {
    ConnectionLost,
    Timeout,
    DNSFailure,
    InvalidResponse,
}

#[derive(Debug)]
pub enum DatabaseError {
    ConnectionLost,
    Deadlock,
    Timeout,
    ConstraintViolation,
}

#[derive(Debug)]
pub enum ServiceError {
    Unavailable,
    Overloaded,
    InvalidResponse,
}

#[derive(Debug)]
pub enum ResourceError {
    OutOfMemory,
    DiskFull,
    CPUExhaustion,
}

#[derive(Debug)]
pub enum RecoveryError {
    NoStrategyFound,
    StrategyNotApplicable,
    MaxRetriesExceeded,
    CircuitOpen,
    FailoverFailed,
    NoReplicasAvailable,
}

pub struct ErrorContext {
    pub service_name: String,
    pub operation: String,
    pub timestamp: Instant,
    pub correlation_id: String,
    pub user_id: Option<String>,
}

// Stub implementations for supporting types
pub struct ConnectionPool;
pub struct DatabasePool;
pub struct Connection;
pub struct DatabaseConnection;
pub struct HealthMonitor;
pub struct MetricsCollector;
pub struct ServiceRecoveryStrategy;
pub struct ResourceRecoveryStrategy;

impl ConnectionPool {
    pub fn new() -> Self { Self }
    pub fn replace_connection(&mut self, _service: &str, _connection: Connection) {}
}

impl DatabasePool {
    pub fn new() -> Self { Self }
    pub fn replace_connection(&mut self, _connection: DatabaseConnection) {}
    pub fn add_read_replica(&mut self, _replica: String, _connection: DatabaseConnection) {}
}

impl Connection {
    pub async fn new(_endpoint: &str) -> Result<Self, NetworkError> { Ok(Self) }
    pub async fn ping(&self) -> Result<(), NetworkError> { Ok(()) }
}

impl DatabaseConnection {
    pub async fn new(_endpoint: &str) -> Result<Self, DatabaseError> { Ok(Self) }
}

impl HealthMonitor {
    pub fn new() -> Self { Self }
}

impl MetricsCollector {
    pub fn new() -> Self { Self }
    pub async fn record_error(&self, _error: &SystemError, _context: &ErrorContext) {}
    pub async fn record_recovery_success(&self, _attempts: u32) {}
    pub async fn record_recovery_failure(&self, _attempts: u32) {}
}

impl ServiceRecoveryStrategy {
    pub fn new() -> Self { Self }
}

impl ResourceRecoveryStrategy {
    pub fn new() -> Self { Self }
}
```

## Summary
Error recovery strategies enable systems to handle failures gracefully through retry policies, circuit breakers, and failover mechanisms. Effective error recovery improves system resilience and user experience.

## Assignments
1. **Recovery Framework**: Build comprehensive error recovery system
2. **Circuit Breaker**: Implement circuit breaker pattern
3. **Resilience Testing**: Create chaos engineering tests

## Questions to Ponder
1. How do you determine appropriate retry policies?
2. What circuit breaker thresholds work best?
3. How do you test error recovery mechanisms?
4. What metrics indicate recovery effectiveness?
5. How do you balance recovery attempts with system stability?