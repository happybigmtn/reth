# Lesson 83: Database Backup and Recovery

*"The best time to plant a tree was 20 years ago. The second best time is now." - Chinese Proverb*

## Overview
Database backup and recovery ensures data integrity and availability. This lesson covers backup strategies, recovery procedures, and disaster recovery planning.

## Key Concepts
- **Backup Strategies**: Different approaches to data protection
- **Recovery Procedures**: Restoring from backups
- **Point-in-Time Recovery**: Restoring to specific moments
- **Disaster Recovery**: Comprehensive recovery planning

## Backup Strategy Implementation

```rust
pub struct BackupManager {
    storage_provider: Arc<dyn StorageProvider>,
    backup_config: BackupConfig,
    backup_scheduler: BackupScheduler,
    compression_engine: CompressionEngine,
    encryption_manager: EncryptionManager,
}

impl BackupManager {
    pub fn new(storage_provider: Arc<dyn StorageProvider>, config: BackupConfig) -> Self {
        Self {
            storage_provider,
            backup_config: config.clone(),
            backup_scheduler: BackupScheduler::new(config.schedule),
            compression_engine: CompressionEngine::new(config.compression),
            encryption_manager: EncryptionManager::new(config.encryption),
        }
    }
    
    pub async fn create_full_backup(&self) -> Result<BackupId, BackupError> {
        let backup_id = BackupId::new();
        let backup_path = self.backup_config.backup_directory.join(format!("full_{}", backup_id));
        
        // Create backup manifest
        let manifest = self.create_backup_manifest(BackupType::Full, &backup_id)?;
        
        // Backup database files
        self.backup_database_files(&backup_path, &manifest).await?;
        
        // Backup configuration
        self.backup_configuration(&backup_path).await?;
        
        // Backup chain data
        self.backup_chain_data(&backup_path).await?;
        
        // Compress backup
        let compressed_path = self.compress_backup(&backup_path).await?;
        
        // Encrypt backup
        let encrypted_path = self.encrypt_backup(&compressed_path).await?;
        
        // Store backup metadata
        self.store_backup_metadata(&backup_id, &manifest, &encrypted_path).await?;
        
        // Clean up temporary files
        self.cleanup_temporary_files(&backup_path).await?;
        
        Ok(backup_id)
    }
    
    pub async fn create_incremental_backup(&self, base_backup: BackupId) -> Result<BackupId, BackupError> {
        let backup_id = BackupId::new();
        let backup_path = self.backup_config.backup_directory.join(format!("incremental_{}", backup_id));
        
        // Get last backup info
        let last_backup = self.get_backup_info(base_backup).await?;
        
        // Create incremental manifest
        let manifest = self.create_incremental_manifest(&backup_id, &last_backup)?;
        
        // Backup changed files since last backup
        self.backup_changed_files(&backup_path, &manifest, last_backup.timestamp).await?;
        
        // Backup transaction logs
        self.backup_transaction_logs(&backup_path, last_backup.timestamp).await?;
        
        // Compress and encrypt
        let compressed_path = self.compress_backup(&backup_path).await?;
        let encrypted_path = self.encrypt_backup(&compressed_path).await?;
        
        // Store metadata
        self.store_backup_metadata(&backup_id, &manifest, &encrypted_path).await?;
        
        // Clean up
        self.cleanup_temporary_files(&backup_path).await?;
        
        Ok(backup_id)
    }
    
    async fn backup_database_files(&self, backup_path: &Path, manifest: &BackupManifest) -> Result<(), BackupError> {
        let database_path = self.storage_provider.get_database_path();
        
        // Create database backup directory
        let db_backup_path = backup_path.join("database");
        fs::create_dir_all(&db_backup_path).await?;
        
        // Backup main database files
        for file in self.storage_provider.get_database_files()? {
            let source_path = database_path.join(&file);
            let dest_path = db_backup_path.join(&file);
            
            // Copy file with verification
            self.copy_file_with_verification(&source_path, &dest_path).await?;
        }
        
        // Backup indices
        self.backup_indices(&db_backup_path).await?;
        
        // Backup metadata
        self.backup_database_metadata(&db_backup_path).await?;
        
        Ok(())
    }
    
    async fn copy_file_with_verification(&self, source: &Path, dest: &Path) -> Result<(), BackupError> {
        // Calculate source file hash
        let source_hash = self.calculate_file_hash(source).await?;
        
        // Copy file
        fs::copy(source, dest).await?;
        
        // Verify copied file
        let dest_hash = self.calculate_file_hash(dest).await?;
        
        if source_hash != dest_hash {
            return Err(BackupError::VerificationFailed);
        }
        
        Ok(())
    }
    
    async fn calculate_file_hash(&self, path: &Path) -> Result<B256, BackupError> {
        let mut file = File::open(path).await?;
        let mut hasher = Sha256::new();
        
        let mut buffer = [0u8; 8192];
        loop {
            let bytes_read = file.read(&mut buffer).await?;
            if bytes_read == 0 {
                break;
            }
            hasher.update(&buffer[..bytes_read]);
        }
        
        Ok(B256::from_slice(&hasher.finalize()))
    }
}
```

## Recovery Engine

```rust
pub struct RecoveryEngine {
    backup_manager: Arc<BackupManager>,
    storage_provider: Arc<dyn StorageProvider>,
    recovery_config: RecoveryConfig,
    validation_engine: ValidationEngine,
}

impl RecoveryEngine {
    pub async fn restore_from_backup(&self, backup_id: BackupId, target_path: &Path) -> Result<(), RecoveryError> {
        // Get backup metadata
        let backup_info = self.backup_manager.get_backup_info(backup_id).await?;
        
        // Validate backup integrity
        self.validate_backup_integrity(&backup_info).await?;
        
        // Prepare recovery environment
        self.prepare_recovery_environment(target_path).await?;
        
        // Decrypt backup
        let decrypted_path = self.decrypt_backup(&backup_info.path).await?;
        
        // Decompress backup
        let decompressed_path = self.decompress_backup(&decrypted_path).await?;
        
        // Restore database files
        self.restore_database_files(&decompressed_path, target_path).await?;
        
        // Restore configuration
        self.restore_configuration(&decompressed_path, target_path).await?;
        
        // Restore chain data
        self.restore_chain_data(&decompressed_path, target_path).await?;
        
        // Validate restored data
        self.validate_restored_data(target_path).await?;
        
        // Clean up temporary files
        self.cleanup_recovery_files(&decrypted_path).await?;
        
        Ok(())
    }
    
    pub async fn point_in_time_recovery(&self, target_time: SystemTime) -> Result<(), RecoveryError> {
        // Find the latest full backup before target time
        let base_backup = self.find_base_backup_before_time(target_time).await?;
        
        // Find all incremental backups between base backup and target time
        let incremental_backups = self.find_incremental_backups_in_range(
            base_backup.backup_id,
            target_time,
        ).await?;
        
        // Restore from base backup
        let temp_path = self.create_temporary_recovery_path()?;
        self.restore_from_backup(base_backup.backup_id, &temp_path).await?;
        
        // Apply incremental backups in order
        for incremental_backup in incremental_backups {
            self.apply_incremental_backup(&incremental_backup, &temp_path).await?;
        }
        
        // Apply transaction logs up to target time
        self.apply_transaction_logs_until_time(&temp_path, target_time).await?;
        
        // Replace current database with recovered data
        self.replace_current_database(&temp_path).await?;
        
        Ok(())
    }
    
    async fn apply_incremental_backup(&self, backup: &BackupInfo, target_path: &Path) -> Result<(), RecoveryError> {
        // Decrypt and decompress incremental backup
        let decrypted_path = self.decrypt_backup(&backup.path).await?;
        let decompressed_path = self.decompress_backup(&decrypted_path).await?;
        
        // Apply changes from incremental backup
        self.apply_incremental_changes(&decompressed_path, target_path).await?;
        
        // Clean up
        self.cleanup_recovery_files(&decrypted_path).await?;
        
        Ok(())
    }
    
    async fn apply_transaction_logs_until_time(&self, target_path: &Path, target_time: SystemTime) -> Result<(), RecoveryError> {
        let log_files = self.find_transaction_logs_until_time(target_time).await?;
        
        for log_file in log_files {
            let transactions = self.parse_transaction_log(&log_file).await?;
            
            for transaction in transactions {
                if transaction.timestamp > target_time {
                    break;
                }
                
                // Apply transaction to recovered database
                self.apply_transaction_to_recovered_db(&transaction, target_path).await?;
            }
        }
        
        Ok(())
    }
    
    async fn validate_backup_integrity(&self, backup_info: &BackupInfo) -> Result<(), RecoveryError> {
        // Check backup file exists
        if !backup_info.path.exists() {
            return Err(RecoveryError::BackupFileNotFound);
        }
        
        // Verify backup checksum
        let calculated_hash = self.calculate_file_hash(&backup_info.path).await?;
        if calculated_hash != backup_info.checksum {
            return Err(RecoveryError::ChecksumMismatch);
        }
        
        // Validate backup manifest
        self.validate_backup_manifest(&backup_info.manifest).await?;
        
        Ok(())
    }
    
    async fn validate_restored_data(&self, path: &Path) -> Result<(), RecoveryError> {
        // Validate database integrity
        self.validation_engine.validate_database_integrity(path).await?;
        
        // Validate chain consistency
        self.validation_engine.validate_chain_consistency(path).await?;
        
        // Validate state consistency
        self.validation_engine.validate_state_consistency(path).await?;
        
        Ok(())
    }
}
```

## Backup Scheduling

```rust
pub struct BackupScheduler {
    schedule: BackupSchedule,
    job_queue: Arc<Mutex<VecDeque<BackupJob>>>,
    worker_handles: Vec<JoinHandle<()>>,
}

impl BackupScheduler {
    pub fn new(schedule: BackupSchedule) -> Self {
        Self {
            schedule,
            job_queue: Arc::new(Mutex::new(VecDeque::new())),
            worker_handles: Vec::new(),
        }
    }
    
    pub fn start(&mut self, backup_manager: Arc<BackupManager>) -> Result<(), SchedulerError> {
        // Start schedule checker
        self.start_schedule_checker(backup_manager.clone())?;
        
        // Start worker threads
        for _ in 0..self.schedule.worker_threads {
            let handle = self.start_worker_thread(backup_manager.clone());
            self.worker_handles.push(handle);
        }
        
        Ok(())
    }
    
    fn start_schedule_checker(&self, backup_manager: Arc<BackupManager>) -> Result<(), SchedulerError> {
        let schedule = self.schedule.clone();
        let job_queue = self.job_queue.clone();
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(60)); // Check every minute
            
            loop {
                interval.tick().await;
                
                let now = SystemTime::now();
                
                // Check for full backup schedule
                if Self::should_run_full_backup(&schedule, now) {
                    let job = BackupJob::new(BackupType::Full, now);
                    job_queue.lock().await.push_back(job);
                }
                
                // Check for incremental backup schedule
                if Self::should_run_incremental_backup(&schedule, now) {
                    let job = BackupJob::new(BackupType::Incremental, now);
                    job_queue.lock().await.push_back(job);
                }
            }
        });
        
        Ok(())
    }
    
    fn start_worker_thread(&self, backup_manager: Arc<BackupManager>) -> JoinHandle<()> {
        let job_queue = self.job_queue.clone();
        
        tokio::spawn(async move {
            loop {
                let job = {
                    let mut queue = job_queue.lock().await;
                    queue.pop_front()
                };
                
                if let Some(job) = job {
                    match job.backup_type {
                        BackupType::Full => {
                            if let Err(e) = backup_manager.create_full_backup().await {
                                error!("Full backup failed: {}", e);
                            }
                        }
                        BackupType::Incremental => {
                            // Find latest backup as base
                            if let Ok(base_backup) = backup_manager.get_latest_backup().await {
                                if let Err(e) = backup_manager.create_incremental_backup(base_backup.backup_id).await {
                                    error!("Incremental backup failed: {}", e);
                                }
                            }
                        }
                    }
                } else {
                    // No jobs available, wait
                    tokio::time::sleep(Duration::from_secs(1)).await;
                }
            }
        })
    }
    
    fn should_run_full_backup(schedule: &BackupSchedule, now: SystemTime) -> bool {
        // Check if it's time for full backup based on schedule
        match schedule.full_backup_interval {
            BackupInterval::Daily => {
                // Check if it's the scheduled hour
                true // Simplified logic
            }
            BackupInterval::Weekly => {
                // Check if it's the scheduled day and hour
                true // Simplified logic
            }
            BackupInterval::Monthly => {
                // Check if it's the scheduled day of month and hour
                true // Simplified logic
            }
        }
    }
    
    fn should_run_incremental_backup(schedule: &BackupSchedule, now: SystemTime) -> bool {
        // Check if it's time for incremental backup
        match schedule.incremental_backup_interval {
            BackupInterval::Hourly => true, // Simplified logic
            BackupInterval::Daily => false, // Would check specific time
            _ => false,
        }
    }
}

#[derive(Clone)]
pub struct BackupSchedule {
    pub full_backup_interval: BackupInterval,
    pub incremental_backup_interval: BackupInterval,
    pub retention_policy: RetentionPolicy,
    pub worker_threads: usize,
}

pub enum BackupInterval {
    Hourly,
    Daily,
    Weekly,
    Monthly,
}

pub struct RetentionPolicy {
    pub keep_daily: usize,
    pub keep_weekly: usize,
    pub keep_monthly: usize,
    pub keep_yearly: usize,
}
```

## Disaster Recovery

```rust
pub struct DisasterRecoveryManager {
    recovery_engine: Arc<RecoveryEngine>,
    backup_manager: Arc<BackupManager>,
    replication_manager: ReplicationManager,
    failover_config: FailoverConfig,
}

impl DisasterRecoveryManager {
    pub async fn execute_disaster_recovery(&self, scenario: DisasterScenario) -> Result<(), DisasterRecoveryError> {
        match scenario {
            DisasterScenario::DatabaseCorruption => {
                self.recover_from_database_corruption().await
            }
            DisasterScenario::HardwareFailure => {
                self.recover_from_hardware_failure().await
            }
            DisasterScenario::SiteFailure => {
                self.recover_from_site_failure().await
            }
            DisasterScenario::SecurityBreach => {
                self.recover_from_security_breach().await
            }
        }
    }
    
    async fn recover_from_database_corruption(&self) -> Result<(), DisasterRecoveryError> {
        // 1. Stop all database operations
        self.stop_database_operations().await?;
        
        // 2. Assess corruption extent
        let corruption_report = self.assess_database_corruption().await?;
        
        // 3. Determine recovery strategy
        let recovery_strategy = self.determine_recovery_strategy(&corruption_report)?;
        
        // 4. Execute recovery
        match recovery_strategy {
            RecoveryStrategy::RepairInPlace => {
                self.repair_database_in_place().await?;
            }
            RecoveryStrategy::RestoreFromBackup => {
                let latest_backup = self.backup_manager.get_latest_valid_backup().await?;
                self.recovery_engine.restore_from_backup(latest_backup.backup_id, &self.get_database_path()).await?;
            }
            RecoveryStrategy::PointInTimeRecovery => {
                let recovery_point = self.determine_recovery_point(&corruption_report)?;
                self.recovery_engine.point_in_time_recovery(recovery_point).await?;
            }
        }
        
        // 5. Validate recovery
        self.validate_recovery().await?;
        
        // 6. Resume operations
        self.resume_database_operations().await?;
        
        Ok(())
    }
    
    async fn recover_from_site_failure(&self) -> Result<(), DisasterRecoveryError> {
        // 1. Activate failover site
        self.activate_failover_site().await?;
        
        // 2. Restore from remote backups
        let remote_backup = self.get_latest_remote_backup().await?;
        self.recovery_engine.restore_from_backup(remote_backup.backup_id, &self.get_failover_database_path()).await?;
        
        // 3. Sync any missing data
        self.sync_missing_data().await?;
        
        // 4. Redirect traffic to failover site
        self.redirect_traffic_to_failover().await?;
        
        // 5. Monitor failover operations
        self.monitor_failover_operations().await?;
        
        Ok(())
    }
    
    async fn create_recovery_plan(&self) -> Result<RecoveryPlan, DisasterRecoveryError> {
        let plan = RecoveryPlan {
            recovery_time_objective: self.failover_config.rto,
            recovery_point_objective: self.failover_config.rpo,
            backup_locations: self.get_backup_locations().await?,
            failover_procedures: self.get_failover_procedures().await?,
            testing_schedule: self.failover_config.testing_schedule.clone(),
        };
        
        Ok(plan)
    }
    
    pub async fn test_disaster_recovery(&self) -> Result<TestResults, DisasterRecoveryError> {
        let mut results = TestResults::new();
        
        // Test backup integrity
        results.backup_integrity = self.test_backup_integrity().await?;
        
        // Test recovery procedures
        results.recovery_procedures = self.test_recovery_procedures().await?;
        
        // Test failover mechanisms
        results.failover_mechanisms = self.test_failover_mechanisms().await?;
        
        // Test communication systems
        results.communication_systems = self.test_communication_systems().await?;
        
        Ok(results)
    }
}
```

## Summary
Database backup and recovery systems ensure data durability and business continuity. Comprehensive backup strategies, automated recovery procedures, and disaster recovery planning protect against data loss and system failures.

## Assignments
1. **Backup System**: Build automated backup and recovery system
2. **Recovery Testing**: Create comprehensive recovery testing framework
3. **Disaster Recovery**: Implement disaster recovery procedures

## Questions to Ponder
1. How do you balance backup frequency with performance?
2. What are the trade-offs between different backup strategies?
3. How do you test recovery procedures without disrupting operations?
4. What are the key metrics for backup and recovery systems?
5. How do you ensure backup security and compliance?