# Lesson 31: Static Files and Cold Storage

*"The worthwhile problems are the ones you can really solve or help solve, the ones you can really contribute something to." - Richard Feynman*

## Files with Inline Comments for This Lesson
- `crates/static-file/static-file/src/static_file_producer.rs` - Main static file producer implementation
- `crates/storage/nippy-jar/src/lib.rs` - NippyJar immutable storage format
- `crates/static-file/static-file/src/segments/mod.rs` - Static file data segments
- `crates/static-file/types/src/segment.rs` - Static file segment types

## What Are Static Files?

Static files represent Reth's approach to storing immutable blockchain data efficiently. Instead of keeping all historical data in the main database, finalized data is moved to compressed, read-only files that provide better storage efficiency and access patterns.

```
Database Evolution in Reth:
┌──────────────────────────────────────────────────┐
│               Active Database                    │
│  ┌─────────────┬─────────────┬─────────────┐     │
│  │ Block 1000  │ Block 1001  │ Block 1002  │     │
│  │ (Active)    │ (Pending)   │ (Building)  │     │
│  └─────────────┴─────────────┴─────────────┘     │
└──────────────────────────────────────────────────┘
                      ↓ (After finalization)
┌──────────────────────────────────────────────────┐
│                Static Files                      │
│  ┌─────────────┬─────────────┬─────────────┐     │
│  │ headers.jar │ bodies.jar  │receipts.jar │     │
│  │ Blocks 0-999│ Blocks 0-999│ Blocks 0-999│     │
│  │ Compressed  │ Compressed  │ Compressed  │     │
│  │ Read-only   │ Read-only   │ Read-only   │     │
│  └─────────────┴─────────────┴─────────────┘     │
└──────────────────────────────────────────────────┘

Benefits:
• 70-90% space savings through compression
• Faster sequential reads via memory mapping
• Reduced database maintenance overhead
• Parallel access to different data types
```

## The Static File Producer

The Static File Producer is responsible for moving finalized data from the database to static files:

```rust
/// The main orchestrator for moving data to static files
/// Located in: crates/static-file/static-file/src/static_file_producer.rs

#[derive(Debug)]
pub struct StaticFileProducer<Provider> {
    /// Provider for database and static file access
    provider: Provider,
    
    /// Pruning configuration - determines what can be moved
    prune_modes: PruneModes,
    
    /// Event notifier for monitoring progress
    event_sender: EventSender<StaticFileProducerEvent>,
}

impl<Provider> StaticFileProducerInner<Provider>
where
    Provider: StaticFileProviderFactory + DatabaseProviderFactory,
{
    /// Move data from database to static files
    /// This is the main entry point for static file creation
    pub fn run(&self, targets: StaticFileTargets) -> StaticFileProducerResult {
        // Early exit if no work to do
        if !targets.any() {
            return Ok(targets);
        }
        
        self.event_sender.notify(StaticFileProducerEvent::Started { 
            targets: targets.clone() 
        });
        
        let start = Instant::now();
        let mut segments = Vec::new();
        
        // Prepare work for each data type
        if let Some(block_range) = targets.transactions.clone() {
            segments.push((Box::new(segments::Transactions), block_range));
        }
        if let Some(block_range) = targets.headers.clone() {
            segments.push((Box::new(segments::Headers), block_range));
        }
        if let Some(block_range) = targets.receipts.clone() {
            segments.push((Box::new(segments::Receipts), block_range));
        }
        
        // Process all segments in parallel for maximum throughput
        segments.par_iter().try_for_each(|(segment, block_range)| -> ProviderResult<()> {
            let provider = self.provider.database_provider_ro()?
                .disable_long_read_transaction_safety();
            
            // Each segment knows how to copy its data efficiently
            segment.copy_to_static_files(provider, block_range.clone())?;
            Ok(())
        })?;
        
        // Commit all changes atomically
        self.provider.static_file_provider().commit()?;
        
        // Update indices for fast lookups
        for (segment, block_range) in segments {
            self.provider.static_file_provider()
                .update_index(segment.segment(), Some(*block_range.end()))?;
        }
        
        self.event_sender.notify(StaticFileProducerEvent::Finished { 
            targets: targets.clone(), 
            elapsed: start.elapsed() 
        });
        
        Ok(targets)
    }
    
    /// Determine what data should be moved to static files
    /// Only finalized data that won't be pruned should be moved
    pub fn get_static_file_targets(
        &self,
        finalized_block_numbers: HighestStaticFiles,
    ) -> ProviderResult<StaticFileTargets> {
        let highest_static_files = self.provider
            .static_file_provider()
            .get_highest_static_files();
        
        let targets = StaticFileTargets {
            // Headers can always be moved to static files
            headers: finalized_block_numbers.headers.and_then(|finalized_block_number| {
                self.get_static_file_target(
                    highest_static_files.headers, 
                    finalized_block_number
                )
            }),
            
            // Receipts only if they're not being pruned
            receipts: if self.prune_modes.receipts.is_none() &&
                self.prune_modes.receipts_log_filter.is_empty()
            {
                finalized_block_numbers.receipts.and_then(|finalized_block_number| {
                    self.get_static_file_target(
                        highest_static_files.receipts,
                        finalized_block_number,
                    )
                })
            } else {
                None
            },
            
            // Transactions can be moved if not being pruned
            transactions: finalized_block_numbers.transactions.and_then(|finalized_block_number| {
                self.get_static_file_target(
                    highest_static_files.transactions,
                    finalized_block_number,
                )
            }),
            
            block_meta: finalized_block_numbers.block_meta.and_then(|finalized_block_number| {
                self.get_static_file_target(
                    highest_static_files.block_meta, 
                    finalized_block_number
                )
            }),
        };
        
        Ok(targets)
    }
    
    /// Calculate the range of blocks to move for a segment
    fn get_static_file_target(
        &self,
        highest_static_file: Option<BlockNumber>,
        finalized_block_number: BlockNumber,
    ) -> Option<RangeInclusive<BlockNumber>> {
        // Create a range from the last static file block to the finalized block
        let range = highest_static_file
            .map_or(0, |block| block + 1)..=finalized_block_number;
        
        // Only return the range if it's not empty
        (!range.is_empty()).then_some(range)
    }
}
```

## The NippyJar Format

NippyJar is Reth's custom storage format designed for immutable data with optimal compression and access patterns:

```rust
/// High-performance immutable storage format
/// Located in: crates/storage/nippy-jar/src/lib.rs

#[derive(Serialize, Deserialize)]
pub struct NippyJar<H = ()> {
    /// Format version for compatibility
    version: usize,
    
    /// User-defined metadata (e.g., block ranges)
    user_header: H,
    
    /// Number of data columns (e.g., headers, bodies, receipts)
    columns: usize,
    
    /// Number of data rows (blocks)
    rows: usize,
    
    /// Compression algorithm (Zstd or LZ4)
    compressor: Option<Compressors>,
    
    /// Maximum uncompressed row size for buffer allocation
    max_row_size: usize,
    
    /// File path for the main data
    path: PathBuf,
}

impl<H: NippyJarHeader> NippyJar<H> {
    /// Create a new jar with specified compression
    pub fn new(columns: usize, path: &Path, user_header: H) -> Self {
        Self {
            version: NIPPY_JAR_VERSION,
            user_header,
            columns,
            rows: 0,
            max_row_size: 0,
            compressor: None,
            filter: None,
            phf: None,
            path: path.to_path_buf(),
        }
    }
    
    /// Add Zstd compression with optional dictionary training
    /// Dictionary training provides better compression for similar data
    pub fn with_zstd(mut self, use_dict: bool, max_dict_size: usize) -> Self {
        self.compressor = Some(Compressors::Zstd(
            compression::Zstd::new(use_dict, max_dict_size, self.columns)
        ));
        self
    }
    
    /// Add LZ4 compression for faster decompression
    pub fn with_lz4(mut self) -> Self {
        self.compressor = Some(Compressors::Lz4(
            compression::Lz4::default()
        ));
        self
    }
}
```

## Memory-Mapped Data Access

Static files use memory mapping for efficient data access:

```rust
/// Memory-mapped data reader for static files
/// Located in: crates/storage/nippy-jar/src/lib.rs

#[derive(Debug)]
pub struct DataReader {
    /// Data file descriptor (must stay alive for mmap)
    data_file: File,
    
    /// Memory-mapped data for zero-copy reads
    data_mmap: Mmap,
    
    /// Offset file for locating specific rows
    offset_file: File,
    
    /// Memory-mapped offsets
    offset_mmap: Mmap,
    
    /// Size of each offset entry
    offset_size: u8,
}

impl DataReader {
    /// Create a new reader for a static file
    pub fn new(path: impl AsRef<Path>) -> Result<Self, NippyJarError> {
        let data_file = File::open(path.as_ref())?;
        
        // SAFETY: File is read-only and kept alive
        let data_mmap = unsafe { Mmap::map(&data_file)? };
        
        let offset_file = File::open(
            path.as_ref().with_extension(OFFSETS_FILE_EXTENSION)
        )?;
        
        // SAFETY: File is read-only and kept alive
        let offset_mmap = unsafe { Mmap::map(&offset_file)? };
        
        // First byte indicates offset size
        let offset_size = offset_mmap[0];
        
        if offset_size > 8 || offset_size == 0 {
            return Err(NippyJarError::InvalidOffsetSize { offset_size });
        }
        
        Ok(Self { 
            data_file, 
            data_mmap, 
            offset_file, 
            offset_size, 
            offset_mmap 
        })
    }
    
    /// Get the offset for a specific row index
    pub fn offset(&self, index: usize) -> Result<u64, NippyJarError> {
        // Skip the first byte which stores offset size
        let from = index * self.offset_size as usize + 1;
        self.offset_at(from)
    }
    
    /// Read raw data for a given range
    /// This provides zero-copy access to the compressed data
    pub fn data(&self, range: Range<usize>) -> &[u8] {
        &self.data_mmap[range]
    }
    
    /// Read an offset value from the memory-mapped offset file
    fn offset_at(&self, index: usize) -> Result<u64, NippyJarError> {
        let mut buffer: [u8; 8] = [0; 8];
        
        let offset_end = index.saturating_add(self.offset_size as usize);
        if offset_end > self.offset_mmap.len() {
            return Err(NippyJarError::OffsetOutOfBounds { index });
        }
        
        // Copy only the required bytes
        buffer[..self.offset_size as usize]
            .copy_from_slice(&self.offset_mmap[index..offset_end]);
        
        Ok(u64::from_le_bytes(buffer))
    }
}
```

## Storage Strategy and Benefits

Static files provide several key advantages over keeping all data in the database:

```rust
/// Example of static file usage in practice
/// This shows how data flows from database to static files

pub fn static_file_workflow_example() -> Result<(), Box<dyn std::error::Error>> {
    // 1. Node processes new blocks in the database
    let recent_blocks = process_new_blocks()?;
    
    // 2. After finalization (typically 64 blocks), data becomes immutable
    let finalized_height = get_finalized_block_height()?;
    
    // 3. Static file producer identifies data to move
    let targets = static_file_producer.get_static_file_targets(
        HighestStaticFiles {
            headers: Some(finalized_height),
            receipts: Some(finalized_height),
            transactions: Some(finalized_height),
            block_meta: Some(finalized_height),
        }
    )?;
    
    // 4. Data is compressed and written to static files
    if targets.any() {
        static_file_producer.run(targets)?;
        
        // 5. Database entries can now be pruned to free space
        pruner.run(finalized_height)?;
    }
    
    Ok(())
}

/// Compression effectiveness varies by data type
pub fn compression_ratios_example() {
    // Headers: ~60-70% compression
    // - Mostly hashes and numbers
    // - Good compression with Zstd + dictionary
    
    // Transaction bodies: ~40-50% compression
    // - Variable-length data
    // - Some repeated patterns (common contract calls)
    
    // Receipts: ~70-80% compression
    // - Many zero values (successful transactions)
    // - Repeated log patterns
    // - Excellent compression with dictionary training
}
```

## Assignments

### Assignment 1: Basic Static File Creation
Create a simple static file producer that demonstrates the basic concepts:

```rust
use std::path::Path;
use tempfile::TempDir;

/// Create a minimal static file with sample blockchain data
fn create_sample_static_file() -> Result<(), Box<dyn std::error::Error>> {
    // TODO: Implement the following:
    // 1. Create a NippyJar with 3 columns (block_hash, timestamp, gas_used)
    // 2. Add LZ4 compression
    // 3. Create sample data for 10 blocks
    // 4. Write the data to the static file
    // 5. Verify the file was created correctly
    
    todo!("Implement static file creation")
}
```

### Assignment 2: Compression Analysis
Compare different compression strategies:

```rust
/// Compare compression ratios and access times for different algorithms
fn compression_comparison() -> Result<(), Box<dyn std::error::Error>> {
    // TODO: Implement the following:
    // 1. Create sample receipt data (mix of successful and failed transactions)
    // 2. Create three NippyJars: uncompressed, LZ4, and Zstd
    // 3. Measure compression ratios
    // 4. Measure decompression times for random access
    // 5. Report which algorithm works best for different access patterns
    
    todo!("Implement compression analysis")
}
```

### Assignment 3: Advanced Static File Management
Implement a static file manager with pruning integration:

```rust
/// Advanced static file manager that coordinates with pruning
struct StaticFileManager {
    producer: StaticFileProducer<TestProvider>,
    pruner: Pruner<TestProvider>,
    finalized_height: BlockNumber,
}

impl StaticFileManager {
    /// Orchestrate the movement of data from database to static files
    fn process_finalized_blocks(&mut self) -> Result<StorageStats, Box<dyn std::error::Error>> {
        // TODO: Implement the following:
        // 1. Determine what data can be moved to static files
        // 2. Check that data isn't marked for pruning
        // 3. Move data to static files with appropriate compression
        // 4. Run pruner to remove data from database
        // 5. Return statistics about space saved
        
        todo!("Implement static file management workflow")
    }
}

struct StorageStats {
    database_size_before: u64,
    database_size_after: u64,
    static_file_size: u64,
    compression_ratio: f64,
}
```

## Assignment Answers

### Assignment 1 Answer: Basic Static File Creation

```rust
use reth_storage_nippy_jar::{NippyJar, NippyJarWriter};
use alloy_primitives::{B256, U256};
use std::path::Path;
use tempfile::TempDir;

fn create_sample_static_file() -> Result<(), Box<dyn std::error::Error>> {
    let temp_dir = TempDir::new()?;
    let file_path = temp_dir.path().join("sample_blocks");
    
    // 1. Create a NippyJar with 3 columns
    let mut jar = NippyJar::new_without_header(3, &file_path);
    
    // 2. Add LZ4 compression for fast decompression
    jar = jar.with_lz4();
    
    // 3. Create sample blockchain data for 10 blocks
    let mut columns = Vec::new();
    for i in 0..3 {
        let mut column_data = Vec::new();
        for block_num in 0..10 {
            match i {
                0 => {
                    // Block hash column
                    let hash = B256::from_slice(&[block_num as u8; 32]);
                    column_data.push(Ok(hash.as_slice().to_vec()));
                },
                1 => {
                    // Timestamp column  
                    let timestamp = 1000000 + block_num * 12; // 12 second blocks
                    column_data.push(Ok(timestamp.to_le_bytes().to_vec()));
                },
                2 => {
                    // Gas used column
                    let gas_used = 15000000 + (block_num * 1000000); // Varying gas usage
                    column_data.push(Ok(gas_used.to_le_bytes().to_vec()));
                },
                _ => unreachable!(),
            }
        }
        columns.push(column_data);
    }
    
    // 4. Write the data to static file
    jar = jar.freeze(columns, 10)?;
    
    // 5. Verify the file was created correctly
    let loaded_jar = NippyJar::load_without_header(&file_path)?;
    assert_eq!(loaded_jar.rows(), 10);
    assert_eq!(loaded_jar.columns(), 3);
    assert!(loaded_jar.compressor().is_some());
    
    // Verify we can read the data back
    let data_reader = loaded_jar.open_data_reader()?;
    assert!(data_reader.size() > 0);
    assert_eq!(data_reader.offsets_count()?, 30); // 10 rows * 3 columns
    
    println!("Successfully created static file with {} rows and {} columns", 
             loaded_jar.rows(), loaded_jar.columns());
    println!("Compressed data size: {} bytes", data_reader.size());
    
    Ok(())
}

// Analysis: This demonstrates the basic static file workflow. The LZ4 compression
// provides fast access while still reducing storage requirements. The columnar
// format allows efficient access to specific data types (e.g., just timestamps).
```

### Assignment 2 Answer: Compression Analysis

```rust
use reth_storage_nippy_jar::{NippyJar, NippyJarCursor, compression::Compressors};
use std::time::Instant;
use rand::{Rng, SeedableRng};
use rand::rngs::StdRng;

fn compression_comparison() -> Result<(), Box<dyn std::error::Error>> {
    let temp_dir = TempDir::new()?;
    let mut rng = StdRng::seed_from_u64(42);
    
    // 1. Create sample receipt data (realistic Ethereum receipts)
    let num_receipts = 1000;
    let mut status_data = Vec::new();
    let mut gas_used_data = Vec::new();
    let mut logs_data = Vec::new();
    
    for i in 0..num_receipts {
        // Status: 1 for success, 0 for failure (90% success rate)
        let status = if rng.gen_bool(0.9) { 1u8 } else { 0u8 };
        status_data.push(Ok(vec![status]));
        
        // Gas used: varies based on transaction complexity
        let gas_used = if status == 1 {
            rng.gen_range(21000..500000u64)
        } else {
            rng.gen_range(21000..100000u64) // Failed txs use less gas
        };
        gas_used_data.push(Ok(gas_used.to_le_bytes().to_vec()));
        
        // Logs: successful transactions may have logs, failed ones don't
        let logs_count = if status == 1 {
            rng.gen_range(0..5)
        } else { 0 };
        let mut logs = Vec::new();
        for _ in 0..logs_count {
            logs.extend_from_slice(&[0u8; 32]); // Topic hash
            logs.extend_from_slice(&[0u8; 32]); // Data
        }
        logs_data.push(Ok(logs));
    }
    
    // 2. Create three different jars with different compression
    let mut results = Vec::new();
    
    // Uncompressed
    {
        let file_path = temp_dir.path().join("receipts_none");
        let jar = NippyJar::new_without_header(3, &file_path);
        
        let start = Instant::now();
        let jar = jar.freeze(vec![
            status_data.clone(),
            gas_used_data.clone(), 
            logs_data.clone()
        ], num_receipts)?;
        let write_time = start.elapsed();
        
        let size = std::fs::metadata(jar.data_path())?.len();
        results.push(("Uncompressed", size, write_time, jar.data_path().to_path_buf()));
    }
    
    // LZ4 compressed
    {
        let file_path = temp_dir.path().join("receipts_lz4");
        let jar = NippyJar::new_without_header(3, &file_path).with_lz4();
        
        let start = Instant::now();
        let jar = jar.freeze(vec![
            status_data.clone(),
            gas_used_data.clone(), 
            logs_data.clone()
        ], num_receipts)?;
        let write_time = start.elapsed();
        
        let size = std::fs::metadata(jar.data_path())?.len();
        results.push(("LZ4", size, write_time, jar.data_path().to_path_buf()));
    }
    
    // Zstd compressed (with dictionary)
    {
        let file_path = temp_dir.path().join("receipts_zstd");
        let mut jar = NippyJar::new_without_header(3, &file_path)
            .with_zstd(true, 8192);
        
        // Prepare compression dictionary
        jar.prepare_compression(vec![
            status_data.iter().map(|r| r.as_ref().unwrap().clone()).collect(),
            gas_used_data.iter().map(|r| r.as_ref().unwrap().clone()).collect(),
            logs_data.iter().map(|r| r.as_ref().unwrap().clone()).collect(),
        ])?;
        
        let start = Instant::now();
        let jar = jar.freeze(vec![
            status_data.clone(),
            gas_used_data.clone(), 
            logs_data.clone()
        ], num_receipts)?;
        let write_time = start.elapsed();
        
        let size = std::fs::metadata(jar.data_path())?.len();
        results.push(("Zstd+Dict", size, write_time, jar.data_path().to_path_buf()));
    }
    
    // 3. Calculate compression ratios
    let uncompressed_size = results[0].1;
    println!("\n=== Compression Analysis ===");
    for (name, size, write_time, path) in &results {
        let ratio = (uncompressed_size as f64 - *size as f64) / uncompressed_size as f64 * 100.0;
        println!("{}: {} bytes ({:.1}% compression), write time: {:?}", 
                 name, size, ratio, write_time);
        
        // 4. Test random access performance
        let jar = NippyJar::load_without_header(path)?;
        let mut cursor = NippyJarCursor::new(&jar)?;
        
        let access_start = Instant::now();
        for _ in 0..100 {
            let random_idx = rng.gen_range(0..num_receipts as usize);
            cursor.row_by_number(random_idx)?;
        }
        let access_time = access_start.elapsed();
        println!("  Random access (100 reads): {:?}", access_time);
    }
    
    // 5. Analysis and recommendations
    println!("\n=== Analysis ===");
    println!("Best compression: Zstd with dictionary training");
    println!("Best for random access: LZ4 (fastest decompression)");
    println!("Best for sequential reads: Zstd (highest compression)");
    println!("Recommendation: Use Zstd for archival data, LZ4 for frequently accessed data");
    
    Ok(())
}

// Analysis: Receipt data compresses very well due to many zero values and repeated
// patterns. Zstd with dictionary training achieves the best compression ratio 
// (~80-85%) while LZ4 provides the fastest random access. The choice depends on
// the access pattern - archival data benefits from maximum compression while
// active data benefits from fast access.
```

### Assignment 3 Answer: Advanced Static File Management

```rust
use reth_static_file::{StaticFileProducer, StaticFileTargets};
use reth_prune::{Pruner, PruneMode};
use reth_provider::{DatabaseProviderFactory, StaticFileProviderFactory};
use alloy_primitives::BlockNumber;

struct StaticFileManager<Provider> {
    producer: StaticFileProducer<Provider>,
    pruner: Pruner<Provider::ProviderRW, Provider>,
    finalized_height: BlockNumber,
    last_processed_height: Option<BlockNumber>,
}

struct StorageStats {
    database_size_before: u64,
    database_size_after: u64,
    static_file_size: u64,
    compression_ratio: f64,
    blocks_processed: u64,
}

impl<Provider> StaticFileManager<Provider>
where 
    Provider: StaticFileProviderFactory + DatabaseProviderFactory + Clone,
    Provider::ProviderRW: reth_provider::PruneCheckpointWriter + reth_provider::PruneCheckpointReader,
{
    fn new(
        provider: Provider,
        prune_modes: reth_prune_types::PruneModes,
        finalized_height: BlockNumber,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        let producer = StaticFileProducer::new(provider.clone(), prune_modes.clone());
        let pruner = Pruner::new_with_factory(
            provider,
            vec![], // Segments would be populated based on configuration
            100,    // Min block interval
            10000,  // Delete limit
            None,   // No timeout
            tokio::sync::watch::channel(reth_exex_types::FinishedExExHeight::NoExExs).1,
        );
        
        Ok(Self {
            producer,
            pruner,
            finalized_height,
            last_processed_height: None,
        })
    }
    
    fn process_finalized_blocks(&mut self) -> Result<StorageStats, Box<dyn std::error::Error>> {
        let producer_inner = self.producer.lock();
        
        // 1. Determine what data can be moved to static files
        let start_height = self.last_processed_height
            .map(|h| h + 1)
            .unwrap_or(0);
        
        if start_height > self.finalized_height {
            return Ok(StorageStats {
                database_size_before: 0,
                database_size_after: 0,
                static_file_size: 0,
                compression_ratio: 0.0,
                blocks_processed: 0,
            });
        }
        
        // 2. Check current storage size
        let db_size_before = self.get_database_size()?;
        
        // 3. Get targets for static file creation
        let targets = producer_inner.get_static_file_targets(
            reth_static_file_types::HighestStaticFiles {
                headers: Some(self.finalized_height),
                receipts: Some(self.finalized_height),
                transactions: Some(self.finalized_height),
                block_meta: Some(self.finalized_height),
            }
        )?;
        
        if !targets.any() {
            return Ok(StorageStats {
                database_size_before: db_size_before,
                database_size_after: db_size_before,
                static_file_size: 0,
                compression_ratio: 0.0,
                blocks_processed: 0,
            });
        }
        
        println!("Moving blocks {} to {} to static files", start_height, self.finalized_height);
        
        // 4. Move data to static files with compression
        producer_inner.run(targets.clone())?;
        
        // 5. Calculate static file sizes
        let static_file_size = self.calculate_static_file_size(&targets)?;
        
        drop(producer_inner); // Release the lock
        
        // 6. Run pruner to remove data from database
        let mut pruner = self.pruner.lock();
        let prune_result = pruner.run(self.finalized_height)?;
        drop(pruner);
        
        // 7. Calculate final storage statistics
        let db_size_after = self.get_database_size()?;
        let blocks_processed = self.finalized_height - start_height + 1;
        
        // Calculate compression ratio based on space savings
        let original_data_size = db_size_before - db_size_after + static_file_size;
        let compression_ratio = if original_data_size > 0 {
            static_file_size as f64 / original_data_size as f64
        } else {
            1.0
        };
        
        self.last_processed_height = Some(self.finalized_height);
        
        let stats = StorageStats {
            database_size_before: db_size_before,
            database_size_after: db_size_after,
            static_file_size,
            compression_ratio,
            blocks_processed,
        };
        
        println!("Storage optimization complete:");
        println!("  Blocks processed: {}", stats.blocks_processed);
        println!("  Database size: {} -> {} bytes ({:.1}% reduction)", 
                 stats.database_size_before, 
                 stats.database_size_after,
                 (1.0 - stats.database_size_after as f64 / stats.database_size_before as f64) * 100.0);
        println!("  Static file size: {} bytes", stats.static_file_size);
        println!("  Compression ratio: {:.1}%", (1.0 - stats.compression_ratio) * 100.0);
        
        Ok(stats)
    }
    
    fn get_database_size(&self) -> Result<u64, Box<dyn std::error::Error>> {
        // In a real implementation, this would query the database size
        // For this example, we'll simulate it
        Ok(1_000_000_000) // 1GB simulated size
    }
    
    fn calculate_static_file_size(
        &self, 
        targets: &StaticFileTargets
    ) -> Result<u64, Box<dyn std::error::Error>> {
        let mut total_size = 0u64;
        
        // Calculate sizes for each static file type
        if targets.headers.is_some() {
            total_size += 50_000_000; // ~50MB for headers
        }
        if targets.receipts.is_some() {
            total_size += 200_000_000; // ~200MB for receipts
        }
        if targets.transactions.is_some() {
            total_size += 300_000_000; // ~300MB for transactions  
        }
        if targets.block_meta.is_some() {
            total_size += 10_000_000; // ~10MB for block metadata
        }
        
        Ok(total_size)
    }
    
    /// Estimate the potential space savings from static file conversion
    fn estimate_savings(&self, block_range: u64) -> StorageEstimate {
        // Based on real-world Reth data, static files typically achieve:
        // - Headers: ~65% compression
        // - Receipts: ~80% compression  
        // - Transactions: ~45% compression
        // - Overall database size reduction: ~60-70%
        
        let estimated_raw_size = block_range * 1_000_000; // ~1MB per block average
        let estimated_compressed_size = (estimated_raw_size as f64 * 0.35) as u64; // ~65% compression
        let estimated_db_reduction = (estimated_raw_size as f64 * 0.7) as u64; // 70% reduction
        
        StorageEstimate {
            raw_data_size: estimated_raw_size,
            compressed_size: estimated_compressed_size,
            database_reduction: estimated_db_reduction,
            net_savings: estimated_db_reduction - estimated_compressed_size,
        }
    }
}

struct StorageEstimate {
    raw_data_size: u64,
    compressed_size: u64,
    database_reduction: u64,
    net_savings: u64,
}

// Analysis: This implementation shows how static files work with pruning to optimize
// storage. The key insight is that only finalized, non-prunable data should be moved
// to static files. The compression ratios achieved (typically 60-80%) combined with
// the ability to prune the original database entries provide significant space savings.
// The columnar format enables efficient compression and selective access patterns.
```

## Questions to Ponder

1. **Compression Strategy**: Why might Reth choose different compression algorithms for different data types? Consider the trade-offs between compression ratio and decompression speed.

2. **Memory Mapping**: How does memory-mapped file access improve performance compared to traditional file I/O? What are the potential drawbacks?

3. **Data Lifecycle**: At what point should blockchain data be moved from the active database to static files? How does this relate to Ethereum's finality guarantees?

4. **Recovery Scenarios**: How would you handle corruption in static files? What backup and recovery strategies would be appropriate?

5. **Query Optimization**: How could the columnar format of static files be leveraged to optimize specific types of blockchain queries?
