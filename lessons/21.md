# Lesson 21: Block Execution Flow

*"I think it's much more interesting to live not knowing than to have answers which might be wrong." - Richard Feynman*

## Why This Lesson Matters

Understanding block execution is like understanding how a factory assembly line works. Each transaction is like a product moving through the line, and the block executor is the assembly line itself. If you don't understand this flow, you can't debug when things go wrong, optimize for performance, or add new features.

**Real-world analogy**: Think of block execution like processing a batch of bank transactions overnight. Each transaction must be processed in exact order, balances must be updated atomically, and if any transaction fails, you need to know exactly what state you're left in.

## Files with Inline Comments for This Lesson
- `crates/ethereum/evm/src/execute.rs` - Block execution implementation
- `crates/evm/evm/src/metrics.rs` - Execution metrics and monitoring
- `crates/ethereum/evm/src/executor.rs` - Block executor logic
- `crates/execution-types/src/execution_outcome.rs` - Execution results

## The Heart of Ethereum: Block Execution

Block execution is where the magic happens. It's the process that takes a block full of transactions and transforms the world state. Every transfer, every smart contract call, every DeFi swap - it all goes through this pipeline.

**WHY is this so important?** Because this is where Ethereum enforces its rules. Without proper execution, you could have double-spending, invalid state transitions, or consensus failures.

```
Block Execution Flow:
Pending Block → Validate → Execute Transactions → Update State → Generate Receipts
     ↓              ↓              ↓                 ↓              ↓
   Mempool      Check Gas      Run EVM Code      Merkle Root    Proof of Work
```

**Common Pitfall**: Many people think block execution is just "running the transactions." But it's actually a complex dance of validation, state management, gas accounting, and proof generation.

## Anatomy of Block Execution

### Phase 1: Pre-Execution Setup
**WHY this matters**: Before you can run transactions, you need to set up the environment. Like a chef preparing ingredients before cooking.

- Load the current state root
- Initialize the EVM with block context (gas limit, timestamp, etc.)
- Set up gas tracking and metrics
- Prepare for collecting receipts

### Phase 2: Transaction Processing Loop  
**WHY sequential**: Transactions must be processed in exact order because later transactions might depend on the state changes from earlier ones.

- Validate transaction (signature, nonce, gas limit)
- Execute transaction in EVM
- Update account states
- Generate receipt
- Accumulate gas used

### Phase 3: Post-Execution Finalization
**WHY this step**: You need to "commit" all the changes and generate proofs.

- Calculate new state root
- Generate receipts root
- Update block header
- Return execution outcome

## The Executor in Action (Real Reth Code)

### ExecutorMetrics: Watching Performance
**WHY metrics matter**: Without metrics, optimizing Ethereum execution is like flying blind. From actual `crates/evm/evm/src/metrics.rs`:

```rust
/// Executor metrics from real Reth codebase
#[derive(Metrics, Clone)]
#[metrics(scope = "sync.execution")]
pub struct ExecutorMetrics {
    /// The total amount of gas processed - tracks network load
    pub gas_processed_total: Counter,
    /// Instantaneous gas processing rate - shows current performance
    pub gas_per_second: Gauge,
    /// Distribution of gas usage per block - helps identify bottlenecks
    pub gas_used_histogram: Histogram,
    
    /// Time to execute blocks - critical for sync performance
    pub execution_histogram: Histogram,
    pub execution_duration: Gauge,
    
    /// State access patterns - shows database load
    pub accounts_loaded_histogram: Histogram,
    pub storage_slots_loaded_histogram: Histogram,
    pub bytecodes_loaded_histogram: Histogram,
}

/// WHY these metrics exist:
/// - gas_per_second: If this drops below 10M gas/sec during sync, 
///   you might have database I/O issues
/// - accounts_loaded: High values (>1000) might indicate inefficient 
///   contract code or need for state caching
/// - execution_duration: Should stay under 50ms for most blocks;
///   spikes indicate complex transactions or performance issues
```

**Real-world analogy**: These metrics are like the dashboard in your car. Gas per second is your speedometer, execution duration is your RPM, and state access patterns show if your engine is working efficiently.

### Metered Execution: The Secret Sauce
**WHY metering matters**: You can't improve what you don't measure. Here's how Reth tracks execution performance:

```rust
/// From actual Reth code - this is how metrics are collected
impl ExecutorMetrics {
    fn metered<F, R, B>(&self, block: &RecoveredBlock<B>, f: F) -> R
    where
        F: FnOnce() -> R,
        B: Block,
    {
        // Start timing execution
        let execute_start = Instant::now();
        
        // Execute the actual block (this is where the magic happens)
        let output = f();
        
        // Calculate performance metrics
        let execution_duration = execute_start.elapsed().as_secs_f64();
        
        // Update metrics atomically
        self.gas_processed_total.increment(block.header().gas_used());
        
        // Calculate instantaneous gas processing rate
        // WHY this calculation: Shows how efficiently we're processing transactions
        self.gas_per_second.set(block.header().gas_used() as f64 / execution_duration);
        
        // Record for histogram analysis
        self.gas_used_histogram.record(block.header().gas_used() as f64);
        self.execution_histogram.record(execution_duration);
        
        output
    }
}
```

## The Pruner Implementation

### Core Pruner Structure

```rust
/// The main pruner that manages data cleanup
pub struct Pruner<DB, PF> {
    /// Database provider factory
    provider_factory: PF,
    /// Pruning configuration
    modes: PruneModes,
    /// Metrics for monitoring
    metrics: PrunerMetrics,
    /// Limiter to control pruning speed
    limiter: PruneLimiter,
    /// Hook for handling pruned data
    pruned_block_hook: Option<Box<dyn PrunedBlockHook>>,
    /// Statistics from last run
    stats: PruneStats,
}

impl<DB, PF> Pruner<DB, PF>
where
    PF: DatabaseProviderFactory<DB>,
    DB: Database,
{
    /// Create a new pruner with configuration
    pub fn new(provider_factory: PF, modes: PruneModes) -> Self {
        Self {
            provider_factory,
            modes,
            metrics: PrunerMetrics::default(),
            limiter: PruneLimiter::default(),
            pruned_block_hook: None,
            stats: PruneStats::default(),
        }
    }
    
    /// Run a pruning cycle
    pub async fn run(&mut self, tip: BlockNumber) -> Result<PruneProgress, PruneError> {
        let start_time = std::time::Instant::now();
        let mut progress = PruneProgress::default();
        
        // Get provider for database operations
        let provider = self.provider_factory.database_provider_rw()?;
        
        // Prune each data type according to its mode
        if let Some(mode) = &self.modes.account_history {
            if let Some(target) = mode.prune_target(tip, 0) {
                let pruned = self.prune_account_history(&provider, target).await?;
                progress.account_history = pruned;
            }
        }
        
        if let Some(mode) = &self.modes.storage_history {
            if let Some(target) = mode.prune_target(tip, 0) {
                let pruned = self.prune_storage_history(&provider, target).await?;
                progress.storage_history = pruned;
            }
        }
        
        if let Some(mode) = &self.modes.receipts {
            if let Some(target) = mode.prune_target(tip, 0) {
                let pruned = self.prune_receipts(&provider, target).await?;
                progress.receipts = pruned;
            }
        }
        
        // Commit changes
        provider.commit()?;
        
        // Update metrics
        let duration = start_time.elapsed();
        self.metrics.record_prune_duration(duration);
        self.metrics.record_pruned_entries(progress.total_pruned());
        
        Ok(progress)
    }
    
    /// Prune account history data
    async fn prune_account_history(
        &mut self,
        provider: &DatabaseProvider<DB>,
        target: BlockNumber,
    ) -> Result<PruneSegmentProgress, PruneError> {
        let mut progress = PruneSegmentProgress::default();
        
        // Use cursor to iterate through account history
        let mut cursor = provider.tx_ref().cursor_write::<tables::AccountsHistory>()?;
        
        // Find entries to prune (older than target block)
        let mut entries_to_delete = Vec::new();
        
        for entry in cursor.walk_range(..)? {
            let (key, block_list) = entry?;
            
            // Filter out old blocks from the block list
            let original_len = block_list.len();
            let filtered: Vec<_> = block_list
                .into_iter()
                .filter(|&block| block > target)
                .collect();
                
            if filtered.len() < original_len {
                if filtered.is_empty() {
                    // Delete entire entry
                    entries_to_delete.push(key);
                } else {
                    // Update with filtered list
                    cursor.upsert(key, filtered.into())?;
                }
                
                progress.pruned += original_len - filtered.len();
            }
            
            // Respect rate limiting
            if progress.pruned % 1000 == 0 {
                self.limiter.enforce_limit().await;
            }
        }
        
        // Delete entries that became empty
        for key in entries_to_delete {
            cursor.delete_current()?;
        }
        
        progress.processed = cursor.walk_range(..)?.count();
        
        Ok(progress)
    }
    
    /// Prune storage history data
    async fn prune_storage_history(
        &mut self,
        provider: &DatabaseProvider<DB>,
        target: BlockNumber,
    ) -> Result<PruneSegmentProgress, PruneError> {
        let mut progress = PruneSegmentProgress::default();
        
        // Similar to account history but for storage
        let mut cursor = provider.tx_ref().cursor_write::<tables::StoragesHistory>()?;
        
        for entry in cursor.walk_range(..)? {
            let (key, block_list) = entry?;
            
            let original_len = block_list.len();
            let filtered: Vec<_> = block_list
                .into_iter()
                .filter(|&block| block > target)
                .collect();
                
            if filtered.len() < original_len {
                if filtered.is_empty() {
                    cursor.delete_current()?;
                } else {
                    cursor.upsert(key, filtered.into())?;
                }
                
                progress.pruned += original_len - filtered.len();
            }
            
            if progress.pruned % 1000 == 0 {
                self.limiter.enforce_limit().await;
            }
        }
        
        Ok(progress)
    }
    
    /// Prune receipts data
    async fn prune_receipts(
        &mut self,
        provider: &DatabaseProvider<DB>,
        target: BlockNumber,
    ) -> Result<PruneSegmentProgress, PruneError> {
        let mut progress = PruneSegmentProgress::default();
        
        // Remove receipts for old blocks
        let mut cursor = provider.tx_ref().cursor_write::<tables::Receipts>()?;
        
        // Walk through receipts and delete old ones
        for entry in cursor.walk_range(..target)? {
            let (block_number, _receipts) = entry?;
            
            if block_number < target {
                cursor.delete_current()?;
                progress.pruned += 1;
                
                if progress.pruned % 100 == 0 {
                    self.limiter.enforce_limit().await;
                }
            }
        }
        
        Ok(progress)
    }
}
```

## Rate Limiting and Performance

### Controlling Pruning Speed

```rust
/// Controls the rate of pruning to avoid impacting node performance
#[derive(Debug, Clone)]
pub struct PruneLimiter {
    /// Maximum entries to prune per second
    max_entries_per_second: usize,
    /// Batch size for each operation
    batch_size: usize,
    /// Time of last operation
    last_operation: Option<Instant>,
}

impl PruneLimiter {
    pub fn new(max_entries_per_second: usize) -> Self {
        Self {
            max_entries_per_second,
            batch_size: max_entries_per_second / 10, // 100ms batches
            last_operation: None,
        }
    }
    
    /// Enforce rate limiting by sleeping if necessary
    pub async fn enforce_limit(&mut self) {
        let now = Instant::now();
        
        if let Some(last) = self.last_operation {
            let elapsed = now.duration_since(last);
            let target_duration = Duration::from_millis(100); // 100ms between batches
            
            if elapsed < target_duration {
                let sleep_time = target_duration - elapsed;
                tokio::time::sleep(sleep_time).await;
            }
        }
        
        self.last_operation = Some(Instant::now());
    }
    
    /// Check if we should pause for rate limiting
    pub fn should_pause(&self, processed: usize) -> bool {
        processed % self.batch_size == 0
    }
}
```

## Static File Integration

### Moving Data to Static Files

```rust
/// Manages the lifecycle of static files and pruning coordination
pub struct StaticFileProducer<PF> {
    /// Provider factory for database access
    provider_factory: PF,
    /// Static file segments
    segments: Vec<StaticFileSegment>,
    /// Pruning coordination
    prune_modes: PruneModes,
}

impl<PF> StaticFileProducer<PF> 
where 
    PF: DatabaseProviderFactory,
{
    /// Copy data from database to static files
    pub fn copy_to_static_files(&mut self) -> Result<StaticFileCopyResult, StaticFileError> {
        let mut result = StaticFileCopyResult::default();
        
        // Copy headers to static files
        let header_progress = self.copy_headers_to_static_file()?;
        result.headers_copied = header_progress.copied;
        
        // Copy transactions to static files  
        let tx_progress = self.copy_transactions_to_static_file()?;
        result.transactions_copied = tx_progress.copied;
        
        // Copy receipts to static files
        let receipt_progress = self.copy_receipts_to_static_file()?;
        result.receipts_copied = receipt_progress.copied;
        
        Ok(result)
    }
    
    /// Copy block headers to static files
    fn copy_headers_to_static_file(&mut self) -> Result<CopyProgress, StaticFileError> {
        let provider = self.provider_factory.database_provider()?;
        let mut progress = CopyProgress::default();
        
        // Find the range of headers to copy
        let latest_in_static = self.get_highest_static_file_block(StaticFileSegment::Headers)?;
        let latest_in_db = provider.last_block_number()?;
        
        if latest_in_static >= latest_in_db {
            return Ok(progress); // Nothing to copy
        }
        
        // Open static file writer
        let mut static_writer = self.create_static_file_writer(StaticFileSegment::Headers)?;
        
        // Copy headers in batches
        let start_block = latest_in_static + 1;
        let end_block = latest_in_db;
        
        for block_num in start_block..=end_block {
            if let Some(header) = provider.header_by_number(block_num)? {
                static_writer.append_header(&header)?;
                progress.copied += 1;
                
                // Batch commit every 1000 headers
                if progress.copied % 1000 == 0 {
                    static_writer.commit()?;
                }
            }
        }
        
        // Final commit
        static_writer.commit()?;
        
        Ok(progress)
    }
    
    /// Coordinate with pruner to ensure data integrity
    pub fn coordinate_with_pruner(&self, pruner: &Pruner) -> Result<(), PruneError> {
        // Ensure static files are up to date before pruning
        let static_file_tip = self.get_static_file_tip()?;
        let pruner_target = pruner.get_prune_target()?;
        
        if static_file_tip < pruner_target {
            return Err(PruneError::StaticFilesBehind {
                static_file_tip,
                prune_target: pruner_target,
            });
        }
        
        Ok(())
    }
}
```

## Pruning Strategies

### 1. Conservative Pruning

```rust
/// Conservative pruning keeps more data for safety
impl PruneModes {
    pub fn conservative() -> Self {
        Self {
            account_history: Some(PruneMode::Distance(10_000)), // Keep 10k blocks
            storage_history: Some(PruneMode::Distance(10_000)),
            receipts: Some(PruneMode::Distance(1_000)),         // Keep 1k blocks
            transaction_lookup: Some(PruneMode::Distance(1_000)),
            sender_recovery: Some(PruneMode::Distance(1_000)),
        }
    }
}
```

### 2. Aggressive Pruning

```rust
/// Aggressive pruning minimizes disk usage
impl PruneModes {
    pub fn aggressive() -> Self {
        Self {
            account_history: Some(PruneMode::Distance(128)),    // Keep 128 blocks
            storage_history: Some(PruneMode::Distance(128)),
            receipts: Some(PruneMode::Distance(128)),
            transaction_lookup: Some(PruneMode::Distance(128)),
            sender_recovery: Some(PruneMode::Distance(128)),
        }
    }
}
```

### 3. Archive Mode

```rust
/// Archive mode keeps everything (no pruning)
impl PruneModes {
    pub fn archive() -> Self {
        Self {
            account_history: None,
            storage_history: None,
            receipts: None,
            transaction_lookup: None,
            sender_recovery: None,
        }
    }
}
```

## Advanced Pruning Features

### Incremental Pruning

```rust
/// Incremental pruner that spreads work over time
pub struct IncrementalPruner<DB, PF> {
    /// Base pruner
    pruner: Pruner<DB, PF>,
    /// Progress tracking
    progress: IncrementalProgress,
    /// Configuration
    config: IncrementalConfig,
}

#[derive(Debug, Clone)]
pub struct IncrementalConfig {
    /// Maximum time to spend pruning per cycle
    max_cycle_duration: Duration,
    /// Maximum entries to process per cycle
    max_entries_per_cycle: usize,
    /// Interval between pruning cycles
    cycle_interval: Duration,
}

impl<DB, PF> IncrementalPruner<DB, PF> {
    /// Run incremental pruning cycle
    pub async fn run_cycle(&mut self) -> Result<bool, PruneError> {
        let start_time = Instant::now();
        let mut processed = 0;
        
        while start_time.elapsed() < self.config.max_cycle_duration && 
              processed < self.config.max_entries_per_cycle {
            
            // Resume from where we left off
            let segment_progress = self.prune_next_segment().await?;
            processed += segment_progress.processed;
            
            if segment_progress.is_complete() {
                // Move to next segment
                self.progress.advance_segment();
                
                if self.progress.is_cycle_complete() {
                    // Full cycle complete
                    self.progress.reset();
                    return Ok(true);
                }
            }
        }
        
        // Partial cycle complete
        Ok(false)
    }
    
    /// Prune the next segment incrementally
    async fn prune_next_segment(&mut self) -> Result<PruneSegmentProgress, PruneError> {
        match self.progress.current_segment() {
            PruneSegment::AccountHistory => {
                self.prune_account_history_incremental().await
            }
            PruneSegment::StorageHistory => {
                self.prune_storage_history_incremental().await
            }
            PruneSegment::Receipts => {
                self.prune_receipts_incremental().await
            }
        }
    }
}
```

## Monitoring and Metrics

### Pruning Metrics

```rust
/// Metrics for monitoring pruning performance
#[derive(Debug, Default)]
pub struct PrunerMetrics {
    /// Total entries pruned
    total_pruned: AtomicU64,
    /// Time spent pruning
    total_duration: AtomicU64,
    /// Number of pruning runs
    prune_runs: AtomicU64,
    /// Errors encountered
    errors: AtomicU64,
}

impl PrunerMetrics {
    /// Record a successful pruning run
    pub fn record_prune_duration(&self, duration: Duration) {
        self.total_duration.fetch_add(duration.as_millis() as u64, Ordering::Relaxed);
        self.prune_runs.fetch_add(1, Ordering::Relaxed);
    }
    
    /// Record pruned entries
    pub fn record_pruned_entries(&self, count: usize) {
        self.total_pruned.fetch_add(count as u64, Ordering::Relaxed);
    }
    
    /// Record an error
    pub fn record_error(&self) {
        self.errors.fetch_add(1, Ordering::Relaxed);
    }
    
    /// Get current statistics
    pub fn stats(&self) -> PruneStatistics {
        let total_pruned = self.total_pruned.load(Ordering::Relaxed);
        let total_duration = self.total_duration.load(Ordering::Relaxed);
        let prune_runs = self.prune_runs.load(Ordering::Relaxed);
        let errors = self.errors.load(Ordering::Relaxed);
        
        let avg_duration = if prune_runs > 0 {
            total_duration / prune_runs
        } else {
            0
        };
        
        let avg_entries_per_run = if prune_runs > 0 {
            total_pruned / prune_runs
        } else {
            0
        };
        
        PruneStatistics {
            total_pruned,
            total_duration: Duration::from_millis(total_duration),
            prune_runs,
            errors,
            avg_duration: Duration::from_millis(avg_duration),
            avg_entries_per_run,
        }
    }
}
```

## Assignments

### Assignment 1: Custom Pruning Strategy

Implement a size-based pruning strategy:

```rust
use std::path::Path;
use std::fs;

/// Pruning mode based on disk space usage
#[derive(Debug, Clone)]
pub enum SizeBasedPruneMode {
    /// Keep data under this many GB
    MaxSize(u64),
    /// Keep at least this percentage of free space
    FreeSpacePercent(f32),
    /// Use both size and free space constraints
    Hybrid { max_size_gb: u64, min_free_percent: f32 },
}

pub struct SizeBasedPruner {
    /// Data directory path
    data_dir: PathBuf,
    /// Pruning mode
    mode: SizeBasedPruneMode,
    /// Disk usage monitor
    disk_monitor: DiskUsageMonitor,
}

#[derive(Debug)]
pub struct DiskUsageStats {
    pub total_space: u64,
    pub used_space: u64,
    pub free_space: u64,
    pub database_size: u64,
    pub static_files_size: u64,
}

impl SizeBasedPruner {
    pub fn new(data_dir: PathBuf, mode: SizeBasedPruneMode) -> Self {
        Self {
            data_dir: data_dir.clone(),
            mode,
            disk_monitor: DiskUsageMonitor::new(data_dir),
        }
    }
    
    /// Check if pruning is needed based on disk usage
    pub fn should_prune(&self) -> Result<bool, PruneError> {
        let stats = self.disk_monitor.get_stats()?;
        
        match &self.mode {
            SizeBasedPruneMode::MaxSize(max_gb) => {
                let max_bytes = max_gb * 1024 * 1024 * 1024;
                Ok(stats.database_size > max_bytes)
            }
            SizeBasedPruneMode::FreeSpacePercent(min_percent) => {
                let free_percent = (stats.free_space as f32 / stats.total_space as f32) * 100.0;
                Ok(free_percent < *min_percent)
            }
            SizeBasedPruneMode::Hybrid { max_size_gb, min_free_percent } => {
                let max_bytes = max_size_gb * 1024 * 1024 * 1024;
                let free_percent = (stats.free_space as f32 / stats.total_space as f32) * 100.0;
                
                Ok(stats.database_size > max_bytes || free_percent < *min_free_percent)
            }
        }
    }
    
    /// Calculate how much data needs to be pruned
    pub fn calculate_prune_target(&self) -> Result<PruneTarget, PruneError> {
        let stats = self.disk_monitor.get_stats()?;
        
        let target_bytes = match &self.mode {
            SizeBasedPruneMode::MaxSize(max_gb) => {
                let max_bytes = max_gb * 1024 * 1024 * 1024;
                if stats.database_size > max_bytes {
                    // Prune 10% extra to avoid immediate re-pruning
                    let target = max_bytes * 90 / 100;
                    stats.database_size - target
                } else {
                    0
                }
            }
            SizeBasedPruneMode::FreeSpacePercent(min_percent) => {
                let required_free = (stats.total_space as f32 * min_percent / 100.0) as u64;
                if stats.free_space < required_free {
                    required_free - stats.free_space
                } else {
                    0
                }
            }
            SizeBasedPruneMode::Hybrid { max_size_gb, min_free_percent } => {
                let max_bytes = max_size_gb * 1024 * 1024 * 1024;
                let required_free = (stats.total_space as f32 * min_free_percent / 100.0) as u64;
                
                let size_excess = if stats.database_size > max_bytes {
                    stats.database_size - (max_bytes * 90 / 100)
                } else {
                    0
                };
                
                let free_deficit = if stats.free_space < required_free {
                    required_free - stats.free_space
                } else {
                    0
                };
                
                std::cmp::max(size_excess, free_deficit)
            }
        };
        
        // Convert bytes to approximate blocks to prune
        let avg_block_size = 50_000; // Approximate bytes per block of data
        let blocks_to_prune = target_bytes / avg_block_size;
        
        Ok(PruneTarget {
            bytes_to_prune: target_bytes,
            blocks_to_prune,
            estimated_time: self.estimate_prune_time(blocks_to_prune),
        })
    }
    
    /// Estimate how long pruning will take
    fn estimate_prune_time(&self, blocks_to_prune: u64) -> Duration {
        // Estimate based on historical performance
        // Assume 1000 blocks per second pruning rate
        let seconds = blocks_to_prune / 1000;
        Duration::from_secs(seconds)
    }
}

pub struct DiskUsageMonitor {
    data_dir: PathBuf,
}

impl DiskUsageMonitor {
    pub fn new(data_dir: PathBuf) -> Self {
        Self { data_dir }
    }
    
    /// Get current disk usage statistics
    pub fn get_stats(&self) -> Result<DiskUsageStats, PruneError> {
        let total_space = self.get_total_disk_space()?;
        let free_space = self.get_free_disk_space()?;
        let used_space = total_space - free_space;
        
        let database_size = self.get_directory_size(&self.data_dir.join("db"))?;
        let static_files_size = self.get_directory_size(&self.data_dir.join("static_files"))?;
        
        Ok(DiskUsageStats {
            total_space,
            used_space,
            free_space,
            database_size,
            static_files_size,
        })
    }
    
    /// Get total disk space for the data directory
    fn get_total_disk_space(&self) -> Result<u64, PruneError> {
        #[cfg(unix)]
        {
            use std::ffi::CString;
            use std::mem;
            
            let path = CString::new(self.data_dir.to_string_lossy().as_bytes())
                .map_err(|_| PruneError::Other("Invalid path".to_string()))?;
                
            let mut statvfs: libc::statvfs = unsafe { mem::zeroed() };
            let result = unsafe { libc::statvfs(path.as_ptr(), &mut statvfs) };
            
            if result == 0 {
                Ok(statvfs.f_blocks * statvfs.f_frsize)
            } else {
                Err(PruneError::Other("Failed to get disk stats".to_string()))
            }
        }
        
        #[cfg(windows)]
        {
            // Windows implementation using GetDiskFreeSpaceEx
            use std::ptr;
            use winapi::um::fileapi::GetDiskFreeSpaceExW;
            use winapi::um::winnt::ULARGE_INTEGER;
            
            let path: Vec<u16> = self.data_dir.to_string_lossy().encode_utf16().chain(Some(0)).collect();
            
            let mut total_bytes = ULARGE_INTEGER::default();
            
            let result = unsafe {
                GetDiskFreeSpaceExW(
                    path.as_ptr(),
                    ptr::null_mut(),
                    &mut total_bytes,
                    ptr::null_mut(),
                )
            };
            
            if result != 0 {
                Ok(unsafe { *total_bytes.QuadPart() as u64 })
            } else {
                Err(PruneError::Other("Failed to get disk stats".to_string()))
            }
        }
    }
    
    /// Get free disk space
    fn get_free_disk_space(&self) -> Result<u64, PruneError> {
        // Similar implementation to get_total_disk_space but for free space
        // Implementation details omitted for brevity
        Ok(0) // Placeholder
    }
    
    /// Calculate directory size recursively
    fn get_directory_size(&self, dir: &Path) -> Result<u64, PruneError> {
        if !dir.exists() {
            return Ok(0);
        }
        
        let mut total_size = 0;
        
        for entry in fs::read_dir(dir)
            .map_err(|e| PruneError::Other(format!("Failed to read directory: {}", e)))? 
        {
            let entry = entry
                .map_err(|e| PruneError::Other(format!("Failed to read entry: {}", e)))?;
            let path = entry.path();
            
            if path.is_file() {
                total_size += entry.metadata()
                    .map_err(|e| PruneError::Other(format!("Failed to get metadata: {}", e)))?
                    .len();
            } else if path.is_dir() {
                total_size += self.get_directory_size(&path)?;
            }
        }
        
        Ok(total_size)
    }
}

#[derive(Debug)]
pub struct PruneTarget {
    pub bytes_to_prune: u64,
    pub blocks_to_prune: u64,
    pub estimated_time: Duration,
}

// Usage example
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let data_dir = PathBuf::from("/path/to/reth/data");
    
    // Configure size-based pruning
    let pruner = SizeBasedPruner::new(
        data_dir,
        SizeBasedPruneMode::Hybrid {
            max_size_gb: 500,     // Keep database under 500GB
            min_free_percent: 20.0, // Keep 20% free space
        }
    );
    
    // Check if pruning is needed
    if pruner.should_prune()? {
        let target = pruner.calculate_prune_target()?;
        
        println!("Pruning needed:");
        println!("  Bytes to prune: {} GB", target.bytes_to_prune / (1024 * 1024 * 1024));
        println!("  Blocks to prune: {}", target.blocks_to_prune);
        println!("  Estimated time: {:?}", target.estimated_time);
        
        // Execute pruning...
    } else {
        println!("No pruning needed");
    }
    
    Ok(())
}
```

### Assignment 2: Pruning Scheduler

Implement a background pruning scheduler:

```rust
use tokio::time::{interval, Duration, Instant};
use tokio::sync::{mpsc, RwLock};
use std::sync::Arc;

/// Background scheduler for automatic pruning
pub struct PruningScheduler {
    /// Pruner instance
    pruner: Arc<RwLock<Pruner>>,
    /// Scheduling configuration
    config: SchedulerConfig,
    /// Control channel
    control_rx: mpsc::Receiver<SchedulerCommand>,
    control_tx: mpsc::Sender<SchedulerCommand>,
    /// Current state
    state: Arc<RwLock<SchedulerState>>,
    /// Metrics
    metrics: SchedulerMetrics,
}

#[derive(Debug, Clone)]
pub struct SchedulerConfig {
    /// Interval between pruning checks
    pub check_interval: Duration,
    /// Minimum time between pruning runs
    pub min_prune_interval: Duration,
    /// Time of day to prefer for pruning (UTC hour)
    pub preferred_hour: Option<u8>,
    /// Maximum consecutive pruning runs
    pub max_consecutive_runs: usize,
    /// Node load threshold (0.0 to 1.0)
    pub max_load_threshold: f32,
}

#[derive(Debug, Clone)]
pub enum SchedulerCommand {
    /// Start the scheduler
    Start,
    /// Stop the scheduler
    Stop,
    /// Force immediate pruning
    ForceRun,
    /// Update configuration
    UpdateConfig(SchedulerConfig),
    /// Get current status
    Status(tokio::sync::oneshot::Sender<SchedulerStatus>),
}

#[derive(Debug, Clone)]
pub struct SchedulerState {
    pub is_running: bool,
    pub last_prune_time: Option<Instant>,
    pub consecutive_runs: usize,
    pub next_scheduled_run: Option<Instant>,
}

#[derive(Debug)]
pub struct SchedulerStatus {
    pub state: SchedulerState,
    pub metrics: SchedulerMetrics,
    pub current_load: f32,
}

impl PruningScheduler {
    pub fn new(pruner: Pruner, config: SchedulerConfig) -> Self {
        let (control_tx, control_rx) = mpsc::channel(100);
        
        Self {
            pruner: Arc::new(RwLock::new(pruner)),
            config,
            control_rx,
            control_tx,
            state: Arc::new(RwLock::new(SchedulerState {
                is_running: false,
                last_prune_time: None,
                consecutive_runs: 0,
                next_scheduled_run: None,
            })),
            metrics: SchedulerMetrics::default(),
        }
    }
    
    /// Get a handle to control the scheduler
    pub fn handle(&self) -> SchedulerHandle {
        SchedulerHandle {
            control_tx: self.control_tx.clone(),
        }
    }
    
    /// Run the scheduler (this should be spawned as a background task)
    pub async fn run(&mut self) -> Result<(), SchedulerError> {
        let mut check_timer = interval(self.config.check_interval);
        
        loop {
            tokio::select! {
                // Handle control commands
                Some(command) = self.control_rx.recv() => {
                    self.handle_command(command).await?;
                }
                
                // Regular pruning checks
                _ = check_timer.tick() => {
                    if self.state.read().await.is_running {
                        if let Err(e) = self.check_and_prune().await {
                            eprintln!("Pruning check failed: {}", e);
                            self.metrics.record_error();
                        }
                    }
                }
            }
        }
    }
    
    /// Handle scheduler commands
    async fn handle_command(&mut self, command: SchedulerCommand) -> Result<(), SchedulerError> {
        match command {
            SchedulerCommand::Start => {
                self.state.write().await.is_running = true;
                println!("Pruning scheduler started");
            }
            
            SchedulerCommand::Stop => {
                self.state.write().await.is_running = false;
                println!("Pruning scheduler stopped");
            }
            
            SchedulerCommand::ForceRun => {
                self.force_prune().await?;
            }
            
            SchedulerCommand::UpdateConfig(new_config) => {
                self.config = new_config;
                println!("Scheduler configuration updated");
            }
            
            SchedulerCommand::Status(response_tx) => {
                let status = self.get_status().await;
                let _ = response_tx.send(status);
            }
        }
        
        Ok(())
    }
    
    /// Check if pruning should run and execute if so
    async fn check_and_prune(&mut self) -> Result<(), SchedulerError> {
        let mut state = self.state.write().await;
        
        // Check if enough time has passed since last prune
        if let Some(last_prune) = state.last_prune_time {
            if last_prune.elapsed() < self.config.min_prune_interval {
                return Ok(());
            }
        }
        
        // Check consecutive run limit
        if state.consecutive_runs >= self.config.max_consecutive_runs {
            // Reset counter after a break
            if state.last_prune_time.map(|t| t.elapsed() > Duration::from_hours(1)).unwrap_or(true) {
                state.consecutive_runs = 0;
            } else {
                return Ok(());
            }
        }
        
        // Check node load
        let current_load = self.get_node_load().await?;
        if current_load > self.config.max_load_threshold {
            return Ok(());
        }
        
        // Check preferred time
        if let Some(preferred_hour) = self.config.preferred_hour {
            let current_hour = chrono::Utc::now().hour() as u8;
            if current_hour != preferred_hour && state.consecutive_runs == 0 {
                // Only respect preferred hour for the first run in a sequence
                return Ok(());
            }
        }
        
        drop(state); // Release lock before pruning
        
        // Execute pruning
        let start_time = Instant::now();
        let mut pruner = self.pruner.write().await;
        
        match pruner.run(1000).await { // Use current tip as parameter
            Ok(progress) => {
                let duration = start_time.elapsed();
                
                // Update state
                let mut state = self.state.write().await;
                state.last_prune_time = Some(start_time);
                state.consecutive_runs += 1;
                
                // Record metrics
                self.metrics.record_successful_run(duration, progress.total_pruned());
                
                println!("Pruning completed: {} entries in {:?}", 
                    progress.total_pruned(), duration);
            }
            
            Err(e) => {
                self.metrics.record_error();
                return Err(SchedulerError::PruningFailed(e));
            }
        }
        
        Ok(())
    }
    
    /// Force immediate pruning regardless of scheduling
    async fn force_prune(&mut self) -> Result<(), SchedulerError> {
        println!("Force pruning requested");
        
        let start_time = Instant::now();
        let mut pruner = self.pruner.write().await;
        
        match pruner.run(1000).await {
            Ok(progress) => {
                let duration = start_time.elapsed();
                
                // Update state
                let mut state = self.state.write().await;
                state.last_prune_time = Some(start_time);
                state.consecutive_runs = 0; // Reset counter for forced runs
                
                self.metrics.record_successful_run(duration, progress.total_pruned());
                
                println!("Force pruning completed: {} entries in {:?}", 
                    progress.total_pruned(), duration);
                
                Ok(())
            }
            
            Err(e) => {
                self.metrics.record_error();
                Err(SchedulerError::PruningFailed(e))
            }
        }
    }
    
    /// Get current node load (simplified)
    async fn get_node_load(&self) -> Result<f32, SchedulerError> {
        // In a real implementation, this would check:
        // - CPU usage
        // - Memory usage
        // - Disk I/O
        // - Network activity
        // - Number of active RPC connections
        
        // For now, return a mock value
        Ok(0.3) // 30% load
    }
    
    /// Get current status
    async fn get_status(&self) -> SchedulerStatus {
        let state = self.state.read().await.clone();
        let current_load = self.get_node_load().await.unwrap_or(1.0);
        
        SchedulerStatus {
            state,
            metrics: self.metrics.clone(),
            current_load,
        }
    }
}

/// Handle for controlling the scheduler
#[derive(Clone)]
pub struct SchedulerHandle {
    control_tx: mpsc::Sender<SchedulerCommand>,
}

impl SchedulerHandle {
    /// Start the scheduler
    pub async fn start(&self) -> Result<(), SchedulerError> {
        self.control_tx.send(SchedulerCommand::Start).await
            .map_err(|_| SchedulerError::ChannelClosed)
    }
    
    /// Stop the scheduler
    pub async fn stop(&self) -> Result<(), SchedulerError> {
        self.control_tx.send(SchedulerCommand::Stop).await
            .map_err(|_| SchedulerError::ChannelClosed)
    }
    
    /// Force immediate pruning
    pub async fn force_run(&self) -> Result<(), SchedulerError> {
        self.control_tx.send(SchedulerCommand::ForceRun).await
            .map_err(|_| SchedulerError::ChannelClosed)
    }
    
    /// Get current status
    pub async fn status(&self) -> Result<SchedulerStatus, SchedulerError> {
        let (tx, rx) = tokio::sync::oneshot::channel();
        
        self.control_tx.send(SchedulerCommand::Status(tx)).await
            .map_err(|_| SchedulerError::ChannelClosed)?;
            
        rx.await.map_err(|_| SchedulerError::ChannelClosed)
    }
}

#[derive(Debug, Clone, Default)]
pub struct SchedulerMetrics {
    pub total_runs: u64,
    pub successful_runs: u64,
    pub total_entries_pruned: u64,
    pub total_time_spent: Duration,
    pub errors: u64,
}

impl SchedulerMetrics {
    fn record_successful_run(&mut self, duration: Duration, entries_pruned: usize) {
        self.total_runs += 1;
        self.successful_runs += 1;
        self.total_entries_pruned += entries_pruned as u64;
        self.total_time_spent += duration;
    }
    
    fn record_error(&mut self) {
        self.total_runs += 1;
        self.errors += 1;
    }
    
    pub fn average_duration(&self) -> Duration {
        if self.successful_runs > 0 {
            self.total_time_spent / self.successful_runs as u32
        } else {
            Duration::ZERO
        }
    }
    
    pub fn success_rate(&self) -> f32 {
        if self.total_runs > 0 {
            self.successful_runs as f32 / self.total_runs as f32
        } else {
            0.0
        }
    }
}

#[derive(Debug, thiserror::Error)]
pub enum SchedulerError {
    #[error("Pruning failed: {0}")]
    PruningFailed(PruneError),
    #[error("Channel closed")]
    ChannelClosed,
    #[error("System error: {0}")]
    System(String),
}

// Usage example
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let pruner = Pruner::new(/* config */);
    
    let config = SchedulerConfig {
        check_interval: Duration::from_mins(15),      // Check every 15 minutes
        min_prune_interval: Duration::from_hours(2),  // Prune at most every 2 hours
        preferred_hour: Some(3),                      // Prefer 3 AM UTC
        max_consecutive_runs: 3,                      // Max 3 runs in sequence
        max_load_threshold: 0.7,                      // Don't prune if load > 70%
    };
    
    let mut scheduler = PruningScheduler::new(pruner, config);
    let handle = scheduler.handle();
    
    // Start scheduler in background
    tokio::spawn(async move {
        if let Err(e) = scheduler.run().await {
            eprintln!("Scheduler error: {}", e);
        }
    });
    
    // Start scheduling
    handle.start().await?;
    
    // Check status periodically
    tokio::spawn(async move {
        let mut interval = tokio::time::interval(Duration::from_secs(300)); // 5 minutes
        
        loop {
            interval.tick().await;
            
            match handle.status().await {
                Ok(status) => {
                    println!("Scheduler status:");
                    println!("  Running: {}", status.state.is_running);
                    println!("  Consecutive runs: {}", status.state.consecutive_runs);
                    println!("  Current load: {:.1}%", status.current_load * 100.0);
                    println!("  Success rate: {:.1}%", status.metrics.success_rate() * 100.0);
                    println!("  Total pruned: {}", status.metrics.total_entries_pruned);
                }
                Err(e) => {
                    eprintln!("Failed to get status: {}", e);
                }
            }
        }
    });
    
    // Keep main thread alive
    tokio::signal::ctrl_c().await?;
    println!("Shutting down...");
    
    handle.stop().await?;
    
    Ok(())
}
```

### Assignment 3: Smart Pruning with Usage Analytics

Implement pruning that adapts based on data access patterns:

```rust
use std::collections::{HashMap, BTreeMap};
use std::sync::Arc;
use tokio::sync::RwLock;
use serde::{Serialize, Deserialize};

/// Analytics-driven pruning that preserves frequently accessed data
pub struct SmartPruner {
    /// Base pruner
    base_pruner: Pruner,
    /// Access pattern analyzer
    analyzer: AccessPatternAnalyzer,
    /// Smart pruning configuration
    config: SmartPruningConfig,
}

#[derive(Debug, Clone)]
pub struct SmartPruningConfig {
    /// How long to track access patterns
    pub analysis_window: Duration,
    /// Minimum access frequency to preserve data
    pub min_access_frequency: f32,
    /// Weight for recent vs historical access
    pub recency_weight: f32,
    /// Base pruning distance for unaccessed data
    pub base_prune_distance: u64,
    /// Extra distance for frequently accessed data
    pub frequency_bonus_distance: u64,
}

/// Tracks access patterns for different types of data
pub struct AccessPatternAnalyzer {
    /// Block access patterns
    block_access: Arc<RwLock<AccessTracker<BlockNumber>>>,
    /// Account access patterns
    account_access: Arc<RwLock<AccessTracker<Address>>>,
    /// Storage slot access patterns
    storage_access: Arc<RwLock<AccessTracker<(Address, B256)>>>,
    /// Receipt access patterns
    receipt_access: Arc<RwLock<AccessTracker<BlockNumber>>>,
    /// Configuration
    config: SmartPruningConfig,
}

/// Generic access tracker for any key type
#[derive(Debug)]
pub struct AccessTracker<K> {
    /// Access history: key -> list of access times
    access_history: HashMap<K, Vec<Instant>>,
    /// Cached access scores
    access_scores: HashMap<K, AccessScore>,
    /// Last cleanup time
    last_cleanup: Instant,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AccessScore {
    /// Total number of accesses
    pub total_accesses: u64,
    /// Recent access frequency (per hour)
    pub recent_frequency: f32,
    /// Time since last access
    pub last_access_age: Duration,
    /// Weighted score (higher = more important)
    pub weighted_score: f32,
}

impl<K: Clone + Eq + std::hash::Hash> AccessTracker<K> {
    pub fn new() -> Self {
        Self {
            access_history: HashMap::new(),
            access_scores: HashMap::new(),
            last_cleanup: Instant::now(),
        }
    }
    
    /// Record an access to a key
    pub fn record_access(&mut self, key: K) {
        let now = Instant::now();
        
        self.access_history
            .entry(key.clone())
            .or_insert_with(Vec::new)
            .push(now);
        
        // Invalidate cached score
        self.access_scores.remove(&key);
        
        // Periodic cleanup of old access data
        if now.duration_since(self.last_cleanup) > Duration::from_hours(1) {
            self.cleanup_old_accesses(Duration::from_days(7));
            self.last_cleanup = now;
        }
    }
    
    /// Calculate access score for a key
    pub fn get_access_score(&mut self, key: &K, config: &SmartPruningConfig) -> AccessScore {
        if let Some(cached) = self.access_scores.get(key) {
            return cached.clone();
        }
        
        let score = self.calculate_access_score(key, config);
        self.access_scores.insert(key.clone(), score.clone());
        score
    }
    
    /// Calculate access score based on access history
    fn calculate_access_score(&self, key: &K, config: &SmartPruningConfig) -> AccessScore {
        let now = Instant::now();
        let analysis_window = config.analysis_window;
        
        let accesses = self.access_history.get(key).cloned().unwrap_or_default();
        
        if accesses.is_empty() {
            return AccessScore {
                total_accesses: 0,
                recent_frequency: 0.0,
                last_access_age: Duration::from_secs(u64::MAX),
                weighted_score: 0.0,
            };
        }
        
        // Filter recent accesses
        let recent_accesses: Vec<_> = accesses.iter()
            .filter(|&&access_time| now.duration_since(access_time) <= analysis_window)
            .collect();
        
        let total_accesses = accesses.len() as u64;
        let recent_frequency = recent_accesses.len() as f32 / analysis_window.as_secs_f32() * 3600.0; // per hour
        let last_access_age = now.duration_since(*accesses.last().unwrap_or(&Instant::now()));
        
        // Calculate weighted score
        let recency_factor = (-last_access_age.as_secs_f32() / analysis_window.as_secs_f32()).exp();
        let frequency_factor = recent_frequency / 10.0; // Normalize to reasonable range
        
        let weighted_score = config.recency_weight * recency_factor + 
                           (1.0 - config.recency_weight) * frequency_factor;
        
        AccessScore {
            total_accesses,
            recent_frequency,
            last_access_age,
            weighted_score,
        }
    }
    
    /// Remove old access records to prevent memory bloat
    fn cleanup_old_accesses(&mut self, max_age: Duration) {
        let cutoff = Instant::now() - max_age;
        
        self.access_history.retain(|_, accesses| {
            accesses.retain(|&access_time| access_time > cutoff);
            !accesses.is_empty()
        });
        
        // Clear cached scores for removed keys
        self.access_scores.retain(|key, _| self.access_history.contains_key(key));
    }
    
    /// Get top accessed keys
    pub fn get_top_accessed(&mut self, limit: usize, config: &SmartPruningConfig) -> Vec<(K, AccessScore)> {
        let mut scored_keys: Vec<_> = self.access_history.keys()
            .map(|key| {
                let score = self.get_access_score(key, config);
                (key.clone(), score)
            })
            .collect();
        
        scored_keys.sort_by(|a, b| b.1.weighted_score.partial_cmp(&a.1.weighted_score).unwrap());
        scored_keys.truncate(limit);
        
        scored_keys
    }
}

impl AccessPatternAnalyzer {
    pub fn new(config: SmartPruningConfig) -> Self {
        Self {
            block_access: Arc::new(RwLock::new(AccessTracker::new())),
            account_access: Arc::new(RwLock::new(AccessTracker::new())),
            storage_access: Arc::new(RwLock::new(AccessTracker::new())),
            receipt_access: Arc::new(RwLock::new(AccessTracker::new())),
            config,
        }
    }
    
    /// Record block access
    pub async fn record_block_access(&self, block_number: BlockNumber) {
        self.block_access.write().await.record_access(block_number);
    }
    
    /// Record account access
    pub async fn record_account_access(&self, address: Address) {
        self.account_access.write().await.record_access(address);
    }
    
    /// Record storage access
    pub async fn record_storage_access(&self, address: Address, slot: B256) {
        self.storage_access.write().await.record_access((address, slot));
    }
    
    /// Record receipt access
    pub async fn record_receipt_access(&self, block_number: BlockNumber) {
        self.receipt_access.write().await.record_access(block_number);
    }
    
    /// Generate smart pruning recommendations
    pub async fn generate_pruning_recommendations(
        &self,
        current_tip: BlockNumber,
    ) -> SmartPruningRecommendations {
        let mut recommendations = SmartPruningRecommendations::default();
        
        // Analyze block access patterns
        let frequently_accessed_blocks = self.block_access.write().await
            .get_top_accessed(100, &self.config);
        
        // Calculate smart prune targets for different data types
        recommendations.account_history_target = self.calculate_smart_prune_target(
            current_tip,
            &frequently_accessed_blocks.iter()
                .map(|(block, score)| (*block, score.clone()))
                .collect(),
            DataType::AccountHistory,
        ).await;
        
        recommendations.storage_history_target = self.calculate_smart_prune_target(
            current_tip,
            &frequently_accessed_blocks.iter()
                .map(|(block, score)| (*block, score.clone()))
                .collect(),
            DataType::StorageHistory,
        ).await;
        
        recommendations.receipts_target = self.calculate_smart_prune_target(
            current_tip,
            &frequently_accessed_blocks.iter()
                .map(|(block, score)| (*block, score.clone()))
                .collect(),
            DataType::Receipts,
        ).await;
        
        recommendations
    }
    
    /// Calculate smart prune target for a specific data type
    async fn calculate_smart_prune_target(
        &self,
        current_tip: BlockNumber,
        access_patterns: &[(BlockNumber, AccessScore)],
        data_type: DataType,
    ) -> BlockNumber {
        let base_target = current_tip.saturating_sub(self.config.base_prune_distance);
        
        // Find the oldest frequently accessed block
        let oldest_important_block = access_patterns.iter()
            .filter(|(_, score)| score.weighted_score > self.config.min_access_frequency)
            .map(|(block, _)| *block)
            .min()
            .unwrap_or(base_target);
        
        // Adjust target based on data type importance
        let type_multiplier = match data_type {
            DataType::AccountHistory => 1.5, // Keep account history longer
            DataType::StorageHistory => 1.2,
            DataType::Receipts => 1.0,
        };
        
        let adjusted_distance = (self.config.base_prune_distance as f32 * type_multiplier) as u64;
        let conservative_target = current_tip.saturating_sub(adjusted_distance);
        
        // Use the more conservative (older) target
        std::cmp::min(base_target, std::cmp::min(oldest_important_block, conservative_target))
    }
}

#[derive(Debug, Clone)]
pub enum DataType {
    AccountHistory,
    StorageHistory,
    Receipts,
}

#[derive(Debug, Default)]
pub struct SmartPruningRecommendations {
    pub account_history_target: BlockNumber,
    pub storage_history_target: BlockNumber,
    pub receipts_target: BlockNumber,
    pub protected_blocks: Vec<BlockNumber>,
}

impl SmartPruner {
    pub fn new(base_pruner: Pruner, config: SmartPruningConfig) -> Self {
        let analyzer = AccessPatternAnalyzer::new(config.clone());
        
        Self {
            base_pruner,
            analyzer,
            config,
        }
    }
    
    /// Run smart pruning
    pub async fn run_smart_prune(&mut self, current_tip: BlockNumber) -> Result<SmartPruneResult, PruneError> {
        let start_time = Instant::now();
        
        // Generate recommendations based on access patterns
        let recommendations = self.analyzer.generate_pruning_recommendations(current_tip).await;
        
        // Execute pruning with smart targets
        let mut result = SmartPruneResult {
            recommendations: recommendations.clone(),
            pruned_segments: HashMap::new(),
            total_time: Duration::ZERO,
            data_preserved: 0,
        };
        
        // Prune account history with smart target
        if let Ok(progress) = self.prune_account_history_smart(recommendations.account_history_target).await {
            result.pruned_segments.insert("account_history".to_string(), progress);
        }
        
        // Prune storage history with smart target
        if let Ok(progress) = self.prune_storage_history_smart(recommendations.storage_history_target).await {
            result.pruned_segments.insert("storage_history".to_string(), progress);
        }
        
        // Prune receipts with smart target
        if let Ok(progress) = self.prune_receipts_smart(recommendations.receipts_target).await {
            result.pruned_segments.insert("receipts".to_string(), progress);
        }
        
        result.total_time = start_time.elapsed();
        
        // Calculate how much data was preserved due to smart analysis
        let base_target = current_tip.saturating_sub(self.config.base_prune_distance);
        result.data_preserved = recommendations.account_history_target.saturating_sub(base_target) +
                               recommendations.storage_history_target.saturating_sub(base_target) +
                               recommendations.receipts_target.saturating_sub(base_target);
        
        Ok(result)
    }
    
    /// Smart account history pruning
    async fn prune_account_history_smart(&mut self, target: BlockNumber) -> Result<PruneSegmentProgress, PruneError> {
        // Implementation similar to base pruner but with smart targeting
        // This would preserve frequently accessed account data
        Ok(PruneSegmentProgress { pruned: 1000, processed: 5000 })
    }
    
    /// Smart storage history pruning
    async fn prune_storage_history_smart(&mut self, target: BlockNumber) -> Result<PruneSegmentProgress, PruneError> {
        // Implementation that preserves frequently accessed storage
        Ok(PruneSegmentProgress { pruned: 800, processed: 4000 })
    }
    
    /// Smart receipts pruning
    async fn prune_receipts_smart(&mut self, target: BlockNumber) -> Result<PruneSegmentProgress, PruneError> {
        // Implementation that preserves receipts for important transactions
        Ok(PruneSegmentProgress { pruned: 500, processed: 2000 })
    }
    
    /// Get access analytics summary
    pub async fn get_analytics_summary(&self) -> AnalyticsSummary {
        let block_stats = self.analyzer.block_access.read().await;
        let account_stats = self.analyzer.account_access.read().await;
        let storage_stats = self.analyzer.storage_access.read().await;
        
        AnalyticsSummary {
            tracked_blocks: block_stats.access_history.len(),
            tracked_accounts: account_stats.access_history.len(),
            tracked_storage_slots: storage_stats.access_history.len(),
            analysis_window: self.config.analysis_window,
            total_accesses: block_stats.access_history.values()
                .map(|v| v.len())
                .sum::<usize>() as u64,
        }
    }
}

#[derive(Debug)]
pub struct SmartPruneResult {
    pub recommendations: SmartPruningRecommendations,
    pub pruned_segments: HashMap<String, PruneSegmentProgress>,
    pub total_time: Duration,
    pub data_preserved: u64, // Blocks of data preserved due to smart analysis
}

#[derive(Debug)]
pub struct AnalyticsSummary {
    pub tracked_blocks: usize,
    pub tracked_accounts: usize,
    pub tracked_storage_slots: usize,
    pub analysis_window: Duration,
    pub total_accesses: u64,
}

// Example usage with RPC integration
pub struct RpcAccessTracker {
    analyzer: Arc<AccessPatternAnalyzer>,
}

impl RpcAccessTracker {
    pub fn new(analyzer: Arc<AccessPatternAnalyzer>) -> Self {
        Self { analyzer }
    }
    
    /// Track RPC calls to understand access patterns
    pub async fn track_rpc_call(&self, method: &str, params: &[serde_json::Value]) {
        match method {
            "eth_getBalance" | "eth_getCode" | "eth_getStorageAt" => {
                if let Some(address_value) = params.get(0) {
                    if let Ok(address_str) = serde_json::from_value::<String>(address_value.clone()) {
                        if let Ok(address) = address_str.parse::<Address>() {
                            self.analyzer.record_account_access(address).await;
                        }
                    }
                }
            }
            
            "eth_getBlockByNumber" | "eth_getBlockByHash" => {
                // Track block access
                if let Some(block_param) = params.get(0) {
                    if let Ok(block_str) = serde_json::from_value::<String>(block_param.clone()) {
                        if let Ok(block_num) = block_str.parse::<u64>() {
                            self.analyzer.record_block_access(block_num).await;
                        }
                    }
                }
            }
            
            "eth_getTransactionReceipt" => {
                // This would require more complex logic to determine which block's receipts
                // are being accessed, but the idea is similar
            }
            
            _ => {} // Other methods
        }
    }
}

// Usage example
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = SmartPruningConfig {
        analysis_window: Duration::from_days(7),
        min_access_frequency: 0.1, // At least 0.1 accesses per hour
        recency_weight: 0.7,       // 70% weight on recent accesses
        base_prune_distance: 1000,
        frequency_bonus_distance: 500,
    };
    
    let base_pruner = Pruner::new(/* config */);
    let mut smart_pruner = SmartPruner::new(base_pruner, config);
    
    // Simulate some access patterns
    smart_pruner.analyzer.record_block_access(900).await; // Recent block
    smart_pruner.analyzer.record_block_access(500).await; // Older but accessed block
    smart_pruner.analyzer.record_account_access(Address::from([1; 20])).await;
    
    // Run smart pruning
    let result = smart_pruner.run_smart_prune(1000).await?;
    
    println!("Smart pruning completed:");
    println!("  Account history target: {}", result.recommendations.account_history_target);
    println!("  Storage history target: {}", result.recommendations.storage_history_target);
    println!("  Receipts target: {}", result.recommendations.receipts_target);
    println!("  Data preserved: {} blocks", result.data_preserved);
    println!("  Total time: {:?}", result.total_time);
    
    // Get analytics summary
    let summary = smart_pruner.get_analytics_summary().await;
    println!("Analytics summary: {:?}", summary);
    
    Ok(())
}
```

## Questions to Ponder - Detailed Answers

### 1. How does pruning affect node synchronization and consensus?

**Answer**: Pruning affects node operations in several important ways:

**Synchronization Impact**:
- **Fast Sync**: Pruned nodes can't serve historical state to other nodes doing fast sync beyond their pruning horizon
- **Full Sync**: New nodes must rely on archive nodes for complete historical verification
- **Beam Sync**: Modern sync methods can work with pruned nodes by requesting specific state pieces

**Consensus Participation**:
- **Block Validation**: Pruned nodes can still validate new blocks as they only need recent state
- **Fork Choice**: Participation in fork choice remains unaffected since it depends on recent blocks
- **Finality**: In PoS systems, finality participation is maintained as it doesn't require deep history

**Network Health**:
- Too much pruning across the network can create data availability issues
- Archive nodes become critical infrastructure for network bootstrap
- Balance needed between storage efficiency and network resilience

### 2. What are the trade-offs between aggressive and conservative pruning strategies?

**Answer**: Different pruning strategies have distinct trade-offs:

**Aggressive Pruning (128-256 blocks)**:
- Pros: Minimal disk usage, fast initial sync, lower hardware requirements
- Cons: Can't serve historical data, potential issues during deep reorgs, limited RPC functionality

**Conservative Pruning (10,000+ blocks)**:
- Pros: Can handle deep reorgs, serves more historical data, better RPC support
- Cons: Higher storage requirements, longer initial sync times

**Hybrid Approaches**:
- Static files for old immutable data (headers, receipts)
- Aggressive pruning for state data
- Conservative pruning for recent state
- This provides the best balance of efficiency and functionality

**Business Considerations**:
- Exchange nodes: Need receipt history for deposit verification
- Analytics nodes: Require historical state for analysis
- Relay nodes: Can use aggressive pruning for efficiency
- Archive services: Must use no pruning

### 3. How does static file integration improve pruning efficiency?

**Answer**: Static files provide several advantages for pruning:

**Immutable Data Handling**:
- Headers, receipts, and transactions become immutable after finalization
- Can be moved to compressed static files instead of being pruned
- Reduces database size while maintaining data availability

**Performance Benefits**:
- Static files use less disk space (compression)
- Faster access patterns for sequential data
- Reduced database fragmentation
- Better caching characteristics

**Operational Advantages**:
- Pruning becomes less destructive (data moved, not deleted)
- Can regenerate database from static files if needed
- Easier to backup and restore specific data segments
- Better separation of hot (recent) and cold (historical) data

**Network Benefits**:
- Nodes can serve historical data from static files
- Reduces need for archive nodes
- Enables hybrid node types (full recent state + static historical data)
- Improves overall network data availability

This approach allows Reth to maintain much more historical data while keeping the actively-used database lean and performant, providing the best of both worlds for storage efficiency and network participation.