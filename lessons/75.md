# Lesson 75: IPC Communication

*"The best interface is no interface." - Golden Krishna*

## Overview
IPC (Inter-Process Communication) is like having a secure, private conversation between applications on the same computer. While WebSockets and HTTP are like talking over the phone (network communication), IPC is like passing notes under the door between rooms in your house - faster, more secure, and with no risk of eavesdropping from outside.

## Why IPC Matters for Blockchain Nodes

**Real-World Analogy**: Think of IPC like the intercom system in a secure building:
- **External APIs**: Main lobby entrance (HTTP/WebSocket) - anyone can try to enter
- **IPC**: Internal elevator between floors - only people already inside can use it

**The Security Advantage**:
- **Network APIs**: Exposed to the internet, need authentication, rate limiting
- **IPC**: Only local processes can connect, inherit OS-level security

**Critical Use Cases**:
- **Admin Tools**: "Safely shut down the node without exposing admin APIs"
- **Monitoring**: "Local monitoring without network exposure"
- **Development**: "Debug APIs that should never be public"
- **Integration**: "Connect local applications securely"

## Key Concepts
- **IPC Mechanisms**: Different ways processes can communicate locally
- **Protocol Design**: How to structure messages between processes
- **Security**: Leveraging OS security for access control
- **Performance**: Maximizing speed for local communication

## The Local Advantage

Here's why IPC is fundamentally different from network communication:

**Network Communication Challenges**:
- Packets can be intercepted
- Need complex authentication
- Subject to network latency/failures
- Bandwidth limitations

**IPC Advantages**:
- No network stack overhead
- OS-level process isolation
- Extremely low latency
- Higher bandwidth

**Common Pitfall**: Developers often default to HTTP APIs even for local communication, missing IPC's security and performance benefits.

## IPC Server Implementation

```rust
pub struct IpcServer {
    socket_path: PathBuf,
    listener: Option<UnixListener>,
    connections: Arc<RwLock<HashMap<ConnectionId, IpcConnection>>>,
    message_handler: Arc<dyn IpcMessageHandler>,
    security_manager: SecurityManager,
}

impl IpcServer {
    pub fn new(socket_path: PathBuf, handler: Arc<dyn IpcMessageHandler>) -> Self {
        Self {
            socket_path,
            listener: None,
            connections: Arc::new(RwLock::new(HashMap::new())),
            message_handler: handler,
            security_manager: SecurityManager::new(),
        }
    }
    
    pub async fn start(&mut self) -> Result<(), IpcError> {
        // Remove existing socket file
        if self.socket_path.exists() {
            std::fs::remove_file(&self.socket_path)?;
        }
        
        // Create Unix socket listener
        let listener = UnixListener::bind(&self.socket_path)?;
        self.listener = Some(listener);
        
        // Set appropriate permissions
        self.set_socket_permissions()?;
        
        // Start accepting connections
        self.accept_connections().await;
        
        Ok(())
    }
    
    async fn accept_connections(&self) {
        let listener = self.listener.as_ref().unwrap();
        
        loop {
            match listener.accept().await {
                Ok((stream, _)) => {
                    let connection_id = ConnectionId::new();
                    let connection = IpcConnection::new(connection_id, stream);
                    
                    // Authenticate connection
                    if let Err(e) = self.authenticate_connection(&connection).await {
                        error!("Authentication failed for connection {}: {}", connection_id, e);
                        continue;
                    }
                    
                    // Register connection
                    self.connections.write().unwrap().insert(connection_id, connection.clone());
                    
                    // Handle connection in separate task
                    let handler = self.message_handler.clone();
                    let connections = self.connections.clone();
                    
                    tokio::spawn(async move {
                        if let Err(e) = Self::handle_connection(connection, handler).await {
                            error!("Connection handling failed: {}", e);
                        }
                        
                        // Clean up connection
                        connections.write().unwrap().remove(&connection_id);
                    });
                }
                Err(e) => {
                    error!("Failed to accept connection: {}", e);
                }
            }
        }
    }
    
    async fn handle_connection(mut connection: IpcConnection, handler: Arc<dyn IpcMessageHandler>) -> Result<(), IpcError> {
        let mut buffer = vec![0u8; 4096];
        
        loop {
            let bytes_read = connection.read(&mut buffer).await?;
            
            if bytes_read == 0 {
                break; // Connection closed
            }
            
            // Parse message
            let message = IpcMessage::deserialize(&buffer[..bytes_read])?;
            
            // Handle message
            let response = handler.handle_message(connection.id(), message).await?;
            
            // Send response
            let response_data = response.serialize()?;
            connection.write(&response_data).await?;
        }
        
        Ok(())
    }
    
    fn set_socket_permissions(&self) -> Result<(), IpcError> {
        use std::os::unix::fs::PermissionsExt;
        
        let metadata = std::fs::metadata(&self.socket_path)?;
        let mut permissions = metadata.permissions();
        permissions.set_mode(0o600); // Owner read/write only
        
        std::fs::set_permissions(&self.socket_path, permissions)?;
        
        Ok(())
    }
}
```

## IPC Protocol Design

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum IpcMessage {
    Request {
        id: u64,
        method: String,
        params: serde_json::Value,
    },
    Response {
        id: u64,
        result: Result<serde_json::Value, IpcError>,
    },
    Notification {
        method: String,
        params: serde_json::Value,
    },
    Batch(Vec<IpcMessage>),
}

impl IpcMessage {
    pub fn serialize(&self) -> Result<Vec<u8>, IpcError> {
        // Use length-prefixed format
        let json = serde_json::to_string(self)?;
        let len = json.len() as u32;
        
        let mut buffer = Vec::new();
        buffer.extend_from_slice(&len.to_be_bytes());
        buffer.extend_from_slice(json.as_bytes());
        
        Ok(buffer)
    }
    
    pub fn deserialize(data: &[u8]) -> Result<Self, IpcError> {
        if data.len() < 4 {
            return Err(IpcError::InvalidMessage);
        }
        
        let len = u32::from_be_bytes([data[0], data[1], data[2], data[3]]) as usize;
        
        if data.len() < 4 + len {
            return Err(IpcError::IncompleteMessage);
        }
        
        let json_data = &data[4..4 + len];
        let json_str = std::str::from_utf8(json_data)?;
        
        Ok(serde_json::from_str(json_str)?)
    }
}

pub trait IpcMessageHandler: Send + Sync {
    async fn handle_message(&self, connection_id: ConnectionId, message: IpcMessage) -> Result<IpcMessage, IpcError>;
}
```

## IPC Client Implementation

```rust
pub struct IpcClient {
    socket_path: PathBuf,
    connection: Option<IpcConnection>,
    request_id: AtomicU64,
    pending_requests: Arc<Mutex<HashMap<u64, oneshot::Sender<Result<serde_json::Value, IpcError>>>>>,
}

impl IpcClient {
    pub fn new(socket_path: PathBuf) -> Self {
        Self {
            socket_path,
            connection: None,
            request_id: AtomicU64::new(1),
            pending_requests: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    pub async fn connect(&mut self) -> Result<(), IpcError> {
        let stream = UnixStream::connect(&self.socket_path).await?;
        let connection = IpcConnection::new(ConnectionId::new(), stream);
        
        self.connection = Some(connection.clone());
        
        // Start response handler
        let pending_requests = self.pending_requests.clone();
        tokio::spawn(async move {
            Self::handle_responses(connection, pending_requests).await;
        });
        
        Ok(())
    }
    
    pub async fn request(&self, method: &str, params: serde_json::Value) -> Result<serde_json::Value, IpcError> {
        let connection = self.connection.as_ref().ok_or(IpcError::NotConnected)?;
        
        let request_id = self.request_id.fetch_add(1, Ordering::SeqCst);
        let (sender, receiver) = oneshot::channel();
        
        // Register pending request
        self.pending_requests.lock().await.insert(request_id, sender);
        
        // Send request
        let request = IpcMessage::Request {
            id: request_id,
            method: method.to_string(),
            params,
        };
        
        let request_data = request.serialize()?;
        connection.write(&request_data).await?;
        
        // Wait for response
        let result = receiver.await.map_err(|_| IpcError::RequestCancelled)??;
        
        Ok(result)
    }
    
    pub async fn notify(&self, method: &str, params: serde_json::Value) -> Result<(), IpcError> {
        let connection = self.connection.as_ref().ok_or(IpcError::NotConnected)?;
        
        let notification = IpcMessage::Notification {
            method: method.to_string(),
            params,
        };
        
        let notification_data = notification.serialize()?;
        connection.write(&notification_data).await?;
        
        Ok(())
    }
    
    async fn handle_responses(
        mut connection: IpcConnection,
        pending_requests: Arc<Mutex<HashMap<u64, oneshot::Sender<Result<serde_json::Value, IpcError>>>>>,
    ) {
        let mut buffer = vec![0u8; 4096];
        
        loop {
            match connection.read(&mut buffer).await {
                Ok(0) => break, // Connection closed
                Ok(bytes_read) => {
                    if let Ok(message) = IpcMessage::deserialize(&buffer[..bytes_read]) {
                        Self::process_response_message(message, &pending_requests).await;
                    }
                }
                Err(e) => {
                    error!("Failed to read from IPC connection: {}", e);
                    break;
                }
            }
        }
    }
    
    async fn process_response_message(
        message: IpcMessage,
        pending_requests: &Arc<Mutex<HashMap<u64, oneshot::Sender<Result<serde_json::Value, IpcError>>>>>,
    ) {
        match message {
            IpcMessage::Response { id, result } => {
                if let Some(sender) = pending_requests.lock().await.remove(&id) {
                    let _ = sender.send(result);
                }
            }
            IpcMessage::Notification { method, params } => {
                // Handle notifications (e.g., events)
                debug!("Received notification: {} with params: {:?}", method, params);
            }
            _ => {
                warn!("Unexpected message type in response handler");
            }
        }
    }
}
```

## Named Pipes Implementation

```rust
#[cfg(windows)]
pub struct NamedPipeServer {
    pipe_name: String,
    server_handle: Option<PipeServer>,
    connections: Arc<RwLock<HashMap<ConnectionId, NamedPipeConnection>>>,
}

#[cfg(windows)]
impl NamedPipeServer {
    pub fn new(pipe_name: String) -> Self {
        Self {
            pipe_name,
            server_handle: None,
            connections: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    pub async fn start(&mut self) -> Result<(), IpcError> {
        let pipe_server = PipeServer::new(&self.pipe_name)?;
        self.server_handle = Some(pipe_server);
        
        self.accept_connections().await;
        
        Ok(())
    }
    
    async fn accept_connections(&self) {
        let pipe_server = self.server_handle.as_ref().unwrap();
        
        loop {
            match pipe_server.accept().await {
                Ok(pipe_stream) => {
                    let connection_id = ConnectionId::new();
                    let connection = NamedPipeConnection::new(connection_id, pipe_stream);
                    
                    // Register connection
                    self.connections.write().unwrap().insert(connection_id, connection.clone());
                    
                    // Handle connection
                    let connections = self.connections.clone();
                    tokio::spawn(async move {
                        Self::handle_pipe_connection(connection).await;
                        connections.write().unwrap().remove(&connection_id);
                    });
                }
                Err(e) => {
                    error!("Failed to accept named pipe connection: {}", e);
                }
            }
        }
    }
    
    async fn handle_pipe_connection(connection: NamedPipeConnection) {
        // Similar to Unix socket handling
        // Implementation depends on Windows API
    }
}
```

## Shared Memory IPC

```rust
pub struct SharedMemoryIpc {
    memory_region: Arc<SharedMemory>,
    message_queue: Arc<Mutex<VecDeque<IpcMessage>>>,
    semaphore: Arc<Semaphore>,
}

impl SharedMemoryIpc {
    pub fn new(size: usize) -> Result<Self, IpcError> {
        let memory_region = SharedMemory::create(size)?;
        
        Ok(Self {
            memory_region: Arc::new(memory_region),
            message_queue: Arc::new(Mutex::new(VecDeque::new())),
            semaphore: Arc::new(Semaphore::new(1)),
        })
    }
    
    pub async fn send_message(&self, message: IpcMessage) -> Result<(), IpcError> {
        let _permit = self.semaphore.acquire().await?;
        
        // Serialize message
        let serialized = message.serialize()?;
        
        // Write to shared memory
        self.memory_region.write_message(&serialized)?;
        
        Ok(())
    }
    
    pub async fn receive_message(&self) -> Result<IpcMessage, IpcError> {
        let _permit = self.semaphore.acquire().await?;
        
        // Read from shared memory
        let data = self.memory_region.read_message()?;
        
        // Deserialize message
        let message = IpcMessage::deserialize(&data)?;
        
        Ok(message)
    }
}

pub struct SharedMemory {
    ptr: *mut u8,
    size: usize,
}

impl SharedMemory {
    pub fn create(size: usize) -> Result<Self, IpcError> {
        // Platform-specific shared memory creation
        #[cfg(unix)]
        {
            use libc::{mmap, MAP_SHARED, MAP_ANONYMOUS, PROT_READ, PROT_WRITE};
            
            let ptr = unsafe {
                mmap(
                    std::ptr::null_mut(),
                    size,
                    PROT_READ | PROT_WRITE,
                    MAP_SHARED | MAP_ANONYMOUS,
                    -1,
                    0,
                )
            };
            
            if ptr == libc::MAP_FAILED {
                return Err(IpcError::SharedMemoryError);
            }
            
            Ok(Self {
                ptr: ptr as *mut u8,
                size,
            })
        }
        
        #[cfg(windows)]
        {
            // Windows implementation using CreateFileMapping
            unimplemented!("Windows shared memory not implemented in this example");
        }
    }
    
    pub fn write_message(&self, data: &[u8]) -> Result<(), IpcError> {
        if data.len() > self.size {
            return Err(IpcError::MessageTooLarge);
        }
        
        unsafe {
            std::ptr::copy_nonoverlapping(data.as_ptr(), self.ptr, data.len());
        }
        
        Ok(())
    }
    
    pub fn read_message(&self) -> Result<Vec<u8>, IpcError> {
        // Read message length first
        let len = unsafe { *(self.ptr as *const u32) } as usize;
        
        if len > self.size - 4 {
            return Err(IpcError::InvalidMessage);
        }
        
        let mut data = vec![0u8; len];
        unsafe {
            std::ptr::copy_nonoverlapping(self.ptr.add(4), data.as_mut_ptr(), len);
        }
        
        Ok(data)
    }
}
```

## Deep Dive: IPC Mechanisms and Trade-offs

### IPC Mechanism Comparison

**1. Unix Domain Sockets**
```rust
// Like TCP sockets but local-only
let socket = UnixListener::bind("/tmp/reth.sock")?;
```
- **Pros**: Stream-based, reliable, works like network sockets
- **Cons**: Filesystem permissions can be tricky
- **Best for**: RPC-style communication

**2. Named Pipes (FIFOs)**
```rust
// One-way communication channels
let pipe = OpenOptions::new().write(true).open("/tmp/reth.pipe")?;
```
- **Pros**: Simple, integrated with shell tools
- **Cons**: One-way only, blocking behavior
- **Best for**: Log streaming, simple notifications

**3. Shared Memory**
```rust
// Direct memory sharing between processes
let shmem = SharedMemory::create(1024 * 1024)?; // 1MB shared
```
- **Pros**: Extremely fast, no copying
- **Cons**: Complex synchronization, unsafe
- **Best for**: High-frequency data sharing

### Security Models

**Unix Socket Security**:
```bash
# Only owner can connect
chmod 600 /tmp/reth.sock

# Specific group access
chmod 660 /tmp/reth.sock
chgrp reth-users /tmp/reth.sock
```

**Process-Based Authentication**:
```rust
// Check connecting process credentials
use nix::unistd::{getpid, getuid};

fn authenticate_connection(stream: &UnixStream) -> bool {
    let peer_cred = stream.peer_cred().ok()?;
    // Only allow connections from same user
    peer_cred.uid() == getuid()
}
```

### Performance Characteristics

**Latency Comparison** (typical):
- **Shared Memory**: ~100 nanoseconds
- **Unix Sockets**: ~1-10 microseconds  
- **Named Pipes**: ~1-10 microseconds
- **TCP Localhost**: ~10-100 microseconds
- **TCP Network**: ~1-100 milliseconds

**Bandwidth Comparison**:
- **Shared Memory**: Memory bandwidth (~50 GB/s)
- **Unix Sockets**: ~1-10 GB/s
- **TCP Localhost**: ~1-5 GB/s

### Real Implementation Patterns

**Pattern 1: Admin Socket**
```rust
// Separate socket for admin commands
let admin_socket = "/var/run/reth/admin.sock";
set_permissions(&admin_socket, 0o600); // Owner only
```

**Pattern 2: JSON-RPC over IPC**
```rust
// Same protocol as HTTP, but over Unix socket
let request = r#"{
    "id": 1,
    "method": "admin_nodeInfo", 
    "params": []
}"#;
```

**Pattern 3: Binary Protocol**
```rust
// Custom binary protocol for performance
struct IpcMessage {
    length: u32,
    message_type: u8,
    payload: Vec<u8>,
}
```

### Connection to Other Lessons

- **Lesson 73**: IPC provides secure alternative to custom RPC endpoints
- **Lesson 74**: WebSockets are networked alternative to IPC
- **Lesson 19**: IPC extends RPC architecture to local communication

## Common Mistakes and How to Avoid Them

1. **Ignoring File Permissions**: Don't leave sockets world-readable
   - **Problem**: Anyone on system can connect
   - **Solution**: Set restrictive permissions (600 or 660)

2. **No Cleanup**: Don't leave socket files after shutdown
   - **Problem**: Stale sockets prevent restart
   - **Solution**: Remove socket files on startup and shutdown

3. **Blocking I/O**: Don't use blocking operations in async contexts
   - **Problem**: Can deadlock the entire system
   - **Solution**: Use async IPC libraries or separate threads

4. **Poor Error Handling**: Don't ignore connection errors
   - **Problem**: Silent failures confuse users
   - **Solution**: Proper error propagation and logging

## Platform Differences

**Unix/Linux**: Full IPC support
- Unix domain sockets
- Named pipes (FIFOs)
- Shared memory
- Message queues

**Windows**: Different mechanisms
- Named pipes (different from Unix)
- Shared memory
- No Unix domain sockets (until Windows 10)

## Summary
IPC communication provides secure, high-performance local communication between blockchain node processes. It solves the problem of needing admin/debug interfaces without network exposure. The key is choosing the right IPC mechanism for your use case and implementing proper security through OS-level permissions and process authentication.

## Assignments
1. **IPC Server**: Build complete IPC server with multiple protocols
2. **Client Library**: Create easy-to-use IPC client library
3. **Performance Benchmark**: Compare IPC mechanisms performance

## Questions to Ponder
1. Which IPC mechanism is best for different use cases?
2. How do you handle IPC security and authentication?
3. What are the performance trade-offs between IPC methods?
4. How do you ensure IPC reliability and error handling?
5. What debugging tools help with IPC development?